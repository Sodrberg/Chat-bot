{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23475, Requested 6221. Please try again in 29.088s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23475, Requested 6221. Please try again in 29.088s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 22913, Requested 6221. Please try again in 27.404s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 22913, Requested 6221. Please try again in 27.404s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21821, Requested 6221. Please try again in 24.127s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21821, Requested 6221. Please try again in 24.127s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21821, Requested 6221. Please try again in 24.127s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21821, Requested 6221. Please try again in 24.127s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "they said, the old\npedlar-woman was no one\nelse than the wicked queen, take care and let no one come in\nwhen we are not with you.\n\nBut the wicked woman when she had reached home went in front\nof the glass and asked,\n          looking-glass, looking-glass, on the wall,\n          who in this land is the fairest of all.\n\nAnd it answered as before,\n          oh, queen, thou art fairest of all I see,\n          but over the hills, where the seven dwarfs dwell,\n          snow-white is still alive and well,\n          and none is so fair as she.\n\nWhen she heard that, all her blood rushed to her heart with fear,\nfor she saw plainly that little snow-white was again alive.\nBut now, she said, I will think of something that shall really\nput an end to you.  And by the help of witchcraft, which she\nunderstood, she made a poisonous comb.  Then she disguised\nherself and took the shape of another old woman.  So she went\nover the seven mountains to the seven dwarfs, knocked at the\ndoor, and cried, good things to sell, cheap, cheap.  Little\nsnow-white looked out and said, go away, I cannot let anyone come\nin.  I suppose you can look, said the old woman, and pulled the\npoisonous comb out and held it up.  It pleased the girl so well\nthat she let herself be beguiled, and opened the door.  When they\nhad made a bargain the old woman said, now I will comb you\nproperly for once.  Poor little snow-white had no suspicion, and\nlet the old woman do as she pleased, but hardly had she put the\ncomb in her hair than the poison in it took effect, and the girl\nfell down senseless.  You paragon of beauty, said the wicked\nwoman, you are done for now, and she went away.\n\nBut fortunately it was almost evening, when the seven dwarfs\ncame home.  When they saw snow-white lying as if dead upon the\nground they at once suspected the step-mother, and they looked\nand found the poisoned comb.  Scarcely had they taken it out when\nsnow-white came to herself, and told them what had happened.\nThen they warned her once more to be upon her guard and to open\nthe door to no one.\n\nThe queen, at home, went in front of the glass and said,\n          looking-glass, looking-glass, on the wall,\n          who in this land is the fairest of all.\n\nThen it answered as before,\n          oh, queen, thou art fairest of all I see,\n          but over the hills, where the seven dwarfs dwell,\n          snow-white is still alive and well,\n          and none is so fair as she.\n\nWhen she heard the glass speak thus she trembled and shook\nwith rage.  Snow-white shall die, she cried, even if it costs me\nmy life.\n\nThereupon she went into a quite secret, lonely room, where no\none ever came, and there she made a very poisonous apple.\nOutside it looked pretty, white with a red cheek, so that\neveryone who saw it longed for it, but whoever ate a piece of it\nmust surely die.\n\nWhen the apple was ready she painted her face, and dressed herself\nup as a farmer's wife, and so she went over the seven\nmountains to the seven dwarfs.  She knocked at the door.  Snow-white\nput her head out of the window and said, I cannot let\nanyone in, the seven dwarfs have forbidden me.  It is all the\nsame to me, answered the woman, I shall soon get rid of my apples.\nThere, I will give you one.\n\nNo, said snow-white, I dare not take anything.  Are you afraid\nof poison, said the old woman, look, I will cut the apple in two\npieces, you eat the red cheek, and I will eat the white.  The\napple was so cunningly made that only the red cheek was\npoisoned.  Snow-white longed for the fine apple, and when she saw\nthat the woman ate part of it she could resist no longer, and\nstretched out\nher hand and took the poisonous half.  But hardly had she a bit\nof it in her mouth than she fell down dead.  Then the queen\nlooked at her with a dreadful look, and laughed aloud and said,\nwhite as snow, red as blood, black as ebony-wood, this time the\ndwarfs cannot wake you up again.\n\nAnd when she asked of the looking-glass at home,\n          looking-glass, looking-glass, on the wall,\n          who in this land is the fairest of all.\n\nAnd it answered at last,\n          oh, queen, in this land thou art fairest of all.\nThen her envious heart had rest, so far as an envious heart can\nhave rest.\n\nThe dwarfs, when they came home in the evening, found snow-white\nlying upon the ground, she breathed no longer and was dead.\nThey lifted her up, looked to see whether they could find\nanything poisonous, unlaced her, combed her hair, washed her\nwith water and wine, but it was all of no use, the poor child was\ndead, and remained dead.  They laid her upon a bier, and all\nseven of them sat round it and wept for her, and wept three days\nlong.\n\nThen they were going to bury her, but she still looked as if she\nwere living, and still had her pretty red cheeks.  They"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21647, Requested 246. Please try again in 5.68s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21647, Requested 246. Please try again in 5.68s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"QUEEN\"\nDescription List: [\"The queen is Snow-white's step-mother who is jealous of Snow-white's beauty and tries to kill her\", \"The queen is a beautiful woman, but proud and haughty, and she could not bear that anyone else chould surpass her in beauty\", \"The queen is snow-white's wicked step-mother who is jealous of snow-white's beauty\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21622, Requested 186. Please try again in 5.425s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21622, Requested 186. Please try again in 5.425s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"KROHAARA\"\nDescription List: [\"Krohaara is a city in Quintara\", \"Krohaara is the capital of Quintara\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21623, Requested 213. Please try again in 5.508s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21623, Requested 213. Please try again in 5.508s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: [\"SAMUEL NAMARA\", \"MEGGIE TAZBAH\"]\nDescription List: [\"Samuel Namara and Meggie Tazbah are exchanged in the same hostage release\", \"Samuel Namara and Meggie Tazbah were exchanged in the same hostage release\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21621, Requested 217. Please try again in 5.514s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21621, Requested 217. Please try again in 5.514s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"HUNTSMAN\"\nDescription List: [\"The huntsman is a man who is tasked with killing Snow White, but he has pity on her and lets her go\", \"The huntsman is a man who was tasked with killing Snow-white but spared her life instead\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21619, Requested 206. Please try again in 5.477s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21619, Requested 206. Please try again in 5.477s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: [\"QUEEN\", \"SNOW-WHITE\"]\nDescription List: [\"The queen is jealous of Snow-white's beauty and tries to kill her\", \"The queen was jealous of snow-white's beauty and tried to kill her\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21618, Requested 204. Please try again in 5.468s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21618, Requested 204. Please try again in 5.468s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"TECHGLOBAL\"\nDescription List: [\"TechGlobal is a semiconductor corporation\", \"TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21618, Requested 214. Please try again in 5.497s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21618, Requested 214. Please try again in 5.497s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: [\"DURKE BATAGLANI\", \"MEGGIE TAZBAH\"]\nDescription List: [\"Meggie Tazbah and Durke Bataglani are exchanged in the same hostage release\", \"Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21619, Requested 219. Please try again in 5.514s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21619, Requested 219. Please try again in 5.514s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"SEVEN DWARFS\"\nDescription List: [\"The seven dwarfs are a group of small men who dig and delved in the mountains for ore\", \"The seven dwarfs are a group of small men who live in a house in the mountains and are friendly to Snow-white\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21616, Requested 211. Please try again in 5.481s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21616, Requested 211. Please try again in 5.481s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"TIRUZIA\"\nDescription List: [\"Tiruzia is a city in Firuzabad where hostages are held\", \"Tiruzia is the capital of Firuzabad\", \"Tiruzia is the capital of Firuzabad where the Aurelians were being held\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21617, Requested 226. Please try again in 5.529s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21617, Requested 226. Please try again in 5.529s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"SAMUEL NAMARA\"\nDescription List: [\"Samuel Namara is a 39-year-old businessman who is held in Alhamia prison\", \"Samuel Namara is a 39-year-old businessman who was held captive in Alhamia Prison\", \"Samuel Namara is an Aurelian who was held hostage in Firuzabad\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21616, Requested 185. Please try again in 5.404s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21616, Requested 185. Please try again in 5.404s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"CASHION\"\nDescription List: [\"Cashion is a city in Aurelia\", \"Cashion is the capital of Aurelia\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21615, Requested 231. Please try again in 5.54s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21615, Requested 231. Please try again in 5.54s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"ALHAMIA PRISON\"\nDescription List: [\"Alhamia Prison is a place where Samuel Namara was held captive\", \"Alhamia prison is a place where Samuel Namara is held\", \"Alhamia prison was not found in the text, however, Tiruzia's Alhamia Prison was found in the text, so I will add it below\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21616, Requested 220. Please try again in 5.509s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21616, Requested 220. Please try again in 5.509s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"LOOKING-GLASS\"\nDescription List: [\"The looking-glass is a magical mirror that tells the truth and is used by the queen to check her beauty\", \"The looking-glass is a magical object that can tell the truth about who is the fairest of all\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21606, Requested 207. Please try again in 5.439s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21606, Requested 207. Please try again in 5.439s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"QUINTARA\"\nDescription List: [\"\", \"Quintara is a country that brokered the hostage exchange between Firuzabad and Aurelia\", \"Quintara is a country that helped in the hostage exchange\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21603, Requested 216. Please try again in 5.457s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21603, Requested 216. Please try again in 5.457s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"CENTRAL INSTITUTION\"\nDescription List: [\"The Central Institution is the Federal Reserve of Verdantis\", \"The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21605, Requested 234. Please try again in 5.518s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21605, Requested 234. Please try again in 5.518s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"MEGGIE TAZBAH\"\nDescription List: [\"Meggie Tazbah is a 53-year-old environmentalist who is held in Alhamia prison\", \"Meggie Tazbah is a 53-year-old environmentalist who was held hostage by Firuzabad\", \"Meggie Tazbah is a Bratinas national and environmentalist who was held hostage in Firuzabad\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21605, Requested 209. Please try again in 5.442s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21605, Requested 209. Please try again in 5.442s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"FOREST\"\nDescription List: [\"The forest is where Snow White runs away to and finds the seven dwarfs' cottage\", \"The forest was where the king's son met the dwarfs and saw snow-white's coffin\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21605, Requested 229. Please try again in 5.502s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21605, Requested 229. Please try again in 5.502s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"DURKE BATAGLANI\"\nDescription List: [\"Durke Bataglani is a 59-year-old journalist who is held in Alhamia prison\", \"Durke Bataglani is a 59-year-old journalist who was held hostage by Firuzabad\", \"Durke Bataglani is an Aurelian journalist who was held hostage in Firuzabad\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21604, Requested 213. Please try again in 5.452s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21604, Requested 213. Please try again in 5.452s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"AURELIA\"\nDescription List: [\"Aurelia is a country that is seeking to release its hostages\", \"Aurelia is a country where Snow White and the seven dwarfs live\", \"Aurelia is a country where snow-white was from\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21602, Requested 195. Please try again in 5.393s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21602, Requested 195. Please try again in 5.393s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: [\"SAMUEL NAMARA\", \"FIRUZABAD\"]\nDescription List: [\"Samuel Namara is a hostage in Firuzabad\", \"Samuel Namara was a hostage in Firuzabad\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21601, Requested 197. Please try again in 5.395s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21601, Requested 197. Please try again in 5.395s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: [\"ALHAMIA PRISON\", \"SAMUEL NAMARA\"]\nDescription List: [\"Samuel Namara is held in Alhamia prison\", \"Samuel Namara was a prisoner at Alhamia prison\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21602, Requested 209. Please try again in 5.434s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21602, Requested 209. Please try again in 5.434s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"FIRUZABAD\"\nDescription List: [\"Firuzabad is a country where the dwarfs were from\", \"Firuzabad is a place where hostages are held\", \"Firuzabad is a place where the Aurelians were held hostage\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21447, Requested 214. Please try again in 4.983s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 21447, Requested 214. Please try again in 4.983s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: [\"SAMUEL NAMARA\", \"DURKE BATAGLANI\"]\nDescription List: [\"Samuel Namara and Durke Bataglani are exchanged in the same hostage release\", \"Samuel Namara and Durke Bataglani were exchanged in the same hostage release\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 20951, Requested 209. Please try again in 3.481s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 20951, Requested 209. Please try again in 3.481s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"FOREST\"\nDescription List: [\"The forest is where Snow White runs away to and finds the seven dwarfs' cottage\", \"The forest was where the king's son met the dwarfs and saw snow-white's coffin\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 20346, Requested 196. Please try again in 1.626s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 20346, Requested 196. Please try again in 1.626s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: [\"DURKE BATAGLANI\", \"FIRUZABAD\"]\nDescription List: [\"Durke Bataglani is a hostage in Firuzabad\", \"Durke Bataglani was a hostage in Firuzabad\"]\n#######\nOutput:\n"}}
