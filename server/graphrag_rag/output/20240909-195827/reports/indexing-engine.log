19:58:27,723 graphrag.index.cli INFO Logging enabled at graphrager/output/20240909-195827/reports/indexing-engine.log
19:58:27,726 graphrag.index.cli INFO Starting pipeline run for: 20240909-195827, dryrun=False
19:58:27,727 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "llama-3.1-8b-instant",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://api.groq.com/openai/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 4500,
        "requests_per_minute": 25,
        "max_retries": 3,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "graphrager",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "llama-3.1-8b-instant",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.groq.com/openai/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 4500,
            "requests_per_minute": 25,
            "max_retries": 3,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "llama-3.1-8b-instant",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.groq.com/openai/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 4500,
            "requests_per_minute": 25,
            "max_retries": 3,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "llama-3.1-8b-instant",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.groq.com/openai/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 4500,
            "requests_per_minute": 25,
            "max_retries": 3,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 1000,
        "max_input_length": 4000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "llama-3.1-8b-instant",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.groq.com/openai/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 4500,
            "requests_per_minute": 25,
            "max_retries": 3,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 5000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 5000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
19:58:27,731 graphrag.index.create_pipeline_config INFO skipping workflows 
19:58:27,731 graphrag.index.run INFO Running pipeline
19:58:27,731 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at graphrager/output/20240909-195827/artifacts
19:58:27,732 graphrag.index.input.load_input INFO loading input from root_dir=input
19:58:27,732 graphrag.index.input.load_input INFO using file storage for input
19:58:27,732 graphrag.index.storage.file_pipeline_storage INFO search graphrager/input for files matching .*\.txt$
19:58:27,733 graphrag.index.input.text INFO found text files from input, found [('frankenstein.txt', {})]
19:58:27,737 graphrag.index.input.text INFO Found 1 files, loading 1
19:58:27,739 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
19:58:27,740 graphrag.index.run INFO Final # of rows loaded: 1
19:58:27,903 graphrag.index.run INFO Running workflow: create_base_text_units...
19:58:27,904 graphrag.index.run INFO dependencies for create_base_text_units: []
19:58:27,905 datashaper.workflow.workflow INFO executing verb orderby
19:58:27,906 datashaper.workflow.workflow INFO executing verb zip
19:58:27,908 datashaper.workflow.workflow INFO executing verb aggregate_override
19:58:27,914 datashaper.workflow.workflow INFO executing verb chunk
19:58:28,304 datashaper.workflow.workflow INFO executing verb select
19:58:28,305 datashaper.workflow.workflow INFO executing verb unroll
19:58:28,307 datashaper.workflow.workflow INFO executing verb rename
19:58:28,308 datashaper.workflow.workflow INFO executing verb genid
19:58:28,317 datashaper.workflow.workflow INFO executing verb unzip
19:58:28,319 datashaper.workflow.workflow INFO executing verb copy
19:58:28,320 datashaper.workflow.workflow INFO executing verb filter
19:58:28,335 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:58:28,507 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
19:58:28,508 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
19:58:28,508 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
19:58:28,520 datashaper.workflow.workflow INFO executing verb entity_extract
19:58:28,541 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://api.groq.com/openai/v1
19:58:28,598 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for llama-3.1-8b-instant: TPM=4500, RPM=25
19:58:28,598 graphrag.index.llm.load_llm INFO create concurrency limiter for llama-3.1-8b-instant: 25
19:58:30,800 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
19:58:30,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.17717779497616. input_tokens=2936, output_tokens=710
19:59:10,393 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
19:59:10,396 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.610926622990519. input_tokens=2934, output_tokens=791
19:59:50,245 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:00:10,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.303702916018665. input_tokens=2936, output_tokens=1497
20:00:51,345 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:01:02,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0156857249094173. input_tokens=2934, output_tokens=799
20:01:47,683 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:02:22,983 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:02:38,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6704923359211534. input_tokens=2936, output_tokens=1175
20:03:20,638 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:03:34,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.835958328912966. input_tokens=2936, output_tokens=1022
20:04:10,7 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:04:23,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.9132942520081997. input_tokens=2936, output_tokens=1022
20:05:01,670 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:05:11,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.146738661918789. input_tokens=2935, output_tokens=718
20:05:52,703 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:06:03,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3071808769600466. input_tokens=2936, output_tokens=807
20:06:38,691 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:06:51,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3741520419716835. input_tokens=2935, output_tokens=975
20:07:27,841 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:07:38,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0879447309998795. input_tokens=2936, output_tokens=811
20:08:17,959 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:08:27,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.22856346599292. input_tokens=2935, output_tokens=683
20:09:09,233 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:09:20,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.016357759013772. input_tokens=2936, output_tokens=821
20:09:56,435 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:10:07,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0031379680149257. input_tokens=2936, output_tokens=796
20:10:46,659 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:11:07,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.0758459989447147. input_tokens=2936, output_tokens=1590
20:11:47,580 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:11:57,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.543641966069117. input_tokens=2936, output_tokens=737
20:12:37,498 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:13:03,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.4700518670724705. input_tokens=2936, output_tokens=1963
20:13:44,409 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:13:51,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5851423499407247. input_tokens=2936, output_tokens=518
20:14:27,584 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:14:48,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.415068040951155. input_tokens=2936, output_tokens=1543
20:15:28,238 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:15:38,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0225084900157526. input_tokens=2935, output_tokens=804
20:16:20,189 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:16:31,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1190029829740524. input_tokens=2936, output_tokens=883
20:17:13,75 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:17:13,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.9644768900470808. input_tokens=2936, output_tokens=641
20:17:54,634 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:18:04,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.4091168160084635. input_tokens=2936, output_tokens=725
20:18:45,338 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:19:04,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.4871961110038683. input_tokens=2935, output_tokens=1454
20:19:43,548 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:19:46,633 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:19:50,828 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:19:56,330 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:19:56,601 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:19:56,616 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:19:56,617 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:19:58,809 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:19:58,811 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:19:58,811 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:01,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.92699659592472. input_tokens=2935, output_tokens=1378
20:20:01,992 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:01,994 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:01,994 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:01,994 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 16849, Requested 3613. Please try again in 1.386s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:20:02,12 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'greatest attention.\n\nAbout two hours after this occurrence we heard the ground sea, and before\nnight the ice broke and freed our ship. We, however, lay to until the\nmorning, fearing to encounter in the dark those large loose masses which\nfloat about after the breaking up of the ice. I profited of this time to\nrest for a few hours.\n\nIn the morning, however, as soon as it was light, I went upon deck and\nfound all the sailors busy on one side of the vessel, apparently\ntalking to someone in the sea. It was, in fact, a sledge, like that we\nhad seen before, which had drifted towards us in the night on a large\nfragment of ice. Only one dog remained alive; but there was a human\nbeing within it whom the sailors were persuading to enter the vessel.\nHe was not, as the other traveller seemed to be, a savage inhabitant of\nsome undiscovered island, but a European. When I appeared on deck the\nmaster said, “Here is our captain, and he will not allow you to perish\non the open sea.”\n\nOn perceiving me, the stranger addressed me in English, although with a\nforeign accent. “Before I come on board your vessel,” said he,\n“will you have the kindness to inform me whither you are bound?”\n\nYou may conceive my astonishment on hearing such a question addressed\nto me from a man on the brink of destruction and to whom I should have\nsupposed that my vessel would have been a resource which he would not\nhave exchanged for the most precious wealth the earth can afford. I\nreplied, however, that we were on a voyage of discovery towards the\nnorthern pole.\n\nUpon hearing this he appeared satisfied and consented to come on board.\nGood God! Margaret, if you had seen the man who thus capitulated for\nhis safety, your surprise would have been boundless. His limbs were\nnearly frozen, and his body dreadfully emaciated by fatigue and\nsuffering. I never saw a man in so wretched a condition. We attempted\nto carry him into the cabin, but as soon as he had quitted the fresh\nair he fainted. We accordingly brought him back to the deck and\nrestored him to animation by rubbing him with brandy and forcing him to\nswallow a small quantity. As soon as he showed signs of life we\nwrapped him up in blankets and placed him near the chimney of the\nkitchen stove. By slow degrees he recovered and ate a little soup,\nwhich restored him wonderfully.\n\nTwo days passed in this manner before he was able to speak, and I often\nfeared that his sufferings had deprived him of understanding. When he\nhad in some measure recovered, I removed him to my own cabin and\nattended on him as much as my duty would permit. I never saw a more\ninteresting creature: his eyes have generally an expression of\nwildness, and even madness, but there are moments when, if anyone\nperforms an act of kindness towards him or does him any the most\ntrifling service, his whole countenance is lighted up, as it were, with\na beam of benevolence and sweetness that I never saw equalled. But he\nis generally melancholy and despairing, and sometimes he gnashes his\nteeth, as if impatient of the weight of woes that oppresses him.\n\nWhen my guest was a little recovered I had great trouble to keep off\nthe men, who wished to ask him a thousand questions; but I would not\nallow him to be tormented by their idle curiosity, in a state of body\nand mind whose restoration evidently depended upon entire repose.\nOnce, however, the lieutenant asked why he had come so far upon the ice\nin so strange a vehicle.\n\nHis countenance instantly assumed an aspect of the deepest gloom, and\nhe replied, “To seek one who fled from me.”\n\n“And did the man whom you pursued travel in the same fashion?”\n\n“Yes.”\n\n“Then I fancy we have seen him, for the day before we picked you up we\nsaw some dogs drawing a sledge, with a man in it, across the ice.”\n\nThis aroused the stranger’s attention, and he asked a multitude of\nquestions concerning the route which the dæmon, as he called him, had\npursued. Soon after, when he was alone with me, he said, “I have,\ndoubtless, excited your curiosity, as well as that of these good\npeople; but you are too considerate to make inquiries.”\n\n“Certainly; it would indeed be very impertinent and inhuman in me to\ntrouble you with any inquisitiveness of mine.”\n\n“And yet you rescued me from a strange and perilous situation; you have\nbenevolently restored me to life.”\n\nSoon after this he inquired if I thought that the breaking up of the\nice had destroyed the other sledge. I replied that I could not answer\nwith any degree of certainty, for the ice had not broken until near\nmidnight, and the traveller might have arrived at a place of safety\nbefore that time; but of this I could not judge.\n\nFrom this time a new spirit of life animated the decaying frame of the\nstranger. He manifested the greatest eagerness to be upon deck to watch for\nthe sledge which had before appeared; but I have persuaded him to remain in\nthe cabin, for he is far too weak to sustain the rawness of the atmosphere.\nI have promised that someone should watch for him and give him instant\nnotice if any new object should appear in sight.\n\nSuch is my journal of what relates to this strange occurrence up to the\npresent day. The stranger'}
20:20:02,363 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:02,365 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:02,365 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:04,459 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:04,460 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:04,461 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:12,993 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:20:13,281 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:13,282 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:13,282 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:15,468 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:15,469 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:15,469 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:18,522 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:18,524 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:18,524 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:18,524 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17719, Requested 3716. Please try again in 4.304s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:20:18,528 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'bestowed on\nthem by Heaven, whom to bring up to good, and whose future lot it was in\ntheir hands to direct to happiness or misery, according as they fulfilled\ntheir duties towards me. With this deep consciousness of what they owed\ntowards the being to which they had given life, added to the active spirit\nof tenderness that animated both, it may be imagined that while during\nevery hour of my infant life I received a lesson of patience, of charity,\nand of self-control, I was so guided by a silken cord that all seemed but\none train of enjoyment to me.\n\nFor a long time I was their only care. My mother had much desired to have a\ndaughter, but I continued their single offspring. When I was about five\nyears old, while making an excursion beyond the frontiers of Italy, they\npassed a week on the shores of the Lake of Como. Their benevolent\ndisposition often made them enter the cottages of the poor. This, to my\nmother, was more than a duty; it was a necessity, a\npassion—remembering what she had suffered, and how she had been\nrelieved—for her to act in her turn the guardian angel to the\nafflicted. During one of their walks a poor cot in the foldings of a vale\nattracted their notice as being singularly disconsolate, while the number\nof half-clothed children gathered about it spoke of penury in its worst\nshape. One day, when my father had gone by himself to Milan, my mother,\naccompanied by me, visited this abode. She found a peasant and his wife,\nhard working, bent down by care and labour, distributing a scanty meal to\nfive hungry babes. Among these there was one which attracted my mother far\nabove all the rest. She appeared of a different stock. The four others were\ndark-eyed, hardy little vagrants; this child was thin and very fair. Her\nhair was the brightest living gold, and despite the poverty of her\nclothing, seemed to set a crown of distinction on her head. Her brow was\nclear and ample, her blue eyes cloudless, and her lips and the moulding of\nher face so expressive of sensibility and sweetness that none could behold\nher without looking on her as of a distinct species, a being heaven-sent,\nand bearing a celestial stamp in all her features.\n\nThe peasant woman, perceiving that my mother fixed eyes of wonder and\nadmiration on this lovely girl, eagerly communicated her history. She was\nnot her child, but the daughter of a Milanese nobleman. Her mother was a\nGerman and had died on giving her birth. The infant had been placed with\nthese good people to nurse: they were better off then. They had not been\nlong married, and their eldest child was but just born. The father of their\ncharge was one of those Italians nursed in the memory of the antique glory\nof Italy—one among the _schiavi ognor frementi,_ who exerted\nhimself to obtain the liberty of his country. He became the victim of its\nweakness. Whether he had died or still lingered in the dungeons of Austria\nwas not known. His property was confiscated; his child became an orphan and\na beggar. She continued with her foster parents and bloomed in their rude\nabode, fairer than a garden rose among dark-leaved brambles.\n\nWhen my father returned from Milan, he found playing with me in the hall of\nour villa a child fairer than pictured cherub—a creature who seemed\nto shed radiance from her looks and whose form and motions were lighter\nthan the chamois of the hills. The apparition was soon explained. With his\npermission my mother prevailed on her rustic guardians to yield their\ncharge to her. They were fond of the sweet orphan. Her presence had seemed\na blessing to them, but it would be unfair to her to keep her in poverty\nand want when Providence afforded her such powerful protection. They\nconsulted their village priest, and the result was that Elizabeth Lavenza\nbecame the inmate of my parents’ house—my more than\nsister—the beautiful and adored companion of all my occupations and\nmy pleasures.\n\nEveryone loved Elizabeth. The passionate and almost reverential\nattachment with which all regarded her became, while I shared it, my\npride and my delight. On the evening previous to her being brought to\nmy home, my mother had said playfully, “I have a pretty present for my\nVictor—tomorrow he shall have it.” And when, on the morrow, she\npresented Elizabeth to me as her promised gift, I, with childish\nseriousness, interpreted her words literally and looked upon Elizabeth\nas mine—mine to protect, love, and cherish. All praises bestowed on\nher I received as made to a possession of my own. We called each other\nfamiliarly by the name of cousin. No word, no expression could body\nforth the kind of relation in which she stood to me—my more than\nsister, since till death she was to be mine only.\n\n\n\n\nChapter 2\n\n\nWe were brought up together; there was not quite a year difference in\nour ages. I need not say that we were strangers to any species of\ndisunion or dispute. Harmony was the soul of our companionship, and\nthe diversity and contrast that subsisted in our characters drew us\nnearer together. Elizabeth was of a calmer and more concentrated\ndisposition; but, with all my ardour, I was capable of a more intense\napplication and was more deeply smitten with the thirst for knowledge.\nShe busied herself with following the aerial'}
20:20:18,870 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:18,872 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:18,872 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:20,711 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:20,713 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:20,713 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:23,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.0835421659285203. input_tokens=34, output_tokens=1369
20:20:26,753 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:20:27,29 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:27,32 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:27,32 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:28,338 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:28,340 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:28,340 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:30,675 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:30,676 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:30,676 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:30,676 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19737, Requested 3549. Please try again in 9.858s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:20:30,680 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'our\nfuture prospects. The first of those sorrows which are sent to wean us from\nthe earth had visited her, and its dimming influence quenched her dearest\nsmiles.\n\n“When I reflect, my dear cousin,” said she, “on the miserable death of\nJustine Moritz, I no longer see the world and its works as they before\nappeared to me. Before, I looked upon the accounts of vice and\ninjustice that I read in books or heard from others as tales of ancient\ndays or imaginary evils; at least they were remote and more familiar to\nreason than to the imagination; but now misery has come home, and men\nappear to me as monsters thirsting for each other’s blood. Yet I am\ncertainly unjust. Everybody believed that poor girl to be guilty; and\nif she could have committed the crime for which she suffered, assuredly\nshe would have been the most depraved of human creatures. For the sake\nof a few jewels, to have murdered the son of her benefactor and friend,\na child whom she had nursed from its birth, and appeared to love as if\nit had been her own! I could not consent to the death of any human\nbeing, but certainly I should have thought such a creature unfit to\nremain in the society of men. But she was innocent. I know, I feel\nshe was innocent; you are of the same opinion, and that confirms me.\nAlas! Victor, when falsehood can look so like the truth, who can\nassure themselves of certain happiness? I feel as if I were walking on\nthe edge of a precipice, towards which thousands are crowding and\nendeavouring to plunge me into the abyss. William and Justine were\nassassinated, and the murderer escapes; he walks about the world free,\nand perhaps respected. But even if I were condemned to suffer on the\nscaffold for the same crimes, I would not change places with such a\nwretch.”\n\nI listened to this discourse with the extremest agony. I, not in deed,\nbut in effect, was the true murderer. Elizabeth read my anguish in my\ncountenance, and kindly taking my hand, said, “My dearest friend, you\nmust calm yourself. These events have affected me, God knows how\ndeeply; but I am not so wretched as you are. There is an expression of\ndespair, and sometimes of revenge, in your countenance that makes me\ntremble. Dear Victor, banish these dark passions. Remember the\nfriends around you, who centre all their hopes in you. Have we lost\nthe power of rendering you happy? Ah! While we love, while we are\ntrue to each other, here in this land of peace and beauty, your native\ncountry, we may reap every tranquil blessing—what can disturb our\npeace?”\n\nAnd could not such words from her whom I fondly prized before every\nother gift of fortune suffice to chase away the fiend that lurked in my\nheart? Even as she spoke I drew near to her, as if in terror, lest at\nthat very moment the destroyer had been near to rob me of her.\n\nThus not the tenderness of friendship, nor the beauty of earth, nor of\nheaven, could redeem my soul from woe; the very accents of love were\nineffectual. I was encompassed by a cloud which no beneficial\ninfluence could penetrate. The wounded deer dragging its fainting\nlimbs to some untrodden brake, there to gaze upon the arrow which had\npierced it, and to die, was but a type of me.\n\nSometimes I could cope with the sullen despair that overwhelmed me, but\nsometimes the whirlwind passions of my soul drove me to seek, by bodily\nexercise and by change of place, some relief from my intolerable\nsensations. It was during an access of this kind that I suddenly left\nmy home, and bending my steps towards the near Alpine valleys, sought\nin the magnificence, the eternity of such scenes, to forget myself and\nmy ephemeral, because human, sorrows. My wanderings were directed\ntowards the valley of Chamounix. I had visited it frequently during my\nboyhood. Six years had passed since then: _I_ was a wreck, but nought\nhad changed in those savage and enduring scenes.\n\nI performed the first part of my journey on horseback. I afterwards\nhired a mule, as the more sure-footed and least liable to receive\ninjury on these rugged roads. The weather was fine; it was about the\nmiddle of the month of August, nearly two months after the death of\nJustine, that miserable epoch from which I dated all my woe. The\nweight upon my spirit was sensibly lightened as I plunged yet deeper in\nthe ravine of Arve. The immense mountains and precipices that overhung\nme on every side, the sound of the river raging among the rocks, and\nthe dashing of the waterfalls around spoke of a power mighty as\nOmnipotence—and I ceased to fear or to bend before any being less\nalmighty than that which had created and ruled the elements, here\ndisplayed in their most terrific guise. Still, as I ascended higher,\nthe valley assumed a more magnificent and astonishing character.\nRuined castles hanging on the precipices of piny mountains, the\nimpetuous Arve, and cottages every here and there peeping forth from\namong the trees formed a scene of singular beauty. But it was\naugmented and rendered sublime by the mighty Alps, whose white and\nshining pyramids and domes towered above all,'}
20:20:31,425 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:31,427 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:31,427 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:34,48 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:34,50 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:34,50 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:36,440 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:36,442 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:36,442 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:36,442 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17808, Requested 3629. Please try again in 4.311s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:20:36,445 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'misery, for when I would\naccount to myself for the birth of that passion which afterwards ruled my\ndestiny I find it arise, like a mountain river, from ignoble and almost\nforgotten sources; but, swelling as it proceeded, it became the torrent\nwhich, in its course, has swept away all my hopes and joys.\n\nNatural philosophy is the genius that has regulated my fate; I desire,\ntherefore, in this narration, to state those facts which led to my\npredilection for that science. When I was thirteen years of age we all went\non a party of pleasure to the baths near Thonon; the inclemency of the\nweather obliged us to remain a day confined to the inn. In this house I\nchanced to find a volume of the works of Cornelius Agrippa. I opened it\nwith apathy; the theory which he attempts to demonstrate and the wonderful\nfacts which he relates soon changed this feeling into enthusiasm. A new\nlight seemed to dawn upon my mind, and bounding with joy, I communicated my\ndiscovery to my father. My father looked carelessly at the title page of my\nbook and said, “Ah! Cornelius Agrippa! My dear Victor, do not waste\nyour time upon this; it is sad trash.”\n\nIf, instead of this remark, my father had taken the pains to explain to me\nthat the principles of Agrippa had been entirely exploded and that a modern\nsystem of science had been introduced which possessed much greater powers\nthan the ancient, because the powers of the latter were chimerical, while\nthose of the former were real and practical, under such circumstances I\nshould certainly have thrown Agrippa aside and have contented my\nimagination, warmed as it was, by returning with greater ardour to my\nformer studies. It is even possible that the train of my ideas would never\nhave received the fatal impulse that led to my ruin. But the cursory glance\nmy father had taken of my volume by no means assured me that he was\nacquainted with its contents, and I continued to read with the greatest\navidity.\n\nWhen I returned home my first care was to procure the whole works of this\nauthor, and afterwards of Paracelsus and Albertus Magnus. I read and\nstudied the wild fancies of these writers with delight; they appeared to me\ntreasures known to few besides myself. I have described myself as always\nhaving been imbued with a fervent longing to penetrate the secrets of\nnature. In spite of the intense labour and wonderful discoveries of modern\nphilosophers, I always came from my studies discontented and unsatisfied.\nSir Isaac Newton is said to have avowed that he felt like a child picking\nup shells beside the great and unexplored ocean of truth. Those of his\nsuccessors in each branch of natural philosophy with whom I was acquainted\nappeared even to my boy’s apprehensions as tyros engaged in the same\npursuit.\n\nThe untaught peasant beheld the elements around him and was acquainted\nwith their practical uses. The most learned philosopher knew little\nmore. He had partially unveiled the face of Nature, but her immortal\nlineaments were still a wonder and a mystery. He might dissect,\nanatomise, and give names; but, not to speak of a final cause, causes\nin their secondary and tertiary grades were utterly unknown to him. I\nhad gazed upon the fortifications and impediments that seemed to keep\nhuman beings from entering the citadel of nature, and rashly and\nignorantly I had repined.\n\nBut here were books, and here were men who had penetrated deeper and knew\nmore. I took their word for all that they averred, and I became their\ndisciple. It may appear strange that such should arise in the eighteenth\ncentury; but while I followed the routine of education in the schools of\nGeneva, I was, to a great degree, self-taught with regard to my favourite\nstudies. My father was not scientific, and I was left to struggle with a\nchild’s blindness, added to a student’s thirst for knowledge.\nUnder the guidance of my new preceptors I entered with the greatest\ndiligence into the search of the philosopher’s stone and the elixir\nof life; but the latter soon obtained my undivided attention. Wealth was an\ninferior object, but what glory would attend the discovery if I could\nbanish disease from the human frame and render man invulnerable to any but\na violent death!\n\nNor were these my only visions. The raising of ghosts or devils was a\npromise liberally accorded by my favourite authors, the fulfilment of which\nI most eagerly sought; and if my incantations were always unsuccessful, I\nattributed the failure rather to my own inexperience and mistake than to a\nwant of skill or fidelity in my instructors. And thus for a time I was\noccupied by exploded systems, mingling, like an unadept, a thousand\ncontradictory theories and floundering desperately in a very slough of\nmultifarious knowledge, guided by an ardent imagination and childish\nreasoning, till an accident again changed the current of my ideas.\n\nWhen I was about fifteen years old we had retired to our house near\nBelrive, when we witnessed a most violent and terrible thunderstorm. It\nadvanced from behind the mountains of Jura, and the thunder burst at once\nwith frightful loudness from various quarters of the heavens. I remained,\nwhile the storm lasted, watching its progress with curiosity and delight.\nAs I stood at the door, on a sudden I beheld a stream of fire issue from an\nold and beautiful oak which'}
20:20:36,717 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:36,719 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:36,719 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:38,315 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:38,317 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:38,317 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:43,460 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:20:43,744 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:43,745 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:43,746 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:45,394 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:45,395 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:45,395 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:48,7 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:48,8 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:48,8 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:48,8 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19335, Requested 3582. Please try again in 8.751s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:20:48,11 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'indeed always the\neffect of solemnising my mind and causing me to forget the passing\ncares of life. I determined to go without a guide, for I was well\nacquainted with the path, and the presence of another would destroy the\nsolitary grandeur of the scene.\n\nThe ascent is precipitous, but the path is cut into continual and short\nwindings, which enable you to surmount the perpendicularity of the\nmountain. It is a scene terrifically desolate. In a thousand spots\nthe traces of the winter avalanche may be perceived, where trees lie\nbroken and strewed on the ground, some entirely destroyed, others bent,\nleaning upon the jutting rocks of the mountain or transversely upon\nother trees. The path, as you ascend higher, is intersected by ravines\nof snow, down which stones continually roll from above; one of them is\nparticularly dangerous, as the slightest sound, such as even speaking\nin a loud voice, produces a concussion of air sufficient to draw\ndestruction upon the head of the speaker. The pines are not tall or\nluxuriant, but they are sombre and add an air of severity to the scene.\nI looked on the valley beneath; vast mists were rising from the rivers\nwhich ran through it and curling in thick wreaths around the opposite\nmountains, whose summits were hid in the uniform clouds, while rain\npoured from the dark sky and added to the melancholy impression I\nreceived from the objects around me. Alas! Why does man boast of\nsensibilities superior to those apparent in the brute; it only renders\nthem more necessary beings. If our impulses were confined to hunger,\nthirst, and desire, we might be nearly free; but now we are moved by\nevery wind that blows and a chance word or scene that that word may\nconvey to us.\n\n  We rest; a dream has power to poison sleep.\n     We rise; one wand’ring thought pollutes the day.\n  We feel, conceive, or reason; laugh or weep,\n     Embrace fond woe, or cast our cares away;\n  It is the same: for, be it joy or sorrow,\n     The path of its departure still is free.\n  Man’s yesterday may ne’er be like his morrow;\n     Nought may endure but mutability!\n\n\nIt was nearly noon when I arrived at the top of the ascent. For some\ntime I sat upon the rock that overlooks the sea of ice. A mist covered\nboth that and the surrounding mountains. Presently a breeze dissipated\nthe cloud, and I descended upon the glacier. The surface is very\nuneven, rising like the waves of a troubled sea, descending low, and\ninterspersed by rifts that sink deep. The field of ice is almost a\nleague in width, but I spent nearly two hours in crossing it. The\nopposite mountain is a bare perpendicular rock. From the side where I\nnow stood Montanvert was exactly opposite, at the distance of a league;\nand above it rose Mont Blanc, in awful majesty. I remained in a recess\nof the rock, gazing on this wonderful and stupendous scene. The sea,\nor rather the vast river of ice, wound among its dependent mountains,\nwhose aerial summits hung over its recesses. Their icy and glittering\npeaks shone in the sunlight over the clouds. My heart, which was\nbefore sorrowful, now swelled with something like joy; I exclaimed,\n“Wandering spirits, if indeed ye wander, and do not rest in your narrow\nbeds, allow me this faint happiness, or take me, as your companion,\naway from the joys of life.”\n\nAs I said this I suddenly beheld the figure of a man, at some distance,\nadvancing towards me with superhuman speed. He bounded over the\ncrevices in the ice, among which I had walked with caution; his\nstature, also, as he approached, seemed to exceed that of man. I was\ntroubled; a mist came over my eyes, and I felt a faintness seize me,\nbut I was quickly restored by the cold gale of the mountains. I\nperceived, as the shape came nearer (sight tremendous and abhorred!)\nthat it was the wretch whom I had created. I trembled with rage and\nhorror, resolving to wait his approach and then close with him in\nmortal combat. He approached; his countenance bespoke bitter anguish,\ncombined with disdain and malignity, while its unearthly ugliness\nrendered it almost too horrible for human eyes. But I scarcely\nobserved this; rage and hatred had at first deprived me of utterance,\nand I recovered only to overwhelm him with words expressive of furious\ndetestation and contempt.\n\n“Devil,” I exclaimed, “do you dare approach me? And do\nnot you fear the fierce vengeance of my arm wreaked on your miserable head?\nBegone, vile insect! Or rather, stay, that I may trample you to dust! And,\noh! That I could, with the extinction of your miserable existence, restore\nthose victims whom you have so diabolically murdered!”\n\n“I expected this reception,” said the dæmon. “All men hate the\nwretched; how, then, must I be hated, who am miserable beyond all\nliving things! Yet you, my creator, detest and spurn me, thy creature,\nto whom thou art bound by ties only dissoluble by the annihilation of\none of us. You purpose to kill me. How dare you sport thus with life?\nDo your duty towards me, and I will do mine towards you and the rest of'}
20:20:48,500 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:48,501 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:48,501 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:50,327 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:50,329 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:50,329 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:52,694 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:52,696 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:52,696 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:52,696 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17775, Requested 3462. Please try again in 3.711s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:20:52,699 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'skims over the page to seek the words\nwhich are to convey to you the horrible tidings.\n\n“William is dead!—that sweet child, whose smiles delighted and warmed\nmy heart, who was so gentle, yet so gay! Victor, he is murdered!\n\n“I will not attempt to console you; but will simply relate the\ncircumstances of the transaction.\n\n“Last Thursday (May 7th), I, my niece, and your two brothers, went to\nwalk in Plainpalais. The evening was warm and serene, and we prolonged\nour walk farther than usual. It was already dusk before we thought of\nreturning; and then we discovered that William and Ernest, who had gone\non before, were not to be found. We accordingly rested on a seat until\nthey should return. Presently Ernest came, and enquired if we had seen\nhis brother; he said, that he had been playing with him, that William\nhad run away to hide himself, and that he vainly sought for him, and\nafterwards waited for a long time, but that he did not return.\n\n“This account rather alarmed us, and we continued to search for him\nuntil night fell, when Elizabeth conjectured that he might have\nreturned to the house. He was not there. We returned again, with\ntorches; for I could not rest, when I thought that my sweet boy had\nlost himself, and was exposed to all the damps and dews of night;\nElizabeth also suffered extreme anguish. About five in the morning I\ndiscovered my lovely boy, whom the night before I had seen blooming and\nactive in health, stretched on the grass livid and motionless; the\nprint of the murder’s finger was on his neck.\n\n“He was conveyed home, and the anguish that was visible in my\ncountenance betrayed the secret to Elizabeth. She was very earnest to\nsee the corpse. At first I attempted to prevent her but she persisted,\nand entering the room where it lay, hastily examined the neck of the\nvictim, and clasping her hands exclaimed, ‘O God! I have murdered my\ndarling child!’\n\n“She fainted, and was restored with extreme difficulty. When she again\nlived, it was only to weep and sigh. She told me, that that same\nevening William had teased her to let him wear a very valuable\nminiature that she possessed of your mother. This picture is gone, and\nwas doubtless the temptation which urged the murderer to the deed. We\nhave no trace of him at present, although our exertions to discover him\nare unremitted; but they will not restore my beloved William!\n\n“Come, dearest Victor; you alone can console Elizabeth. She weeps\ncontinually, and accuses herself unjustly as the cause of his death;\nher words pierce my heart. We are all unhappy; but will not that be an\nadditional motive for you, my son, to return and be our comforter?\nYour dear mother! Alas, Victor! I now say, Thank God she did not live\nto witness the cruel, miserable death of her youngest darling!\n\n“Come, Victor; not brooding thoughts of vengeance against the assassin,\nbut with feelings of peace and gentleness, that will heal, instead of\nfestering, the wounds of our minds. Enter the house of mourning, my\nfriend, but with kindness and affection for those who love you, and not\nwith hatred for your enemies.\n\n“Your affectionate and afflicted father,\n\n“Alphonse Frankenstein.\n\n\n\n“Geneva, May 12th, 17—.”\n\n\n\nClerval, who had watched my countenance as I read this letter, was\nsurprised to observe the despair that succeeded the joy I at first\nexpressed on receiving new from my friends. I threw the letter on the\ntable, and covered my face with my hands.\n\n“My dear Frankenstein,” exclaimed Henry, when he perceived me\nweep with bitterness, “are you always to be unhappy? My dear friend,\nwhat has happened?”\n\nI motioned him to take up the letter, while I walked up and down the\nroom in the extremest agitation. Tears also gushed from the eyes of\nClerval, as he read the account of my misfortune.\n\n“I can offer you no consolation, my friend,” said he;\n“your disaster is irreparable. What do you intend to do?”\n\n“To go instantly to Geneva: come with me, Henry, to order the horses.”\n\nDuring our walk, Clerval endeavoured to say a few words of consolation;\nhe could only express his heartfelt sympathy. “Poor William!” said he,\n“dear lovely child, he now sleeps with his angel mother! Who that had\nseen him bright and joyous in his young beauty, but must weep over his\nuntimely loss! To die so miserably; to feel the murderer’s grasp! How\nmuch more a murdered that could destroy radiant innocence! Poor little\nfellow! one only consolation have we; his friends mourn and weep, but\nhe is at rest. The pang is over, his sufferings are at an end for ever.\nA sod covers his gentle form, and he knows no pain. He can no longer\nbe a subject for pity; we must reserve that for his miserable\nsurvivors.”\n\nClerval spoke thus as we hurried through the streets; the words\nimpressed themselves on my mind and I remembered them afterwards in\nsolitude. But now, as soon as the horses arrived, I hurried into a\ncabriolet, and bade farewell to my friend.\n\nMy journey was very melancholy. At first I wished to hurry on, for I longed\nto console and sympathise with my loved and sorrowing friends;'}
20:20:52,978 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:52,980 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:52,980 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:20:55,184 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:20:55,185 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:20:55,185 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:01,290 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:21:01,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.025479917996563. input_tokens=34, output_tokens=2055
20:21:01,606 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:01,608 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:01,608 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:03,437 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:03,439 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:03,440 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:06,55 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:06,57 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:06,57 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:06,58 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19177, Requested 3655. Please try again in 8.496s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:21:06,61 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'was delighted\nwhen I first discovered that a pleasant sound, which often saluted my\nears, proceeded from the throats of the little winged animals who had\noften intercepted the light from my eyes. I began also to observe,\nwith greater accuracy, the forms that surrounded me and to perceive the\nboundaries of the radiant roof of light which canopied me. Sometimes I\ntried to imitate the pleasant songs of the birds but was unable.\nSometimes I wished to express my sensations in my own mode, but the\nuncouth and inarticulate sounds which broke from me frightened me into\nsilence again.\n\n“The moon had disappeared from the night, and again, with a lessened\nform, showed itself, while I still remained in the forest. My\nsensations had by this time become distinct, and my mind received every\nday additional ideas. My eyes became accustomed to the light and to\nperceive objects in their right forms; I distinguished the insect from\nthe herb, and by degrees, one herb from another. I found that the\nsparrow uttered none but harsh notes, whilst those of the blackbird and\nthrush were sweet and enticing.\n\n“One day, when I was oppressed by cold, I found a fire which had been\nleft by some wandering beggars, and was overcome with delight at the\nwarmth I experienced from it. In my joy I thrust my hand into the live\nembers, but quickly drew it out again with a cry of pain. How strange,\nI thought, that the same cause should produce such opposite effects! I\nexamined the materials of the fire, and to my joy found it to be\ncomposed of wood. I quickly collected some branches, but they were wet\nand would not burn. I was pained at this and sat still watching the\noperation of the fire. The wet wood which I had placed near the heat\ndried and itself became inflamed. I reflected on this, and by touching\nthe various branches, I discovered the cause and busied myself in\ncollecting a great quantity of wood, that I might dry it and have a\nplentiful supply of fire. When night came on and brought sleep with\nit, I was in the greatest fear lest my fire should be extinguished. I\ncovered it carefully with dry wood and leaves and placed wet branches\nupon it; and then, spreading my cloak, I lay on the ground and sank\ninto sleep.\n\n“It was morning when I awoke, and my first care was to visit the fire.\nI uncovered it, and a gentle breeze quickly fanned it into a flame. I\nobserved this also and contrived a fan of branches, which roused the\nembers when they were nearly extinguished. When night came again I\nfound, with pleasure, that the fire gave light as well as heat and that\nthe discovery of this element was useful to me in my food, for I found\nsome of the offals that the travellers had left had been roasted, and\ntasted much more savoury than the berries I gathered from the trees. I\ntried, therefore, to dress my food in the same manner, placing it on\nthe live embers. I found that the berries were spoiled by this\noperation, and the nuts and roots much improved.\n\n“Food, however, became scarce, and I often spent the whole day\nsearching in vain for a few acorns to assuage the pangs of hunger. When\nI found this, I resolved to quit the place that I had hitherto\ninhabited, to seek for one where the few wants I experienced would be\nmore easily satisfied. In this emigration I exceedingly lamented the\nloss of the fire which I had obtained through accident and knew not how\nto reproduce it. I gave several hours to the serious consideration of\nthis difficulty, but I was obliged to relinquish all attempt to supply\nit, and wrapping myself up in my cloak, I struck across the wood\ntowards the setting sun. I passed three days in these rambles and at\nlength discovered the open country. A great fall of snow had taken\nplace the night before, and the fields were of one uniform white; the\nappearance was disconsolate, and I found my feet chilled by the cold\ndamp substance that covered the ground.\n\n“It was about seven in the morning, and I longed to obtain food and\nshelter; at length I perceived a small hut, on a rising ground, which\nhad doubtless been built for the convenience of some shepherd. This\nwas a new sight to me, and I examined the structure with great\ncuriosity. Finding the door open, I entered. An old man sat in it,\nnear a fire, over which he was preparing his breakfast. He turned on\nhearing a noise, and perceiving me, shrieked loudly, and quitting the\nhut, ran across the fields with a speed of which his debilitated form\nhardly appeared capable. His appearance, different from any I had ever\nbefore seen, and his flight somewhat surprised me. But I was enchanted\nby the appearance of the hut; here the snow and rain could not\npenetrate; the ground was dry; and it presented to me then as exquisite\nand divine a retreat as Pandæmonium appeared to the dæmons of hell\nafter their sufferings in the lake of fire. I greedily devoured the\nremnants of the shepherd’s breakfast, which consisted of bread, cheese,\nmilk, and wine; the latter, however, I did not like. Then, overcome by\nfatigue, I lay down among some straw and fell asleep.\n\n“It was noon when I awoke, and allured by the warmth of the sun'}
20:21:06,346 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:06,348 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:06,348 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:08,20 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:08,22 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:08,22 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:11,71 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:11,74 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:11,74 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:11,74 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17504, Requested 4066. Please try again in 4.71s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:21:11,78 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'divine a retreat as Pandæmonium appeared to the dæmons of hell\nafter their sufferings in the lake of fire. I greedily devoured the\nremnants of the shepherd’s breakfast, which consisted of bread, cheese,\nmilk, and wine; the latter, however, I did not like. Then, overcome by\nfatigue, I lay down among some straw and fell asleep.\n\n“It was noon when I awoke, and allured by the warmth of the sun, which\nshone brightly on the white ground, I determined to recommence my\ntravels; and, depositing the remains of the peasant’s breakfast in a\nwallet I found, I proceeded across the fields for several hours, until\nat sunset I arrived at a village. How miraculous did this appear! The\nhuts, the neater cottages, and stately houses engaged my admiration by\nturns. The vegetables in the gardens, the milk and cheese that I saw\nplaced at the windows of some of the cottages, allured my appetite. One\nof the best of these I entered, but I had hardly placed my foot within\nthe door before the children shrieked, and one of the women fainted.\nThe whole village was roused; some fled, some attacked me, until,\ngrievously bruised by stones and many other kinds of missile weapons, I\nescaped to the open country and fearfully took refuge in a low hovel,\nquite bare, and making a wretched appearance after the palaces I had\nbeheld in the village. This hovel however, joined a cottage of a neat\nand pleasant appearance, but after my late dearly bought experience, I\ndared not enter it. My place of refuge was constructed of wood, but so\nlow that I could with difficulty sit upright in it. No wood, however,\nwas placed on the earth, which formed the floor, but it was dry; and\nalthough the wind entered it by innumerable chinks, I found it an\nagreeable asylum from the snow and rain.\n\n“Here, then, I retreated and lay down happy to have found a shelter,\nhowever miserable, from the inclemency of the season, and still more\nfrom the barbarity of man. As soon as morning dawned I crept from my\nkennel, that I might view the adjacent cottage and discover if I could\nremain in the habitation I had found. It was situated against the back\nof the cottage and surrounded on the sides which were exposed by a pig\nsty and a clear pool of water. One part was open, and by that I had\ncrept in; but now I covered every crevice by which I might be perceived\nwith stones and wood, yet in such a manner that I might move them on\noccasion to pass out; all the light I enjoyed came through the sty, and\nthat was sufficient for me.\n\n“Having thus arranged my dwelling and carpeted it with clean straw, I\nretired, for I saw the figure of a man at a distance, and I remembered\ntoo well my treatment the night before to trust myself in his power. I\nhad first, however, provided for my sustenance for that day by a loaf\nof coarse bread, which I purloined, and a cup with which I could drink\nmore conveniently than from my hand of the pure water which flowed by\nmy retreat. The floor was a little raised, so that it was kept\nperfectly dry, and by its vicinity to the chimney of the cottage it was\ntolerably warm.\n\n“Being thus provided, I resolved to reside in this hovel until\nsomething should occur which might alter my determination. It was\nindeed a paradise compared to the bleak forest, my former residence,\nthe rain-dropping branches, and dank earth. I ate my breakfast with\npleasure and was about to remove a plank to procure myself a little\nwater when I heard a step, and looking through a small chink, I beheld\na young creature, with a pail on her head, passing before my hovel. The\ngirl was young and of gentle demeanour, unlike what I have since found\ncottagers and farmhouse servants to be. Yet she was meanly dressed, a\ncoarse blue petticoat and a linen jacket being her only garb; her fair\nhair was plaited but not adorned: she looked patient yet sad. I lost\nsight of her, and in about a quarter of an hour she returned bearing\nthe pail, which was now partly filled with milk. As she walked along,\nseemingly incommoded by the burden, a young man met her, whose\ncountenance expressed a deeper despondence. Uttering a few sounds with\nan air of melancholy, he took the pail from her head and bore it to the\ncottage himself. She followed, and they disappeared. Presently I saw\nthe young man again, with some tools in his hand, cross the field\nbehind the cottage; and the girl was also busied, sometimes in the\nhouse and sometimes in the yard.\n\n“On examining my dwelling, I found that one of the windows of the\ncottage had formerly occupied a part of it, but the panes had been\nfilled up with wood. In one of these was a small and almost\nimperceptible chink through which the eye could just penetrate.\nThrough this crevice a small room was visible, whitewashed and clean\nbut very bare of furniture. In one corner, near a small fire, sat an\nold man, leaning his head on his hands in a disconsolate attitude. The\nyoung girl was occupied in arranging the cottage; but presently she\ntook'}
20:21:11,336 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:11,338 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:11,338 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:13,142 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:13,144 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:13,144 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:18,496 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:21:18,909 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:18,911 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:18,911 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:21,149 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:21,151 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:21,151 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:24,151 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:24,153 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:24,153 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:24,153 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 18382, Requested 4457. Please try again in 8.516s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:21:24,158 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'tears, which she endeavoured to wipe away unperceived; but I\ngenerally found that her countenance and tone were more cheerful after\nhaving listened to the exhortations of her father. It was not thus\nwith Felix. He was always the saddest of the group, and even to my\nunpractised senses, he appeared to have suffered more deeply than his\nfriends. But if his countenance was more sorrowful, his voice was more\ncheerful than that of his sister, especially when he addressed the old\nman.\n\n“I could mention innumerable instances which, although slight, marked\nthe dispositions of these amiable cottagers. In the midst of poverty\nand want, Felix carried with pleasure to his sister the first little\nwhite flower that peeped out from beneath the snowy ground. Early in\nthe morning, before she had risen, he cleared away the snow that\nobstructed her path to the milk-house, drew water from the well, and\nbrought the wood from the outhouse, where, to his perpetual\nastonishment, he found his store always replenished by an invisible\nhand. In the day, I believe, he worked sometimes for a neighbouring\nfarmer, because he often went forth and did not return until dinner,\nyet brought no wood with him. At other times he worked in the garden,\nbut as there was little to do in the frosty season, he read to the old\nman and Agatha.\n\n“This reading had puzzled me extremely at first, but by degrees I\ndiscovered that he uttered many of the same sounds when he read as when\nhe talked. I conjectured, therefore, that he found on the paper signs\nfor speech which he understood, and I ardently longed to comprehend\nthese also; but how was that possible when I did not even understand\nthe sounds for which they stood as signs? I improved, however,\nsensibly in this science, but not sufficiently to follow up any kind of\nconversation, although I applied my whole mind to the endeavour, for I\neasily perceived that, although I eagerly longed to discover myself to\nthe cottagers, I ought not to make the attempt until I had first become\nmaster of their language, which knowledge might enable me to make them\noverlook the deformity of my figure, for with this also the contrast\nperpetually presented to my eyes had made me acquainted.\n\n“I had admired the perfect forms of my cottagers—their grace, beauty,\nand delicate complexions; but how was I terrified when I viewed myself\nin a transparent pool! At first I started back, unable to believe that\nit was indeed I who was reflected in the mirror; and when I became\nfully convinced that I was in reality the monster that I am, I was\nfilled with the bitterest sensations of despondence and mortification.\nAlas! I did not yet entirely know the fatal effects of this miserable\ndeformity.\n\n“As the sun became warmer and the light of day longer, the snow\nvanished, and I beheld the bare trees and the black earth. From this\ntime Felix was more employed, and the heart-moving indications of\nimpending famine disappeared. Their food, as I afterwards found, was\ncoarse, but it was wholesome; and they procured a sufficiency of it.\nSeveral new kinds of plants sprang up in the garden, which they\ndressed; and these signs of comfort increased daily as the season\nadvanced.\n\n“The old man, leaning on his son, walked each day at noon, when it did\nnot rain, as I found it was called when the heavens poured forth its\nwaters. This frequently took place, but a high wind quickly dried the\nearth, and the season became far more pleasant than it had been.\n\n“My mode of life in my hovel was uniform. During the morning I\nattended the motions of the cottagers, and when they were dispersed in\nvarious occupations, I slept; the remainder of the day was spent in\nobserving my friends. When they had retired to rest, if there was any\nmoon or the night was star-light, I went into the woods and collected\nmy own food and fuel for the cottage. When I returned, as often as it\nwas necessary, I cleared their path from the snow and performed those\noffices that I had seen done by Felix. I afterwards found that these\nlabours, performed by an invisible hand, greatly astonished them; and\nonce or twice I heard them, on these occasions, utter the words _good\nspirit, wonderful_; but I did not then understand the signification\nof these terms.\n\n“My thoughts now became more active, and I longed to discover the\nmotives and feelings of these lovely creatures; I was inquisitive to\nknow why Felix appeared so miserable and Agatha so sad. I thought\n(foolish wretch!) that it might be in my power to restore happiness to\nthese deserving people. When I slept or was absent, the forms of the\nvenerable blind father, the gentle Agatha, and the excellent Felix\nflitted before me. I looked upon them as superior beings who would be\nthe arbiters of my future destiny. I formed in my imagination a\nthousand pictures of presenting myself to them, and their reception of\nme. I imagined that they would be disgusted, until, by my gentle\ndemeanour and conciliating words, I should first win their favour and\nafterwards their love.\n\n“These thoughts exhilarated me and led me to apply with fresh ardour to\nthe acquiring the art of language. My organs were indeed harsh, but\nsupple; and although my voice was very unlike the soft music of their\ntones, yet'}
20:21:24,406 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:24,408 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:24,408 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:25,962 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:25,964 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:25,964 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:28,996 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:28,998 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:28,998 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:28,999 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 16703, Requested 3396. Please try again in 295ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:21:29,2 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'ties are rent\nby that most irreparable evil, the void that presents itself to the\nsoul, and the despair that is exhibited on the countenance. It is so\nlong before the mind can persuade itself that she whom we saw every day\nand whose very existence appeared a part of our own can have departed\nfor ever—that the brightness of a beloved eye can have been\nextinguished and the sound of a voice so familiar and dear to the ear\ncan be hushed, never more to be heard. These are the reflections of\nthe first days; but when the lapse of time proves the reality of the\nevil, then the actual bitterness of grief commences. Yet from whom has\nnot that rude hand rent away some dear connection? And why should I\ndescribe a sorrow which all have felt, and must feel? The time at\nlength arrives when grief is rather an indulgence than a necessity; and\nthe smile that plays upon the lips, although it may be deemed a\nsacrilege, is not banished. My mother was dead, but we had still\nduties which we ought to perform; we must continue our course with the\nrest and learn to think ourselves fortunate whilst one remains whom the\nspoiler has not seized.\n\nMy departure for Ingolstadt, which had been deferred by these events,\nwas now again determined upon. I obtained from my father a respite of\nsome weeks. It appeared to me sacrilege so soon to leave the repose,\nakin to death, of the house of mourning and to rush into the thick of\nlife. I was new to sorrow, but it did not the less alarm me. I was\nunwilling to quit the sight of those that remained to me, and above\nall, I desired to see my sweet Elizabeth in some degree consoled.\n\nShe indeed veiled her grief and strove to act the comforter to us all.\nShe looked steadily on life and assumed its duties with courage and\nzeal. She devoted herself to those whom she had been taught to call\nher uncle and cousins. Never was she so enchanting as at this time,\nwhen she recalled the sunshine of her smiles and spent them upon us.\nShe forgot even her own regret in her endeavours to make us forget.\n\nThe day of my departure at length arrived. Clerval spent the last\nevening with us. He had endeavoured to persuade his father to permit\nhim to accompany me and to become my fellow student, but in vain. His\nfather was a narrow-minded trader and saw idleness and ruin in the\naspirations and ambition of his son. Henry deeply felt the misfortune\nof being debarred from a liberal education. He said little, but when\nhe spoke I read in his kindling eye and in his animated glance a\nrestrained but firm resolve not to be chained to the miserable details\nof commerce.\n\nWe sat late. We could not tear ourselves away from each other nor\npersuade ourselves to say the word “Farewell!” It was said, and we\nretired under the pretence of seeking repose, each fancying that the\nother was deceived; but when at morning’s dawn I descended to the\ncarriage which was to convey me away, they were all there—my father\nagain to bless me, Clerval to press my hand once more, my Elizabeth to\nrenew her entreaties that I would write often and to bestow the last\nfeminine attentions on her playmate and friend.\n\nI threw myself into the chaise that was to convey me away and indulged in\nthe most melancholy reflections. I, who had ever been surrounded by\namiable companions, continually engaged in endeavouring to bestow mutual\npleasure—I was now alone. In the university whither I was going I\nmust form my own friends and be my own protector. My life had hitherto\nbeen remarkably secluded and domestic, and this had given me invincible\nrepugnance to new countenances. I loved my brothers, Elizabeth, and\nClerval; these were “old familiar faces,” but I believed myself\ntotally unfitted for the company of strangers. Such were my reflections as\nI commenced my journey; but as I proceeded, my spirits and hopes rose. I\nardently desired the acquisition of knowledge. I had often, when at home,\nthought it hard to remain during my youth cooped up in one place and had\nlonged to enter the world and take my station among other human beings. \nNow my desires were complied with, and it would, indeed, have been folly to\nrepent.\n\nI had sufficient leisure for these and many other reflections during my\njourney to Ingolstadt, which was long and fatiguing. At length the\nhigh white steeple of the town met my eyes. I alighted and was\nconducted to my solitary apartment to spend the evening as I pleased.\n\nThe next morning I delivered my letters of introduction and paid a visit to\nsome of the principal professors. Chance—or rather the evil\ninfluence, the Angel of Destruction, which asserted omnipotent sway over me\nfrom the moment I turned my reluctant steps from my father’s\ndoor—led me first to M. Krempe, professor of natural philosophy. He\nwas an uncouth man, but deeply imbued in the secrets of his science. He\nasked me several questions concerning my progress in the different branches\nof science appertaining to natural philosophy. I replied carelessly, and\npartly in contempt, mentioned the names of my alchemists as the principal\nauthors I had studied. The professor stared. “Have you,” he\nsaid, “really spent your time in studying such nonsense?”\n\nI'}
20:21:29,394 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:29,395 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:29,395 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:31,493 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:31,495 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:31,495 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:37,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.002713711000979. input_tokens=34, output_tokens=1431
20:21:40,928 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:21:41,188 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:41,191 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:41,191 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:43,278 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:43,280 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:43,280 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:45,769 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:45,770 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:45,771 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:45,771 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 18048, Requested 3479. Please try again in 4.581s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:21:45,776 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'reserve that for his miserable\nsurvivors.”\n\nClerval spoke thus as we hurried through the streets; the words\nimpressed themselves on my mind and I remembered them afterwards in\nsolitude. But now, as soon as the horses arrived, I hurried into a\ncabriolet, and bade farewell to my friend.\n\nMy journey was very melancholy. At first I wished to hurry on, for I longed\nto console and sympathise with my loved and sorrowing friends; but when I\ndrew near my native town, I slackened my progress. I could hardly sustain\nthe multitude of feelings that crowded into my mind. I passed through\nscenes familiar to my youth, but which I had not seen for nearly six years.\nHow altered every thing might be during that time! One sudden and\ndesolating change had taken place; but a thousand little circumstances\nmight have by degrees worked other alterations, which, although they were\ndone more tranquilly, might not be the less decisive. Fear overcame me; I\ndared no advance, dreading a thousand nameless evils that made me tremble,\nalthough I was unable to define them.\n\nI remained two days at Lausanne, in this painful state of mind. I\ncontemplated the lake: the waters were placid; all around was calm; and the\nsnowy mountains, “the palaces of nature,” were not changed. By\ndegrees the calm and heavenly scene restored me, and I continued my journey\ntowards Geneva.\n\nThe road ran by the side of the lake, which became narrower as I\napproached my native town. I discovered more distinctly the black\nsides of Jura, and the bright summit of Mont Blanc. I wept like a\nchild. “Dear mountains! my own beautiful lake! how do you welcome your\nwanderer? Your summits are clear; the sky and lake are blue and\nplacid. Is this to prognosticate peace, or to mock at my unhappiness?”\n\nI fear, my friend, that I shall render myself tedious by dwelling on\nthese preliminary circumstances; but they were days of comparative\nhappiness, and I think of them with pleasure. My country, my beloved\ncountry! who but a native can tell the delight I took in again\nbeholding thy streams, thy mountains, and, more than all, thy lovely\nlake!\n\nYet, as I drew nearer home, grief and fear again overcame me. Night also\nclosed around; and when I could hardly see the dark mountains, I felt still\nmore gloomily. The picture appeared a vast and dim scene of evil, and I\nforesaw obscurely that I was destined to become the most wretched of human\nbeings. Alas! I prophesied truly, and failed only in one single\ncircumstance, that in all the misery I imagined and dreaded, I did not\nconceive the hundredth part of the anguish I was destined to endure.\n\nIt was completely dark when I arrived in the environs of Geneva; the gates\nof the town were already shut; and I was obliged to pass the night at\nSecheron, a village at the distance of half a league from the city. The sky\nwas serene; and, as I was unable to rest, I resolved to visit the spot\nwhere my poor William had been murdered. As I could not pass through the\ntown, I was obliged to cross the lake in a boat to arrive at Plainpalais.\nDuring this short voyage I saw the lightning playing on the summit of Mont\nBlanc in the most beautiful figures. The storm appeared to approach\nrapidly, and, on landing, I ascended a low hill, that I might observe its\nprogress. It advanced; the heavens were clouded, and I soon felt the rain\ncoming slowly in large drops, but its violence quickly increased.\n\nI quitted my seat, and walked on, although the darkness and storm\nincreased every minute, and the thunder burst with a terrific crash\nover my head. It was echoed from Salêve, the Juras, and the Alps of\nSavoy; vivid flashes of lightning dazzled my eyes, illuminating the\nlake, making it appear like a vast sheet of fire; then for an instant\nevery thing seemed of a pitchy darkness, until the eye recovered itself\nfrom the preceding flash. The storm, as is often the case in\nSwitzerland, appeared at once in various parts of the heavens. The\nmost violent storm hung exactly north of the town, over the part of the\nlake which lies between the promontory of Belrive and the village of\nCopêt. Another storm enlightened Jura with faint flashes; and another\ndarkened and sometimes disclosed the Môle, a peaked mountain to the\neast of the lake.\n\nWhile I watched the tempest, so beautiful yet terrific, I wandered on with\na hasty step. This noble war in the sky elevated my spirits; I clasped my\nhands, and exclaimed aloud, “William, dear angel! this is thy\nfuneral, this thy dirge!” As I said these words, I perceived in the\ngloom a figure which stole from behind a clump of trees near me; I stood\nfixed, gazing intently: I could not be mistaken. A flash of lightning\nilluminated the object, and discovered its shape plainly to me; its\ngigantic stature, and the deformity of its aspect more hideous than belongs\nto humanity, instantly informed me that it was the wretch, the filthy\ndæmon, to whom I had given life. What did he there? Could he be (I\nshuddered at the conception) the murderer of my brother?'}
20:21:46,171 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:46,173 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:46,173 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:47,742 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:47,743 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:47,743 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:50,602 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:50,604 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:50,604 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:50,604 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 16437, Requested 3737. Please try again in 520ms. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:21:50,608 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'professor of natural philosophy. He\nwas an uncouth man, but deeply imbued in the secrets of his science. He\nasked me several questions concerning my progress in the different branches\nof science appertaining to natural philosophy. I replied carelessly, and\npartly in contempt, mentioned the names of my alchemists as the principal\nauthors I had studied. The professor stared. “Have you,” he\nsaid, “really spent your time in studying such nonsense?”\n\nI replied in the affirmative. “Every minute,” continued M. Krempe with\nwarmth, “every instant that you have wasted on those books is utterly\nand entirely lost. You have burdened your memory with exploded systems\nand useless names. Good God! In what desert land have you lived,\nwhere no one was kind enough to inform you that these fancies which you\nhave so greedily imbibed are a thousand years old and as musty as they\nare ancient? I little expected, in this enlightened and scientific\nage, to find a disciple of Albertus Magnus and Paracelsus. My dear\nsir, you must begin your studies entirely anew.”\n\nSo saying, he stepped aside and wrote down a list of several books\ntreating of natural philosophy which he desired me to procure, and\ndismissed me after mentioning that in the beginning of the following\nweek he intended to commence a course of lectures upon natural\nphilosophy in its general relations, and that M. Waldman, a fellow\nprofessor, would lecture upon chemistry the alternate days that he\nomitted.\n\nI returned home not disappointed, for I have said that I had long\nconsidered those authors useless whom the professor reprobated; but I\nreturned not at all the more inclined to recur to these studies in any\nshape. M. Krempe was a little squat man with a gruff voice and a\nrepulsive countenance; the teacher, therefore, did not prepossess me in\nfavour of his pursuits. In rather a too philosophical and connected a\nstrain, perhaps, I have given an account of the conclusions I had come\nto concerning them in my early years. As a child I had not been\ncontent with the results promised by the modern professors of natural\nscience. With a confusion of ideas only to be accounted for by my\nextreme youth and my want of a guide on such matters, I had retrod the\nsteps of knowledge along the paths of time and exchanged the\ndiscoveries of recent inquirers for the dreams of forgotten alchemists.\nBesides, I had a contempt for the uses of modern natural philosophy.\nIt was very different when the masters of the science sought\nimmortality and power; such views, although futile, were grand; but now\nthe scene was changed. The ambition of the inquirer seemed to limit\nitself to the annihilation of those visions on which my interest in\nscience was chiefly founded. I was required to exchange chimeras of\nboundless grandeur for realities of little worth.\n\nSuch were my reflections during the first two or three days of my\nresidence at Ingolstadt, which were chiefly spent in becoming\nacquainted with the localities and the principal residents in my new\nabode. But as the ensuing week commenced, I thought of the information\nwhich M. Krempe had given me concerning the lectures. And although I\ncould not consent to go and hear that little conceited fellow deliver\nsentences out of a pulpit, I recollected what he had said of M.\nWaldman, whom I had never seen, as he had hitherto been out of town.\n\nPartly from curiosity and partly from idleness, I went into the lecturing\nroom, which M. Waldman entered shortly after. This professor was very\nunlike his colleague. He appeared about fifty years of age, but with an\naspect expressive of the greatest benevolence; a few grey hairs covered his\ntemples, but those at the back of his head were nearly black. His person\nwas short but remarkably erect and his voice the sweetest I had ever heard.\nHe began his lecture by a recapitulation of the history of chemistry and\nthe various improvements made by different men of learning, pronouncing\nwith fervour the names of the most distinguished discoverers. He then took\na cursory view of the present state of the science and explained many of\nits elementary terms. After having made a few preparatory experiments, he\nconcluded with a panegyric upon modern chemistry, the terms of which I\nshall never forget:\n\n“The ancient teachers of this science,” said he,\n“promised impossibilities and performed nothing. The modern masters\npromise very little; they know that metals cannot be transmuted and that\nthe elixir of life is a chimera but these philosophers, whose hands seem\nonly made to dabble in dirt, and their eyes to pore over the microscope or\ncrucible, have indeed performed miracles. They penetrate into the recesses\nof nature and show how she works in her hiding-places. They ascend into the\nheavens; they have discovered how the blood circulates, and the nature of\nthe air we breathe. They have acquired new and almost unlimited powers;\nthey can command the thunders of heaven, mimic the earthquake, and even\nmock the invisible world with its own shadows.”\n\nSuch were the professor’s words—rather let me say such the words of\nthe fate—enounced to destroy me. As he went on I felt as if my soul\nwere grappling with a palpable enemy; one by one the various keys were\ntouched which formed the mechanism of my being; chord after chord was\nsounded, and soon my mind was filled'}
20:21:54,231 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:21:54,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.6235184529796243. input_tokens=34, output_tokens=1805
20:21:54,559 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:54,560 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:54,560 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:56,248 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:56,250 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:56,250 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:58,753 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:58,755 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:58,755 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:21:58,755 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19176, Requested 3482. Please try again in 7.974s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:21:58,758 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'projects, but that on which I finally\nfixed was to enter the dwelling when the blind old man should be alone.\nI had sagacity enough to discover that the unnatural hideousness of my\nperson was the chief object of horror with those who had formerly\nbeheld me. My voice, although harsh, had nothing terrible in it; I\nthought, therefore, that if in the absence of his children I could gain\nthe good will and mediation of the old De Lacey, I might by his means\nbe tolerated by my younger protectors.\n\n“One day, when the sun shone on the red leaves that strewed the ground\nand diffused cheerfulness, although it denied warmth, Safie, Agatha,\nand Felix departed on a long country walk, and the old man, at his own\ndesire, was left alone in the cottage. When his children had departed,\nhe took up his guitar and played several mournful but sweet airs, more\nsweet and mournful than I had ever heard him play before. At first his\ncountenance was illuminated with pleasure, but as he continued,\nthoughtfulness and sadness succeeded; at length, laying aside the\ninstrument, he sat absorbed in reflection.\n\n“My heart beat quick; this was the hour and moment of trial, which\nwould decide my hopes or realise my fears. The servants were gone to a\nneighbouring fair. All was silent in and around the cottage; it was an\nexcellent opportunity; yet, when I proceeded to execute my plan, my\nlimbs failed me and I sank to the ground. Again I rose, and exerting\nall the firmness of which I was master, removed the planks which I had\nplaced before my hovel to conceal my retreat. The fresh air revived\nme, and with renewed determination I approached the door of their\ncottage.\n\n“I knocked. ‘Who is there?’ said the old man. ‘Come in.’\n\n“I entered. ‘Pardon this intrusion,’ said I; ‘I am\na traveller in want of a little rest; you would greatly oblige me if you\nwould allow me to remain a few minutes before the fire.’\n\n“‘Enter,’ said De Lacey, ‘and I will try in what\nmanner I can to relieve your wants; but, unfortunately, my children are\nfrom home, and as I am blind, I am afraid I shall find it difficult to\nprocure food for you.’\n\n“‘Do not trouble yourself, my kind host; I have food; it is\nwarmth and rest only that I need.’\n\n“I sat down, and a silence ensued. I knew that every minute was\nprecious to me, yet I remained irresolute in what manner to commence\nthe interview, when the old man addressed me.\n\n‘By your language, stranger, I suppose you are my countryman; are you\nFrench?’\n\n“‘No; but I was educated by a French family and understand that\nlanguage only. I am now going to claim the protection of some friends,\nwhom I sincerely love, and of whose favour I have some hopes.’\n\n“‘Are they Germans?’\n\n“‘No, they are French. But let us change the subject. I am an\nunfortunate and deserted creature, I look around and I have no relation\nor friend upon earth. These amiable people to whom I go have never\nseen me and know little of me. I am full of fears, for if I fail\nthere, I am an outcast in the world for ever.’\n\n“‘Do not despair. To be friendless is indeed to be unfortunate, but\nthe hearts of men, when unprejudiced by any obvious self-interest, are\nfull of brotherly love and charity. Rely, therefore, on your hopes;\nand if these friends are good and amiable, do not despair.’\n\n“‘They are kind—they are the most excellent creatures in the world;\nbut, unfortunately, they are prejudiced against me. I have good\ndispositions; my life has been hitherto harmless and in some degree\nbeneficial; but a fatal prejudice clouds their eyes, and where they\nought to see a feeling and kind friend, they behold only a detestable\nmonster.’\n\n“‘That is indeed unfortunate; but if you are really blameless, cannot\nyou undeceive them?’\n\n“‘I am about to undertake that task; and it is on that account that I\nfeel so many overwhelming terrors. I tenderly love these friends; I\nhave, unknown to them, been for many months in the habits of daily\nkindness towards them; but they believe that I wish to injure them, and\nit is that prejudice which I wish to overcome.’\n\n“‘Where do these friends reside?’\n\n“‘Near this spot.’\n\n“The old man paused and then continued, ‘If you will unreservedly\nconfide to me the particulars of your tale, I perhaps may be of use in\nundeceiving them. I am blind and cannot judge of your countenance, but\nthere is something in your words which persuades me that you are\nsincere. I am poor and an exile, but it will afford me true pleasure\nto be in any way serviceable to a human creature.’\n\n“‘Excellent man! I thank you and accept your generous offer. You\nraise me from the dust by this kindness; and I trust that, by your aid,\nI shall not be driven from the society and sympathy of your fellow\ncreatures.’\n\n“‘Heaven forbid! Even if you were really criminal, for that can only\ndrive you to desperation, and not instigate you to virtue. I also am\nunfortunate; I and my family have been condemned, although innocent;\njudge, therefore, if I do not feel for'}
20:21:59,40 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:21:59,41 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:21:59,41 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:22:00,851 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:22:00,852 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:22:00,852 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:22:03,717 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:22:03,719 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:22:03,719 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:22:03,719 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17505, Requested 4138. Please try again in 4.929s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:22:03,724 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'Tears, unrestrained, fell from my brother’s eyes; a sense of mortal\nagony crept over my frame. Before, I had only imagined the\nwretchedness of my desolated home; the reality came on me as a new, and\na not less terrible, disaster. I tried to calm Ernest; I enquired more\nminutely concerning my father, and here I named my cousin.\n\n“She most of all,” said Ernest, “requires consolation; she accused\nherself of having caused the death of my brother, and that made her\nvery wretched. But since the murderer has been discovered—”\n\n“The murderer discovered! Good God! how can that be? who could attempt\nto pursue him? It is impossible; one might as well try to overtake the\nwinds, or confine a mountain-stream with a straw. I saw him too; he\nwas free last night!”\n\n“I do not know what you mean,” replied my brother, in accents of\nwonder, “but to us the discovery we have made completes our misery. No\none would believe it at first; and even now Elizabeth will not be\nconvinced, notwithstanding all the evidence. Indeed, who would credit\nthat Justine Moritz, who was so amiable, and fond of all the family,\ncould suddenly become so capable of so frightful, so appalling a crime?”\n\n“Justine Moritz! Poor, poor girl, is she the accused? But it is\nwrongfully; every one knows that; no one believes it, surely, Ernest?”\n\n“No one did at first; but several circumstances came out, that have\nalmost forced conviction upon us; and her own behaviour has been so\nconfused, as to add to the evidence of facts a weight that, I fear,\nleaves no hope for doubt. But she will be tried today, and you will\nthen hear all.”\n\nHe then related that, the morning on which the murder of poor William\nhad been discovered, Justine had been taken ill, and confined to her\nbed for several days. During this interval, one of the servants,\nhappening to examine the apparel she had worn on the night of the\nmurder, had discovered in her pocket the picture of my mother, which\nhad been judged to be the temptation of the murderer. The servant\ninstantly showed it to one of the others, who, without saying a word to\nany of the family, went to a magistrate; and, upon their deposition,\nJustine was apprehended. On being charged with the fact, the poor girl\nconfirmed the suspicion in a great measure by her extreme confusion of\nmanner.\n\nThis was a strange tale, but it did not shake my faith; and I replied\nearnestly, “You are all mistaken; I know the murderer. Justine, poor,\ngood Justine, is innocent.”\n\nAt that instant my father entered. I saw unhappiness deeply impressed\non his countenance, but he endeavoured to welcome me cheerfully; and,\nafter we had exchanged our mournful greeting, would have introduced\nsome other topic than that of our disaster, had not Ernest exclaimed,\n“Good God, papa! Victor says that he knows who was the murderer of\npoor William.”\n\n“We do also, unfortunately,” replied my father, “for indeed I had\nrather have been for ever ignorant than have discovered so much\ndepravity and ungratitude in one I valued so highly.”\n\n“My dear father, you are mistaken; Justine is innocent.”\n\n“If she is, God forbid that she should suffer as guilty. She is to be\ntried today, and I hope, I sincerely hope, that she will be acquitted.”\n\nThis speech calmed me. I was firmly convinced in my own mind that\nJustine, and indeed every human being, was guiltless of this murder. I\nhad no fear, therefore, that any circumstantial evidence could be\nbrought forward strong enough to convict her. My tale was not one to\nannounce publicly; its astounding horror would be looked upon as\nmadness by the vulgar. Did any one indeed exist, except I, the\ncreator, who would believe, unless his senses convinced him, in the\nexistence of the living monument of presumption and rash ignorance\nwhich I had let loose upon the world?\n\nWe were soon joined by Elizabeth. Time had altered her since I last\nbeheld her; it had endowed her with loveliness surpassing the beauty of\nher childish years. There was the same candour, the same vivacity, but\nit was allied to an expression more full of sensibility and intellect.\nShe welcomed me with the greatest affection. “Your arrival, my dear\ncousin,” said she, “fills me with hope. You perhaps will find some\nmeans to justify my poor guiltless Justine. Alas! who is safe, if she\nbe convicted of crime? I rely on her innocence as certainly as I do\nupon my own. Our misfortune is doubly hard to us; we have not only\nlost that lovely darling boy, but this poor girl, whom I sincerely\nlove, is to be torn away by even a worse fate. If she is condemned, I\nnever shall know joy more. But she will not, I am sure she will not;\nand then I shall be happy again, even after the sad death of my little\nWilliam.”\n\n“She is innocent, my Elizabeth,” said I, “and that shall\nbe proved; fear nothing, but let your spirits be cheered by the assurance\nof her acquittal.”\n\n“How kind and generous you are! every one else believes in her guilt,\nand that made me wretched, for I knew that it was impossible: and to\nsee every one else prejudiced in so deadly a'}
20:22:04,184 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:22:04,186 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:22:04,186 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:22:06,333 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:22:06,335 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:22:06,335 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:22:13,433 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:22:16,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.0550231769448146. input_tokens=34, output_tokens=1389
20:22:45,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.899670864921063. input_tokens=34, output_tokens=2083
20:23:16,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 6.511890304973349. input_tokens=34, output_tokens=2398
20:23:54,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.2941384940641. input_tokens=34, output_tokens=1896
20:24:24,508 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:24:38,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3955997499870136. input_tokens=2936, output_tokens=1059
20:25:04,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.193081176024862. input_tokens=34, output_tokens=2141
20:25:49,58 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:26:24,197 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:26:37,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2390946639934555. input_tokens=2936, output_tokens=985
20:27:14,897 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:27:21,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.693919764016755. input_tokens=2936, output_tokens=520
20:27:53,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 5.675377827021293. input_tokens=34, output_tokens=2160
20:28:31,475 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:28:41,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0275293620070443. input_tokens=2936, output_tokens=731
20:29:22,401 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:29:31,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.023734848946333. input_tokens=2936, output_tokens=700
20:30:13,62 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:30:23,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.163649401976727. input_tokens=2937, output_tokens=807
20:30:59,352 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:31:10,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.298243509954773. input_tokens=2934, output_tokens=842
20:31:49,160 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:32:00,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.4745893500512466. input_tokens=2936, output_tokens=851
20:32:39,954 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:32:49,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8800379229942337. input_tokens=2935, output_tokens=692
20:33:30,518 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:33:42,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1871387870050967. input_tokens=2936, output_tokens=894
20:34:22,241 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:35:01,384 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:35:36,889 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:35:53,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.643283062032424. input_tokens=2936, output_tokens=1266
20:36:35,498 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:36:47,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.5776409360114485. input_tokens=2936, output_tokens=874
20:37:26,214 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:37:42,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.67071383993607. input_tokens=2935, output_tokens=1254
20:38:19,465 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:38:32,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7727599600329995. input_tokens=2936, output_tokens=945
20:39:12,241 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:39:22,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.357033998006955. input_tokens=2935, output_tokens=775
20:40:03,710 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:40:10,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.9796805429505184. input_tokens=2936, output_tokens=515
20:40:47,125 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:40:54,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6750617159996182. input_tokens=2936, output_tokens=572
20:41:35,307 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:41:48,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.4351180129451677. input_tokens=2936, output_tokens=974
20:42:28,729 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:42:35,262 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:42:47,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.005301965982653. input_tokens=2936, output_tokens=1429
20:43:26,72 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:43:30,699 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:43:33,470 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:43:44,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.9114213690627366. input_tokens=2936, output_tokens=1382
20:44:04,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.7661347339162603. input_tokens=34, output_tokens=1167
20:44:37,900 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:44:40,849 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:44:47,338 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:44:49,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.091655576019548. input_tokens=2935, output_tokens=881
20:44:52,633 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:44:52,939 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:44:52,940 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:44:52,940 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:44:54,719 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:44:54,721 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:44:54,721 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:44:57,626 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:44:57,628 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:44:57,628 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:44:57,628 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 16986, Requested 3591. Please try again in 1.73s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:44:57,639 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'irretrievable, and after much consideration I resolved to return to the\ncottage, seek the old man, and by my representations win him to my\nparty.\n\n“These thoughts calmed me, and in the afternoon I sank into a profound\nsleep; but the fever of my blood did not allow me to be visited by\npeaceful dreams. The horrible scene of the preceding day was for ever\nacting before my eyes; the females were flying and the enraged Felix\ntearing me from his father’s feet. I awoke exhausted, and finding that\nit was already night, I crept forth from my hiding-place, and went in\nsearch of food.\n\n“When my hunger was appeased, I directed my steps towards the\nwell-known path that conducted to the cottage. All there was at peace.\nI crept into my hovel and remained in silent expectation of the\naccustomed hour when the family arose. That hour passed, the sun\nmounted high in the heavens, but the cottagers did not appear. I\ntrembled violently, apprehending some dreadful misfortune. The inside\nof the cottage was dark, and I heard no motion; I cannot describe the\nagony of this suspense.\n\n“Presently two countrymen passed by, but pausing near the cottage, they\nentered into conversation, using violent gesticulations; but I did not\nunderstand what they said, as they spoke the language of the country,\nwhich differed from that of my protectors. Soon after, however, Felix\napproached with another man; I was surprised, as I knew that he had not\nquitted the cottage that morning, and waited anxiously to discover from\nhis discourse the meaning of these unusual appearances.\n\n“‘Do you consider,’ said his companion to him,\n‘that you will be obliged to pay three months’ rent and to lose\nthe produce of your garden? I do not wish to take any unfair advantage, and\nI beg therefore that you will take some days to consider of your\ndetermination.’\n\n“‘It is utterly useless,’ replied Felix; ‘we can\nnever again inhabit your cottage. The life of my father is in the greatest\ndanger, owing to the dreadful circumstance that I have related. My wife and\nmy sister will never recover from their horror. I entreat you not to reason\nwith me any more. Take possession of your tenement and let me fly from this\nplace.’\n\n“Felix trembled violently as he said this. He and his companion\nentered the cottage, in which they remained for a few minutes, and then\ndeparted. I never saw any of the family of De Lacey more.\n\n“I continued for the remainder of the day in my hovel in a state of\nutter and stupid despair. My protectors had departed and had broken\nthe only link that held me to the world. For the first time the\nfeelings of revenge and hatred filled my bosom, and I did not strive to\ncontrol them, but allowing myself to be borne away by the stream, I\nbent my mind towards injury and death. When I thought of my friends,\nof the mild voice of De Lacey, the gentle eyes of Agatha, and the\nexquisite beauty of the Arabian, these thoughts vanished and a gush of\ntears somewhat soothed me. But again when I reflected that they had\nspurned and deserted me, anger returned, a rage of anger, and unable to\ninjure anything human, I turned my fury towards inanimate objects. As\nnight advanced, I placed a variety of combustibles around the cottage,\nand after having destroyed every vestige of cultivation in the garden,\nI waited with forced impatience until the moon had sunk to commence my\noperations.\n\n“As the night advanced, a fierce wind arose from the woods and quickly\ndispersed the clouds that had loitered in the heavens; the blast tore\nalong like a mighty avalanche and produced a kind of insanity in my\nspirits that burst all bounds of reason and reflection. I lighted the\ndry branch of a tree and danced with fury around the devoted cottage,\nmy eyes still fixed on the western horizon, the edge of which the moon\nnearly touched. A part of its orb was at length hid, and I waved my\nbrand; it sank, and with a loud scream I fired the straw, and heath,\nand bushes, which I had collected. The wind fanned the fire, and the\ncottage was quickly enveloped by the flames, which clung to it and\nlicked it with their forked and destroying tongues.\n\n“As soon as I was convinced that no assistance could save any part of\nthe habitation, I quitted the scene and sought for refuge in the woods.\n\n“And now, with the world before me, whither should I bend my steps? I\nresolved to fly far from the scene of my misfortunes; but to me, hated\nand despised, every country must be equally horrible. At length the\nthought of you crossed my mind. I learned from your papers that you\nwere my father, my creator; and to whom could I apply with more fitness\nthan to him who had given me life? Among the lessons that Felix had\nbestowed upon Safie, geography had not been omitted; I had learned from\nthese the relative situations of the different countries of the earth.\nYou had mentioned Geneva as the name of your native town, and towards\nthis place I resolved to proceed.\n\n“But how was I to direct myself? I knew that I must travel in a\nsouthwesterly direction to reach my destination, but the sun was my\nonly guide. I did not know the names of the towns that I was to pass\nthrough, nor could I ask information from a'}
20:44:57,925 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:44:57,927 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:44:57,927 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:03,269 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:45:03,532 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:03,535 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:03,535 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:06,14 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:06,16 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:06,16 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:08,805 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:08,806 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:08,806 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:08,807 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19169, Requested 3500. Please try again in 8.006s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:45:08,810 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'age rendered him extremely averse to delay. For myself, there was one\nreward I promised myself from my detested toils—one consolation for my\nunparalleled sufferings; it was the prospect of that day when,\nenfranchised from my miserable slavery, I might claim Elizabeth and\nforget the past in my union with her.\n\nI now made arrangements for my journey, but one feeling haunted me\nwhich filled me with fear and agitation. During my absence I should\nleave my friends unconscious of the existence of their enemy and\nunprotected from his attacks, exasperated as he might be by my\ndeparture. But he had promised to follow me wherever I might go, and\nwould he not accompany me to England? This imagination was dreadful in\nitself, but soothing inasmuch as it supposed the safety of my friends.\nI was agonised with the idea of the possibility that the reverse of\nthis might happen. But through the whole period during which I was the\nslave of my creature I allowed myself to be governed by the impulses of\nthe moment; and my present sensations strongly intimated that the fiend\nwould follow me and exempt my family from the danger of his\nmachinations.\n\nIt was in the latter end of September that I again quitted my native\ncountry. My journey had been my own suggestion, and Elizabeth\ntherefore acquiesced, but she was filled with disquiet at the idea of\nmy suffering, away from her, the inroads of misery and grief. It had\nbeen her care which provided me a companion in Clerval—and yet a man\nis blind to a thousand minute circumstances which call forth a woman’s\nsedulous attention. She longed to bid me hasten my return; a thousand\nconflicting emotions rendered her mute as she bade me a tearful, silent\nfarewell.\n\nI threw myself into the carriage that was to convey me away, hardly\nknowing whither I was going, and careless of what was passing around.\nI remembered only, and it was with a bitter anguish that I reflected on\nit, to order that my chemical instruments should be packed to go with\nme. Filled with dreary imaginations, I passed through many beautiful\nand majestic scenes, but my eyes were fixed and unobserving. I could\nonly think of the bourne of my travels and the work which was to occupy\nme whilst they endured.\n\nAfter some days spent in listless indolence, during which I traversed\nmany leagues, I arrived at Strasburgh, where I waited two days for\nClerval. He came. Alas, how great was the contrast between us! He\nwas alive to every new scene, joyful when he saw the beauties of the\nsetting sun, and more happy when he beheld it rise and recommence a new\nday. He pointed out to me the shifting colours of the landscape and\nthe appearances of the sky. “This is what it is to live,” he cried;\n“now I enjoy existence! But you, my dear Frankenstein, wherefore are\nyou desponding and sorrowful!” In truth, I was occupied by gloomy\nthoughts and neither saw the descent of the evening star nor the golden\nsunrise reflected in the Rhine. And you, my friend, would be far more\namused with the journal of Clerval, who observed the scenery with an\neye of feeling and delight, than in listening to my reflections. I, a\nmiserable wretch, haunted by a curse that shut up every avenue to\nenjoyment.\n\nWe had agreed to descend the Rhine in a boat from Strasburgh to\nRotterdam, whence we might take shipping for London. During this\nvoyage we passed many willowy islands and saw several beautiful towns.\nWe stayed a day at Mannheim, and on the fifth from our departure from\nStrasburgh, arrived at Mainz. The course of the Rhine below Mainz\nbecomes much more picturesque. The river descends rapidly and winds\nbetween hills, not high, but steep, and of beautiful forms. We saw\nmany ruined castles standing on the edges of precipices, surrounded by\nblack woods, high and inaccessible. This part of the Rhine, indeed,\npresents a singularly variegated landscape. In one spot you view\nrugged hills, ruined castles overlooking tremendous precipices, with\nthe dark Rhine rushing beneath; and on the sudden turn of a promontory,\nflourishing vineyards with green sloping banks and a meandering river\nand populous towns occupy the scene.\n\nWe travelled at the time of the vintage and heard the song of the labourers\nas we glided down the stream. Even I, depressed in mind, and my spirits\ncontinually agitated by gloomy feelings, even I was pleased. I lay at the\nbottom of the boat, and as I gazed on the cloudless blue sky, I seemed to\ndrink in a tranquillity to which I had long been a stranger. And if these\nwere my sensations, who can describe those of Henry? He felt as if he had\nbeen transported to Fairy-land and enjoyed a happiness seldom tasted by\nman. “I have seen,” he said, “the most beautiful scenes\nof my own country; I have visited the lakes of Lucerne and Uri, where the\nsnowy mountains descend almost perpendicularly to the water, casting black\nand impenetrable shades, which would cause a gloomy and mournful appearance\nwere it not for the most verdant islands that relieve the eye by their gay\nappearance; I have seen this lake agitated by a tempest, when the wind tore\nup whirlwinds of water and gave you an idea'}
20:45:09,161 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:09,162 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:09,162 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:10,848 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:10,850 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:10,850 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:14,72 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:14,74 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:14,74 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:14,74 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17505, Requested 3642. Please try again in 3.441s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:45:14,77 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'seeing Clerval; his presence brought back\nto my thoughts my father, Elizabeth, and all those scenes of home so dear\nto my recollection. I grasped his hand, and in a moment forgot my horror\nand misfortune; I felt suddenly, and for the first time during many months,\ncalm and serene joy. I welcomed my friend, therefore, in the most cordial\nmanner, and we walked towards my college. Clerval continued talking for\nsome time about our mutual friends and his own good fortune in being\npermitted to come to Ingolstadt. “You may easily believe,” said\nhe, “how great was the difficulty to persuade my father that all\nnecessary knowledge was not comprised in the noble art of book-keeping;\nand, indeed, I believe I left him incredulous to the last, for his constant\nanswer to my unwearied entreaties was the same as that of the Dutch\nschoolmaster in The Vicar of Wakefield: ‘I have ten thousand florins\na year without Greek, I eat heartily without Greek.’ But his\naffection for me at length overcame his dislike of learning, and he has\npermitted me to undertake a voyage of discovery to the land of\nknowledge.”\n\n“It gives me the greatest delight to see you; but tell me how you left\nmy father, brothers, and Elizabeth.”\n\n“Very well, and very happy, only a little uneasy that they hear from\nyou so seldom. By the by, I mean to lecture you a little upon their\naccount myself. But, my dear Frankenstein,” continued he, stopping\nshort and gazing full in my face, “I did not before remark how very ill\nyou appear; so thin and pale; you look as if you had been watching for\nseveral nights.”\n\n“You have guessed right; I have lately been so deeply engaged in one\noccupation that I have not allowed myself sufficient rest, as you see;\nbut I hope, I sincerely hope, that all these employments are now at an\nend and that I am at length free.”\n\nI trembled excessively; I could not endure to think of, and far less to\nallude to, the occurrences of the preceding night. I walked with a\nquick pace, and we soon arrived at my college. I then reflected, and\nthe thought made me shiver, that the creature whom I had left in my\napartment might still be there, alive and walking about. I dreaded to\nbehold this monster, but I feared still more that Henry should see him.\nEntreating him, therefore, to remain a few minutes at the bottom of the\nstairs, I darted up towards my own room. My hand was already on the\nlock of the door before I recollected myself. I then paused, and a\ncold shivering came over me. I threw the door forcibly open, as\nchildren are accustomed to do when they expect a spectre to stand in\nwaiting for them on the other side; but nothing appeared. I stepped\nfearfully in: the apartment was empty, and my bedroom was also freed\nfrom its hideous guest. I could hardly believe that so great a good\nfortune could have befallen me, but when I became assured that my enemy\nhad indeed fled, I clapped my hands for joy and ran down to Clerval.\n\nWe ascended into my room, and the servant presently brought breakfast;\nbut I was unable to contain myself. It was not joy only that possessed\nme; I felt my flesh tingle with excess of sensitiveness, and my pulse\nbeat rapidly. I was unable to remain for a single instant in the same\nplace; I jumped over the chairs, clapped my hands, and laughed aloud.\nClerval at first attributed my unusual spirits to joy on his arrival,\nbut when he observed me more attentively, he saw a wildness in my eyes\nfor which he could not account, and my loud, unrestrained, heartless\nlaughter frightened and astonished him.\n\n“My dear Victor,” cried he, “what, for God’s sake,\nis the matter? Do not laugh in that manner. How ill you are! What is the\ncause of all this?”\n\n“Do not ask me,” cried I, putting my hands before my eyes, for I\nthought I saw the dreaded spectre glide into the room; “_he_ can\ntell. Oh, save me! Save me!” I imagined that the monster seized me;\nI struggled furiously and fell down in a fit.\n\nPoor Clerval! What must have been his feelings? A meeting, which he\nanticipated with such joy, so strangely turned to bitterness. But I\nwas not the witness of his grief, for I was lifeless and did not\nrecover my senses for a long, long time.\n\nThis was the commencement of a nervous fever which confined me for\nseveral months. During all that time Henry was my only nurse. I\nafterwards learned that, knowing my father’s advanced age and unfitness\nfor so long a journey, and how wretched my sickness would make\nElizabeth, he spared them this grief by concealing the extent of my\ndisorder. He knew that I could not have a more kind and attentive\nnurse than himself; and, firm in the hope he felt of my recovery, he\ndid not doubt that, instead of doing harm, he performed the kindest\naction that he could towards them.\n\nBut I was in reality very ill, and surely nothing but the unbounded and\nunremitting attentions of my friend could have restored me to life.\nThe form of the monster on whom I had bestowed existence was for ever\nbefore my eyes, and I raved incessantly concerning him. Doubtless my\nwords surprised Henry'}
20:45:14,600 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:14,602 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:14,602 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:15,978 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:15,980 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:15,980 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:17,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.947231194935739. input_tokens=34, output_tokens=1368
20:45:21,753 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:45:22,20 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:22,22 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:22,22 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:24,92 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:24,99 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:24,99 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:26,594 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:26,596 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:26,597 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:26,597 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19318, Requested 3597. Please try again in 8.743s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:45:26,601 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'Manoir, has suffered several misfortunes\nsince the departure of Clerval from Geneva. But he has already\nrecovered his spirits, and is reported to be on the point of marrying a\nlively pretty Frenchwoman, Madame Tavernier. She is a widow, and much\nolder than Manoir; but she is very much admired, and a favourite with\neverybody.\n\n“I have written myself into better spirits, dear cousin; but my anxiety\nreturns upon me as I conclude. Write, dearest Victor,—one line—one\nword will be a blessing to us. Ten thousand thanks to Henry for his\nkindness, his affection, and his many letters; we are sincerely\ngrateful. Adieu! my cousin; take care of yourself; and, I entreat\nyou, write!\n\n“Elizabeth Lavenza.\n\n\n“Geneva, March 18th, 17—.”\n\n\n\n“Dear, dear Elizabeth!” I exclaimed, when I had read her\nletter: “I will write instantly and relieve them from the anxiety\nthey must feel.” I wrote, and this exertion greatly fatigued me; but\nmy convalescence had commenced, and proceeded regularly. In another\nfortnight I was able to leave my chamber.\n\nOne of my first duties on my recovery was to introduce Clerval to the\nseveral professors of the university. In doing this, I underwent a\nkind of rough usage, ill befitting the wounds that my mind had\nsustained. Ever since the fatal night, the end of my labours, and the\nbeginning of my misfortunes, I had conceived a violent antipathy even\nto the name of natural philosophy. When I was otherwise quite restored\nto health, the sight of a chemical instrument would renew all the agony\nof my nervous symptoms. Henry saw this, and had removed all my\napparatus from my view. He had also changed my apartment; for he\nperceived that I had acquired a dislike for the room which had\npreviously been my laboratory. But these cares of Clerval were made of\nno avail when I visited the professors. M. Waldman inflicted torture\nwhen he praised, with kindness and warmth, the astonishing progress I\nhad made in the sciences. He soon perceived that I disliked the\nsubject; but not guessing the real cause, he attributed my feelings to\nmodesty, and changed the subject from my improvement, to the science\nitself, with a desire, as I evidently saw, of drawing me out. What\ncould I do? He meant to please, and he tormented me. I felt as if he\nhad placed carefully, one by one, in my view those instruments which\nwere to be afterwards used in putting me to a slow and cruel death. I\nwrithed under his words, yet dared not exhibit the pain I felt.\nClerval, whose eyes and feelings were always quick in discerning the\nsensations of others, declined the subject, alleging, in excuse, his\ntotal ignorance; and the conversation took a more general turn. I\nthanked my friend from my heart, but I did not speak. I saw plainly\nthat he was surprised, but he never attempted to draw my secret from\nme; and although I loved him with a mixture of affection and reverence\nthat knew no bounds, yet I could never persuade myself to confide in\nhim that event which was so often present to my recollection, but which\nI feared the detail to another would only impress more deeply.\n\nM. Krempe was not equally docile; and in my condition at that time, of\nalmost insupportable sensitiveness, his harsh blunt encomiums gave me even\nmore pain than the benevolent approbation of M. Waldman. “D—n\nthe fellow!” cried he; “why, M. Clerval, I assure you he has\noutstript us all. Ay, stare if you please; but it is nevertheless true. A\nyoungster who, but a few years ago, believed in Cornelius Agrippa as firmly\nas in the gospel, has now set himself at the head of the university; and if\nhe is not soon pulled down, we shall all be out of countenance.—Ay,\nay,” continued he, observing my face expressive of suffering,\n“M. Frankenstein is modest; an excellent quality in a young man.\nYoung men should be diffident of themselves, you know, M. Clerval: I was\nmyself when young; but that wears out in a very short time.”\n\nM. Krempe had now commenced an eulogy on himself, which happily turned\nthe conversation from a subject that was so annoying to me.\n\nClerval had never sympathised in my tastes for natural science; and his\nliterary pursuits differed wholly from those which had occupied me. He\ncame to the university with the design of making himself complete\nmaster of the oriental languages, and thus he should open a field for\nthe plan of life he had marked out for himself. Resolved to pursue no\ninglorious career, he turned his eyes toward the East, as affording\nscope for his spirit of enterprise. The Persian, Arabic, and Sanskrit\nlanguages engaged his attention, and I was easily induced to enter on\nthe same studies. Idleness had ever been irksome to me, and now that I\nwished to fly from reflection, and hated my former studies, I felt\ngreat relief in being the fellow-pupil with my friend, and found not\nonly instruction but consolation in the works of the orientalists. I\ndid not, like him, attempt a critical knowledge of their dialects, for\nI did not contemplate making any other use of them than'}
20:45:27,0 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:27,2 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:27,2 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:29,98 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:29,100 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:29,100 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:31,590 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:31,592 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:31,592 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:31,592 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17658, Requested 3893. Please try again in 4.653s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:45:31,595 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'of my own country; I have visited the lakes of Lucerne and Uri, where the\nsnowy mountains descend almost perpendicularly to the water, casting black\nand impenetrable shades, which would cause a gloomy and mournful appearance\nwere it not for the most verdant islands that relieve the eye by their gay\nappearance; I have seen this lake agitated by a tempest, when the wind tore\nup whirlwinds of water and gave you an idea of what the water-spout must be\non the great ocean; and the waves dash with fury the base of the mountain,\nwhere the priest and his mistress were overwhelmed by an avalanche and\nwhere their dying voices are still said to be heard amid the pauses of the\nnightly wind; I have seen the mountains of La Valais, and the Pays de Vaud;\nbut this country, Victor, pleases me more than all those wonders. The\nmountains of Switzerland are more majestic and strange, but there is a\ncharm in the banks of this divine river that I never before saw equalled.\nLook at that castle which overhangs yon precipice; and that also on the\nisland, almost concealed amongst the foliage of those lovely trees; and now\nthat group of labourers coming from among their vines; and that village\nhalf hid in the recess of the mountain. Oh, surely the spirit that inhabits\nand guards this place has a soul more in harmony with man than those who\npile the glacier or retire to the inaccessible peaks of the mountains of\nour own country.”\n\nClerval! Beloved friend! Even now it delights me to record your words and\nto dwell on the praise of which you are so eminently deserving. He was a\nbeing formed in the “very poetry of nature.” His wild and\nenthusiastic imagination was chastened by the sensibility of his heart. His\nsoul overflowed with ardent affections, and his friendship was of that\ndevoted and wondrous nature that the worldly-minded teach us to look for only\nin the imagination. But even human sympathies were not sufficient to\nsatisfy his eager mind. The scenery of external nature, which others regard\nonly with admiration, he loved with ardour:—\n\n    ——The sounding cataract\n    Haunted him like a passion: the tall rock,\n    The mountain, and the deep and gloomy wood,\n    Their colours and their forms, were then to him\n    An appetite; a feeling, and a love,\n    That had no need of a remoter charm,\n    By thought supplied, or any interest\n    Unborrow’d from the eye.\n\n          [Wordsworth’s “Tintern Abbey”.]\n\nAnd where does he now exist? Is this gentle and lovely being lost\nfor ever? Has this mind, so replete with ideas, imaginations fanciful\nand magnificent, which formed a world, whose existence depended on the\nlife of its creator;—has this mind perished? Does it now only exist\nin my memory? No, it is not thus; your form so divinely wrought, and\nbeaming with beauty, has decayed, but your spirit still visits and\nconsoles your unhappy friend.\n\nPardon this gush of sorrow; these ineffectual words are but a slight\ntribute to the unexampled worth of Henry, but they soothe my heart,\noverflowing with the anguish which his remembrance creates. I will\nproceed with my tale.\n\nBeyond Cologne we descended to the plains of Holland; and we resolved to\npost the remainder of our way, for the wind was contrary and the stream of\nthe river was too gentle to aid us.\n\nOur journey here lost the interest arising from beautiful scenery, but we\narrived in a few days at Rotterdam, whence we proceeded by sea to England.\nIt was on a clear morning, in the latter days of December, that I first saw\nthe white cliffs of Britain. The banks of the Thames presented a new scene;\nthey were flat but fertile, and almost every town was marked by the\nremembrance of some story. We saw Tilbury Fort and remembered the Spanish\nArmada, Gravesend, Woolwich, and Greenwich—places which I had heard\nof even in my country.\n\nAt length we saw the numerous steeples of London, St. Paul’s towering\nabove all, and the Tower famed in English history.\n\n\n\n\nChapter 19\n\n\nLondon was our present point of rest; we determined to remain several\nmonths in this wonderful and celebrated city. Clerval desired the\nintercourse of the men of genius and talent who flourished at this\ntime, but this was with me a secondary object; I was principally\noccupied with the means of obtaining the information necessary for the\ncompletion of my promise and quickly availed myself of the letters of\nintroduction that I had brought with me, addressed to the most\ndistinguished natural philosophers.\n\nIf this journey had taken place during my days of study and happiness,\nit would have afforded me inexpressible pleasure. But a blight had\ncome over my existence, and I only visited these people for the sake of\nthe information they might give me on the subject in which my interest\nwas so terribly profound. Company was irksome to me; when alone, I\ncould fill my mind with the sights of heaven and earth; the voice of\nHenry soothed me, and I could thus cheat myself into a transitory\npeace. But busy, uninteresting, joyous faces brought back despair to\nmy heart. I saw an insurmountable barrier placed between me and my\nfellow men; this barrier was sealed with the blood of William and\nJustine, and to reflect on the events connected with those names filled\nmy soul'}
20:45:31,916 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:31,918 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:31,918 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:33,929 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:33,931 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:33,931 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:40,832 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:45:41,375 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:41,377 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:41,377 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:43,291 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:43,293 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:43,293 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:46,23 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.6645523929037154. input_tokens=34, output_tokens=1818
20:45:46,279 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:46,282 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:46,282 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:46,282 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 18766, Requested 3651. Please try again in 7.25s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:45:46,286 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'neglected them, but now that I was able to decipher the characters in\nwhich they were written, I began to study them with diligence. It was\nyour journal of the four months that preceded my creation. You\nminutely described in these papers every step you took in the progress\nof your work; this history was mingled with accounts of domestic\noccurrences. You doubtless recollect these papers. Here they are.\nEverything is related in them which bears reference to my accursed\norigin; the whole detail of that series of disgusting circumstances\nwhich produced it is set in view; the minutest description of my odious\nand loathsome person is given, in language which painted your own\nhorrors and rendered mine indelible. I sickened as I read. ‘Hateful\nday when I received life!’ I exclaimed in agony. ‘Accursed creator!\nWhy did you form a monster so hideous that even _you_ turned from me in\ndisgust? God, in pity, made man beautiful and alluring, after his own\nimage; but my form is a filthy type of yours, more horrid even from the\nvery resemblance. Satan had his companions, fellow devils, to admire\nand encourage him, but I am solitary and abhorred.’\n\n“These were the reflections of my hours of despondency and solitude;\nbut when I contemplated the virtues of the cottagers, their amiable and\nbenevolent dispositions, I persuaded myself that when they should\nbecome acquainted with my admiration of their virtues they would\ncompassionate me and overlook my personal deformity. Could they turn\nfrom their door one, however monstrous, who solicited their compassion\nand friendship? I resolved, at least, not to despair, but in every way\nto fit myself for an interview with them which would decide my fate. I\npostponed this attempt for some months longer, for the importance\nattached to its success inspired me with a dread lest I should fail.\nBesides, I found that my understanding improved so much with every\nday’s experience that I was unwilling to commence this undertaking\nuntil a few more months should have added to my sagacity.\n\n“Several changes, in the meantime, took place in the cottage. The\npresence of Safie diffused happiness among its inhabitants, and I also\nfound that a greater degree of plenty reigned there. Felix and Agatha\nspent more time in amusement and conversation, and were assisted in\ntheir labours by servants. They did not appear rich, but they were\ncontented and happy; their feelings were serene and peaceful, while\nmine became every day more tumultuous. Increase of knowledge only\ndiscovered to me more clearly what a wretched outcast I was. I\ncherished hope, it is true, but it vanished when I beheld my person\nreflected in water or my shadow in the moonshine, even as that frail\nimage and that inconstant shade.\n\n“I endeavoured to crush these fears and to fortify myself for the trial\nwhich in a few months I resolved to undergo; and sometimes I allowed my\nthoughts, unchecked by reason, to ramble in the fields of Paradise, and\ndared to fancy amiable and lovely creatures sympathising with my\nfeelings and cheering my gloom; their angelic countenances breathed\nsmiles of consolation. But it was all a dream; no Eve soothed my\nsorrows nor shared my thoughts; I was alone. I remembered Adam’s\nsupplication to his Creator. But where was mine? He had abandoned me,\nand in the bitterness of my heart I cursed him.\n\n“Autumn passed thus. I saw, with surprise and grief, the leaves decay\nand fall, and nature again assume the barren and bleak appearance it\nhad worn when I first beheld the woods and the lovely moon. Yet I did\nnot heed the bleakness of the weather; I was better fitted by my\nconformation for the endurance of cold than heat. But my chief\ndelights were the sight of the flowers, the birds, and all the gay\napparel of summer; when those deserted me, I turned with more attention\ntowards the cottagers. Their happiness was not decreased by the\nabsence of summer. They loved and sympathised with one another; and\ntheir joys, depending on each other, were not interrupted by the\ncasualties that took place around them. The more I saw of them, the\ngreater became my desire to claim their protection and kindness; my\nheart yearned to be known and loved by these amiable creatures; to see\ntheir sweet looks directed towards me with affection was the utmost\nlimit of my ambition. I dared not think that they would turn them from\nme with disdain and horror. The poor that stopped at their door were\nnever driven away. I asked, it is true, for greater treasures than a\nlittle food or rest: I required kindness and sympathy; but I did not\nbelieve myself utterly unworthy of it.\n\n“The winter advanced, and an entire revolution of the seasons had taken\nplace since I awoke into life. My attention at this time was solely\ndirected towards my plan of introducing myself into the cottage of my\nprotectors. I revolved many projects, but that on which I finally\nfixed was to enter the dwelling when the blind old man should be alone.\nI had sagacity enough to discover that the unnatural hideousness of my\nperson was the chief object of horror with those who had formerly\nbeheld me. My voice, although harsh, had nothing terrible in it; I\nthought, therefore, that if in the absence of his children I could gain\nthe good will and mediation of the old De Lacey, I'}
20:45:46,539 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:46,542 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:46,542 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:47,978 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:47,979 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:47,979 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:50,731 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:50,733 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:50,733 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:50,733 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17354, Requested 3351. Please try again in 2.115s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:45:50,736 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'induced to enter on\nthe same studies. Idleness had ever been irksome to me, and now that I\nwished to fly from reflection, and hated my former studies, I felt\ngreat relief in being the fellow-pupil with my friend, and found not\nonly instruction but consolation in the works of the orientalists. I\ndid not, like him, attempt a critical knowledge of their dialects, for\nI did not contemplate making any other use of them than temporary\namusement. I read merely to understand their meaning, and they well\nrepaid my labours. Their melancholy is soothing, and their joy\nelevating, to a degree I never experienced in studying the authors of\nany other country. When you read their writings, life appears to\nconsist in a warm sun and a garden of roses,—in the smiles and frowns\nof a fair enemy, and the fire that consumes your own heart. How\ndifferent from the manly and heroical poetry of Greece and Rome!\n\nSummer passed away in these occupations, and my return to Geneva was\nfixed for the latter end of autumn; but being delayed by several\naccidents, winter and snow arrived, the roads were deemed impassable,\nand my journey was retarded until the ensuing spring. I felt this\ndelay very bitterly; for I longed to see my native town and my beloved\nfriends. My return had only been delayed so long, from an\nunwillingness to leave Clerval in a strange place, before he had become\nacquainted with any of its inhabitants. The winter, however, was spent\ncheerfully; and although the spring was uncommonly late, when it came\nits beauty compensated for its dilatoriness.\n\nThe month of May had already commenced, and I expected the letter daily\nwhich was to fix the date of my departure, when Henry proposed a\npedestrian tour in the environs of Ingolstadt, that I might bid a\npersonal farewell to the country I had so long inhabited. I acceded\nwith pleasure to this proposition: I was fond of exercise, and Clerval\nhad always been my favourite companion in the ramble of this nature\nthat I had taken among the scenes of my native country.\n\nWe passed a fortnight in these perambulations: my health and spirits\nhad long been restored, and they gained additional strength from the\nsalubrious air I breathed, the natural incidents of our progress, and\nthe conversation of my friend. Study had before secluded me from the\nintercourse of my fellow-creatures, and rendered me unsocial; but\nClerval called forth the better feelings of my heart; he again taught\nme to love the aspect of nature, and the cheerful faces of children.\nExcellent friend! how sincerely you did love me, and endeavour to\nelevate my mind until it was on a level with your own. A selfish\npursuit had cramped and narrowed me, until your gentleness and\naffection warmed and opened my senses; I became the same happy creature\nwho, a few years ago, loved and beloved by all, had no sorrow or care.\nWhen happy, inanimate nature had the power of bestowing on me the most\ndelightful sensations. A serene sky and verdant fields filled me with\necstasy. The present season was indeed divine; the flowers of spring\nbloomed in the hedges, while those of summer were already in bud. I\nwas undisturbed by thoughts which during the preceding year had pressed\nupon me, notwithstanding my endeavours to throw them off, with an\ninvincible burden.\n\nHenry rejoiced in my gaiety, and sincerely sympathised in my feelings: he\nexerted himself to amuse me, while he expressed the sensations that filled\nhis soul. The resources of his mind on this occasion were truly\nastonishing: his conversation was full of imagination; and very often, in\nimitation of the Persian and Arabic writers, he invented tales of wonderful\nfancy and passion. At other times he repeated my favourite poems, or drew\nme out into arguments, which he supported with great ingenuity.\n\nWe returned to our college on a Sunday afternoon: the peasants were\ndancing, and every one we met appeared gay and happy. My own spirits were\nhigh, and I bounded along with feelings of unbridled joy and hilarity.\n\n\n\n\nChapter 7\n\n\nOn my return, I found the following letter from my father:—\n\n“My dear Victor,\n\n“You have probably waited impatiently for a letter to fix the date of\nyour return to us; and I was at first tempted to write only a few\nlines, merely mentioning the day on which I should expect you. But\nthat would be a cruel kindness, and I dare not do it. What would be\nyour surprise, my son, when you expected a happy and glad welcome, to\nbehold, on the contrary, tears and wretchedness? And how, Victor, can\nI relate our misfortune? Absence cannot have rendered you callous to\nour joys and griefs; and how shall I inflict pain on my long absent\nson? I wish to prepare you for the woeful news, but I know it is\nimpossible; even now your eye skims over the page to seek the words\nwhich are to convey to you the horrible tidings.\n\n“William is dead!—that sweet child, whose smiles delighted and warmed\nmy heart, who was so gentle, yet so gay! Victor, he is murdered!\n\n“I will not attempt to console you; but will simply relate the\ncircumstances of the transaction.\n\n“Last Thursday (May 7th), I, my niece, and your two brothers, went to\nwalk in'}
20:45:51,126 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:51,127 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:51,127 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:55,712 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:45:56,93 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:56,95 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:56,95 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:45:58,196 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:45:58,197 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:45:58,197 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:46:01,86 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:46:01,87 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:46:01,87 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:46:01,88 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 18750, Requested 3649. Please try again in 7.197s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:46:01,91 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'again felt as\nif I belonged to a race of human beings like myself, and I began to\nreflect upon what had passed with greater composure; yet still the\nwords of the fiend rang in my ears like a death-knell; they appeared\nlike a dream, yet distinct and oppressive as a reality.\n\nThe sun had far descended, and I still sat on the shore, satisfying my\nappetite, which had become ravenous, with an oaten cake, when I saw a\nfishing-boat land close to me, and one of the men brought me a packet;\nit contained letters from Geneva, and one from Clerval entreating me to\njoin him. He said that he was wearing away his time fruitlessly where\nhe was, that letters from the friends he had formed in London desired\nhis return to complete the negotiation they had entered into for his\nIndian enterprise. He could not any longer delay his departure; but as\nhis journey to London might be followed, even sooner than he now\nconjectured, by his longer voyage, he entreated me to bestow as much of\nmy society on him as I could spare. He besought me, therefore, to\nleave my solitary isle and to meet him at Perth, that we might proceed\nsouthwards together. This letter in a degree recalled me to life, and\nI determined to quit my island at the expiration of two days.\n\nYet, before I departed, there was a task to perform, on which I shuddered\nto reflect; I must pack up my chemical instruments, and for that purpose I\nmust enter the room which had been the scene of my odious work, and I must\nhandle those utensils the sight of which was sickening to me. The next\nmorning, at daybreak, I summoned sufficient courage and unlocked the door\nof my laboratory. The remains of the half-finished creature, whom I had\ndestroyed, lay scattered on the floor, and I almost felt as if I had\nmangled the living flesh of a human being. I paused to collect myself and\nthen entered the chamber. With trembling hand I conveyed the instruments\nout of the room, but I reflected that I ought not to leave the relics of my\nwork to excite the horror and suspicion of the peasants; and I accordingly\nput them into a basket, with a great quantity of stones, and laying them\nup, determined to throw them into the sea that very night; and in the\nmeantime I sat upon the beach, employed in cleaning and arranging my\nchemical apparatus.\n\nNothing could be more complete than the alteration that had taken place\nin my feelings since the night of the appearance of the dæmon. I had\nbefore regarded my promise with a gloomy despair as a thing that, with\nwhatever consequences, must be fulfilled; but I now felt as if a film\nhad been taken from before my eyes and that I for the first time saw\nclearly. The idea of renewing my labours did not for one instant occur\nto me; the threat I had heard weighed on my thoughts, but I did not\nreflect that a voluntary act of mine could avert it. I had resolved in\nmy own mind that to create another like the fiend I had first made\nwould be an act of the basest and most atrocious selfishness, and I\nbanished from my mind every thought that could lead to a different\nconclusion.\n\nBetween two and three in the morning the moon rose; and I then, putting my\nbasket aboard a little skiff, sailed out about four miles from the shore.\nThe scene was perfectly solitary; a few boats were returning towards land,\nbut I sailed away from them. I felt as if I was about the commission of a\ndreadful crime and avoided with shuddering anxiety any encounter with my\nfellow creatures. At one time the moon, which had before been clear, was\nsuddenly overspread by a thick cloud, and I took advantage of the moment of\ndarkness and cast my basket into the sea; I listened to the gurgling sound\nas it sank and then sailed away from the spot. The sky became clouded, but\nthe air was pure, although chilled by the northeast breeze that was then\nrising. But it refreshed me and filled me with such agreeable sensations\nthat I resolved to prolong my stay on the water, and fixing the rudder in a\ndirect position, stretched myself at the bottom of the boat. Clouds hid the\nmoon, everything was obscure, and I heard only the sound of the boat as its\nkeel cut through the waves; the murmur lulled me, and in a short time I\nslept soundly.\n\nI do not know how long I remained in this situation, but when I awoke I\nfound that the sun had already mounted considerably. The wind was high, and\nthe waves continually threatened the safety of my little skiff. I found\nthat the wind was northeast and must have driven me far from the coast from\nwhich I had embarked. I endeavoured to change my course but quickly found\nthat if I again made the attempt the boat would be instantly filled with\nwater. Thus situated, my only resource was to drive before the wind. I\nconfess that I felt a few sensations of terror. I had no compass with me\nand was so slenderly acquainted with the geography of this part of the\nworld that the sun was of little benefit to me. I might be driven into the\nwide Atlantic and feel all the tortures of starvation or be swallowed up in\nthe immeasurable waters that roared and buffeted around me. I had already\nbeen out many hours and felt the torment of a burning thirst'}
20:46:01,351 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:46:01,353 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:46:01,353 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:46:03,305 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:46:03,307 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:46:03,307 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:46:06,86 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:46:06,88 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:46:06,88 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:46:06,88 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17058, Requested 4093. Please try again in 3.451s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
20:46:06,93 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'employed his leisure hours on his instrument or\nin contemplation. Nothing could exceed the love and respect which the\nyounger cottagers exhibited towards their venerable companion. They\nperformed towards him every little office of affection and duty with\ngentleness, and he rewarded them by his benevolent smiles.\n\n“They were not entirely happy. The young man and his companion often\nwent apart and appeared to weep. I saw no cause for their unhappiness,\nbut I was deeply affected by it. If such lovely creatures were\nmiserable, it was less strange that I, an imperfect and solitary being,\nshould be wretched. Yet why were these gentle beings unhappy? They\npossessed a delightful house (for such it was in my eyes) and every\nluxury; they had a fire to warm them when chill and delicious viands\nwhen hungry; they were dressed in excellent clothes; and, still more,\nthey enjoyed one another’s company and speech, interchanging each day\nlooks of affection and kindness. What did their tears imply? Did they\nreally express pain? I was at first unable to solve these questions,\nbut perpetual attention and time explained to me many appearances which\nwere at first enigmatic.\n\n“A considerable period elapsed before I discovered one of the causes of\nthe uneasiness of this amiable family: it was poverty, and they\nsuffered that evil in a very distressing degree. Their nourishment\nconsisted entirely of the vegetables of their garden and the milk of\none cow, which gave very little during the winter, when its masters\ncould scarcely procure food to support it. They often, I believe,\nsuffered the pangs of hunger very poignantly, especially the two\nyounger cottagers, for several times they placed food before the old\nman when they reserved none for themselves.\n\n“This trait of kindness moved me sensibly. I had been accustomed,\nduring the night, to steal a part of their store for my own\nconsumption, but when I found that in doing this I inflicted pain on\nthe cottagers, I abstained and satisfied myself with berries, nuts, and\nroots which I gathered from a neighbouring wood.\n\n“I discovered also another means through which I was enabled to assist\ntheir labours. I found that the youth spent a great part of each day\nin collecting wood for the family fire, and during the night I often\ntook his tools, the use of which I quickly discovered, and brought home\nfiring sufficient for the consumption of several days.\n\n“I remember, the first time that I did this, the young woman, when she\nopened the door in the morning, appeared greatly astonished on seeing a great\npile of wood on the outside. She uttered some words in a loud voice, and the\nyouth joined her, who also expressed surprise. I observed, with pleasure,\nthat he did not go to the forest that day, but spent it in repairing the\ncottage and cultivating the garden.\n\n“By degrees I made a discovery of still greater moment. I found that\nthese people possessed a method of communicating their experience and\nfeelings to one another by articulate sounds. I perceived that the words\nthey spoke sometimes produced pleasure or pain, smiles or sadness, in the\nminds and countenances of the hearers. This was indeed a godlike science,\nand I ardently desired to become acquainted with it. But I was baffled in\nevery attempt I made for this purpose. Their pronunciation was quick, and\nthe words they uttered, not having any apparent connection with visible\nobjects, I was unable to discover any clue by which I could unravel the\nmystery of their reference. By great application, however, and after having\nremained during the space of several revolutions of the moon in my hovel, I\ndiscovered the names that were given to some of the most familiar objects of\ndiscourse; I learned and applied the words, _fire, milk, bread,_ and\n_wood._ I learned also the names of the cottagers themselves. The youth\nand his companion had each of them several names, but the old man had only\none, which was _father._ The girl was called _sister_ or\n_Agatha,_ and the youth _Felix, brother,_ or _son_. I cannot\ndescribe the delight I felt when I learned the ideas appropriated to each of\nthese sounds and was able to pronounce them. I distinguished several other\nwords without being able as yet to understand or apply them, such as _good,\ndearest, unhappy._\n\n“I spent the winter in this manner. The gentle manners and beauty of\nthe cottagers greatly endeared them to me; when they were unhappy, I\nfelt depressed; when they rejoiced, I sympathised in their joys. I saw\nfew human beings besides them, and if any other happened to enter the\ncottage, their harsh manners and rude gait only enhanced to me the\nsuperior accomplishments of my friends. The old man, I could perceive,\noften endeavoured to encourage his children, as sometimes I found that\nhe called them, to cast off their melancholy. He would talk in a\ncheerful accent, with an expression of goodness that bestowed pleasure\neven upon me. Agatha listened with respect, her eyes sometimes filled\nwith tears, which she endeavoured to wipe away unperceived; but I\ngenerally found that her countenance and tone were more cheerful after\nhaving listened to the exhortations of her father. It was not thus\nwith Felix. He was always the saddest of the group, and even to my\nunpractised senses, he appeared to have suffered more deeply than his\nfriends. But if his countenance was more sorrowful, his voice was more\ncheer'}
20:46:06,378 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:46:06,379 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:46:06,379 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:46:08,952 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
20:46:08,953 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
20:46:08,953 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
20:46:13,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.850660078926012. input_tokens=34, output_tokens=1315
20:46:15,959 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:46:36,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.979640031931922. input_tokens=34, output_tokens=2069
20:47:07,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.575939618982375. input_tokens=34, output_tokens=1917
20:47:41,252 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:47:44,769 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:47:50,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8827438340522349. input_tokens=2935, output_tokens=695
20:48:30,513 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:48:44,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.880857273004949. input_tokens=2936, output_tokens=1070
20:49:10,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 3.842021126067266. input_tokens=34, output_tokens=2058
20:49:37,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.5143556389957666. input_tokens=34, output_tokens=1695
20:50:18,642 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:50:28,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0403090990148485. input_tokens=2935, output_tokens=767
20:51:02,193 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:51:09,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6821311789099127. input_tokens=2936, output_tokens=575
20:51:49,742 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:52:01,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.252868273993954. input_tokens=2936, output_tokens=867
20:52:40,73 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:52:45,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6102538590785116. input_tokens=2936, output_tokens=430
20:53:21,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.620057211956009. input_tokens=34, output_tokens=2608
20:54:00,55 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:54:11,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1128556790063158. input_tokens=2936, output_tokens=832
20:54:52,256 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:55:02,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.9572759360307828. input_tokens=2936, output_tokens=748
20:55:42,919 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:55:51,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7828265180578455. input_tokens=2936, output_tokens=637
20:56:28,771 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:56:39,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0703603079309687. input_tokens=2936, output_tokens=815
20:57:16,897 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:57:27,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.999788440996781. input_tokens=2936, output_tokens=796
20:58:08,386 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:58:16,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7929586069658399. input_tokens=2936, output_tokens=597
20:58:54,416 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:59:05,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.447049548965879. input_tokens=2936, output_tokens=849
20:59:46,856 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
20:59:56,833 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.278117862995714. input_tokens=2937, output_tokens=748
21:00:34,33 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:00:37,311 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:00:41,371 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:00:46,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.573089712066576. input_tokens=2936, output_tokens=928
21:01:21,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.272040084004402. input_tokens=34, output_tokens=1653
21:01:35,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.056662476039492. input_tokens=34, output_tokens=2038
21:02:16,458 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:02:23,83 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:02:24,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0483212469844148. input_tokens=2936, output_tokens=640
21:02:25,747 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:02:28,858 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:02:29,122 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:29,124 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:29,124 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:33,621 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:02:33,964 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:33,966 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:33,966 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:35,864 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:35,866 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:35,866 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:38,522 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:38,524 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:38,524 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:38,524 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19006, Requested 3670. Please try again in 8.028s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
21:02:38,538 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'Fortunately the books were written in the language, the elements of which I\nhad acquired at the cottage; they consisted of _Paradise Lost_, a volume\nof _Plutarch’s Lives_, and the _Sorrows of Werter_. The\npossession of these treasures gave me extreme delight; I now continually\nstudied and exercised my mind upon these histories, whilst my friends were\nemployed in their ordinary occupations.\n\n“I can hardly describe to you the effect of these books. They produced\nin me an infinity of new images and feelings, that sometimes raised me\nto ecstasy, but more frequently sunk me into the lowest dejection. In\nthe _Sorrows of Werter_, besides the interest of its simple and affecting\nstory, so many opinions are canvassed and so many lights thrown upon\nwhat had hitherto been to me obscure subjects that I found in it a\nnever-ending source of speculation and astonishment. The gentle and\ndomestic manners it described, combined with lofty sentiments and\nfeelings, which had for their object something out of self, accorded\nwell with my experience among my protectors and with the wants which\nwere for ever alive in my own bosom. But I thought Werter himself a\nmore divine being than I had ever beheld or imagined; his character\ncontained no pretension, but it sank deep. The disquisitions upon\ndeath and suicide were calculated to fill me with wonder. I did not\npretend to enter into the merits of the case, yet I inclined towards\nthe opinions of the hero, whose extinction I wept, without precisely\nunderstanding it.\n\n“As I read, however, I applied much personally to my own feelings and\ncondition. I found myself similar yet at the same time strangely\nunlike to the beings concerning whom I read and to whose conversation I\nwas a listener. I sympathised with and partly understood them, but I\nwas unformed in mind; I was dependent on none and related to none.\n‘The path of my departure was free,’ and there was none to lament my\nannihilation. My person was hideous and my stature gigantic. What did\nthis mean? Who was I? What was I? Whence did I come? What was my\ndestination? These questions continually recurred, but I was unable to\nsolve them.\n\n“The volume of _Plutarch’s Lives_ which I possessed contained the\nhistories of the first founders of the ancient republics. This book\nhad a far different effect upon me from the _Sorrows of Werter_. I\nlearned from Werter’s imaginations despondency and gloom, but Plutarch\ntaught me high thoughts; he elevated me above the wretched sphere of my\nown reflections, to admire and love the heroes of past ages. Many\nthings I read surpassed my understanding and experience. I had a very\nconfused knowledge of kingdoms, wide extents of country, mighty rivers,\nand boundless seas. But I was perfectly unacquainted with towns and\nlarge assemblages of men. The cottage of my protectors had been the\nonly school in which I had studied human nature, but this book\ndeveloped new and mightier scenes of action. I read of men concerned\nin public affairs, governing or massacring their species. I felt the\ngreatest ardour for virtue rise within me, and abhorrence for vice, as\nfar as I understood the signification of those terms, relative as they\nwere, as I applied them, to pleasure and pain alone. Induced by these\nfeelings, I was of course led to admire peaceable lawgivers, Numa,\nSolon, and Lycurgus, in preference to Romulus and Theseus. The\npatriarchal lives of my protectors caused these impressions to take a\nfirm hold on my mind; perhaps, if my first introduction to humanity had\nbeen made by a young soldier, burning for glory and slaughter, I should\nhave been imbued with different sensations.\n\n“But _Paradise Lost_ excited different and far deeper emotions. I read\nit, as I had read the other volumes which had fallen into my hands, as\na true history. It moved every feeling of wonder and awe that the\npicture of an omnipotent God warring with his creatures was capable of\nexciting. I often referred the several situations, as their similarity\nstruck me, to my own. Like Adam, I was apparently united by no link to\nany other being in existence; but his state was far different from mine\nin every other respect. He had come forth from the hands of God a\nperfect creature, happy and prosperous, guarded by the especial care of\nhis Creator; he was allowed to converse with and acquire knowledge from\nbeings of a superior nature, but I was wretched, helpless, and alone.\nMany times I considered Satan as the fitter emblem of my condition, for\noften, like him, when I viewed the bliss of my protectors, the bitter\ngall of envy rose within me.\n\n“Another circumstance strengthened and confirmed these feelings. Soon\nafter my arrival in the hovel I discovered some papers in the pocket of\nthe dress which I had taken from your laboratory. At first I had\nneglected them, but now that I was able to decipher the characters in\nwhich they were written, I began to study them with diligence. It was\nyour journal of the four months that preceded my creation. You\nminutely described in these papers every step you took in the progress\nof your work; this history was mingled with accounts of domestic\noccurrences. You doubtless recollect these papers. Here they are.\nEverything is related in them which bears reference to my accursed'}
21:02:38,813 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:38,815 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:38,815 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:40,635 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:40,637 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:40,637 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:41,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.6595535590313375. input_tokens=34, output_tokens=1194
21:02:43,8 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:43,9 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:43,9 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:43,10 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17521, Requested 3578. Please try again in 3.296s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
21:02:43,14 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'not;\nand then I shall be happy again, even after the sad death of my little\nWilliam.”\n\n“She is innocent, my Elizabeth,” said I, “and that shall\nbe proved; fear nothing, but let your spirits be cheered by the assurance\nof her acquittal.”\n\n“How kind and generous you are! every one else believes in her guilt,\nand that made me wretched, for I knew that it was impossible: and to\nsee every one else prejudiced in so deadly a manner rendered me\nhopeless and despairing.” She wept.\n\n“Dearest niece,” said my father, “dry your tears. If she\nis, as you believe, innocent, rely on the justice of our laws, and the\nactivity with which I shall prevent the slightest shadow of\npartiality.”\n\n\n\n\nChapter 8\n\n\nWe passed a few sad hours until eleven o’clock, when the trial was to\ncommence. My father and the rest of the family being obliged to attend\nas witnesses, I accompanied them to the court. During the whole of\nthis wretched mockery of justice I suffered living torture. It was to\nbe decided whether the result of my curiosity and lawless devices would\ncause the death of two of my fellow beings: one a smiling babe full of\ninnocence and joy, the other far more dreadfully murdered, with every\naggravation of infamy that could make the murder memorable in horror.\nJustine also was a girl of merit and possessed qualities which promised\nto render her life happy; now all was to be obliterated in an\nignominious grave, and I the cause! A thousand times rather would I\nhave confessed myself guilty of the crime ascribed to Justine, but I\nwas absent when it was committed, and such a declaration would have\nbeen considered as the ravings of a madman and would not have\nexculpated her who suffered through me.\n\nThe appearance of Justine was calm. She was dressed in mourning, and\nher countenance, always engaging, was rendered, by the solemnity of her\nfeelings, exquisitely beautiful. Yet she appeared confident in\ninnocence and did not tremble, although gazed on and execrated by\nthousands, for all the kindness which her beauty might otherwise have\nexcited was obliterated in the minds of the spectators by the\nimagination of the enormity she was supposed to have committed. She\nwas tranquil, yet her tranquillity was evidently constrained; and as\nher confusion had before been adduced as a proof of her guilt, she\nworked up her mind to an appearance of courage. When she entered the\ncourt she threw her eyes round it and quickly discovered where we were\nseated. A tear seemed to dim her eye when she saw us, but she quickly\nrecovered herself, and a look of sorrowful affection seemed to attest\nher utter guiltlessness.\n\nThe trial began, and after the advocate against her had stated the\ncharge, several witnesses were called. Several strange facts combined\nagainst her, which might have staggered anyone who had not such proof\nof her innocence as I had. She had been out the whole of the night on\nwhich the murder had been committed and towards morning had been\nperceived by a market-woman not far from the spot where the body of the\nmurdered child had been afterwards found. The woman asked her what she\ndid there, but she looked very strangely and only returned a confused\nand unintelligible answer. She returned to the house about eight\no’clock, and when one inquired where she had passed the night, she\nreplied that she had been looking for the child and demanded earnestly\nif anything had been heard concerning him. When shown the body, she\nfell into violent hysterics and kept her bed for several days. The\npicture was then produced which the servant had found in her pocket;\nand when Elizabeth, in a faltering voice, proved that it was the same\nwhich, an hour before the child had been missed, she had placed round\nhis neck, a murmur of horror and indignation filled the court.\n\nJustine was called on for her defence. As the trial had proceeded, her\ncountenance had altered. Surprise, horror, and misery were strongly\nexpressed. Sometimes she struggled with her tears, but when she was\ndesired to plead, she collected her powers and spoke in an audible\nalthough variable voice.\n\n“God knows,” she said, “how entirely I am innocent. But I\ndo not pretend that my protestations should acquit me; I rest my innocence\non a plain and simple explanation of the facts which have been adduced\nagainst me, and I hope the character I have always borne will incline my\njudges to a favourable interpretation where any circumstance appears\ndoubtful or suspicious.”\n\nShe then related that, by the permission of Elizabeth, she had passed\nthe evening of the night on which the murder had been committed at the\nhouse of an aunt at Chêne, a village situated at about a league from\nGeneva. On her return, at about nine o’clock, she met a man who asked\nher if she had seen anything of the child who was lost. She was\nalarmed by this account and passed several hours in looking for him,\nwhen the gates of Geneva were shut, and she was forced to remain\nseveral hours of the night in a barn belonging to a cottage, being\nunwilling to call up the inhabitants, to whom she was well known. Most\nof the night she spent here watching; towards morning she believed that\nshe slept for a few minutes; some steps disturbed her, and she awoke.\nIt was dawn, and she quitted her asylum'}
21:02:43,541 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:43,543 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:43,543 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:45,637 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:45,639 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:45,639 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:51,920 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:02:52,444 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:52,445 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:52,446 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:54,752 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:54,754 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:54,755 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:57,687 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:57,689 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:57,689 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:57,689 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17976, Requested 3604. Please try again in 4.74s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
21:02:57,693 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'with an expression of unbounded wonder,\n“My dearest Victor, what infatuation is this? My dear son, I entreat\nyou never to make such an assertion again.”\n\n“I am not mad,” I cried energetically; “the sun and the heavens, who\nhave viewed my operations, can bear witness of my truth. I am the\nassassin of those most innocent victims; they died by my machinations.\nA thousand times would I have shed my own blood, drop by drop, to have\nsaved their lives; but I could not, my father, indeed I could not\nsacrifice the whole human race.”\n\nThe conclusion of this speech convinced my father that my ideas were\nderanged, and he instantly changed the subject of our conversation and\nendeavoured to alter the course of my thoughts. He wished as much as\npossible to obliterate the memory of the scenes that had taken place in\nIreland and never alluded to them or suffered me to speak of my\nmisfortunes.\n\nAs time passed away I became more calm; misery had her dwelling in my\nheart, but I no longer talked in the same incoherent manner of my own\ncrimes; sufficient for me was the consciousness of them. By the utmost\nself-violence I curbed the imperious voice of wretchedness, which\nsometimes desired to declare itself to the whole world, and my manners\nwere calmer and more composed than they had ever been since my journey\nto the sea of ice.\n\nA few days before we left Paris on our way to Switzerland, I received the\nfollowing letter from Elizabeth:\n\n“My dear Friend,\n\n“It gave me the greatest pleasure to receive a letter from my uncle\ndated at Paris; you are no longer at a formidable distance, and I may\nhope to see you in less than a fortnight. My poor cousin, how much you\nmust have suffered! I expect to see you looking even more ill than\nwhen you quitted Geneva. This winter has been passed most miserably,\ntortured as I have been by anxious suspense; yet I hope to see peace in\nyour countenance and to find that your heart is not totally void of\ncomfort and tranquillity.\n\n“Yet I fear that the same feelings now exist that made you so miserable\na year ago, even perhaps augmented by time. I would not disturb you at\nthis period, when so many misfortunes weigh upon you, but a\nconversation that I had with my uncle previous to his departure renders\nsome explanation necessary before we meet.\n\nExplanation! You may possibly say, What can Elizabeth have to explain? If\nyou really say this, my questions are answered and all my doubts satisfied.\nBut you are distant from me, and it is possible that you may dread and yet\nbe pleased with this explanation; and in a probability of this being the\ncase, I dare not any longer postpone writing what, during your absence, I\nhave often wished to express to you but have never had the courage to begin.\n\n“You well know, Victor, that our union had been the favourite plan of\nyour parents ever since our infancy. We were told this when young, and\ntaught to look forward to it as an event that would certainly take\nplace. We were affectionate playfellows during childhood, and, I\nbelieve, dear and valued friends to one another as we grew older. But\nas brother and sister often entertain a lively affection towards each\nother without desiring a more intimate union, may not such also be our\ncase? Tell me, dearest Victor. Answer me, I conjure you by our mutual\nhappiness, with simple truth—Do you not love another?\n\n“You have travelled; you have spent several years of your life at\nIngolstadt; and I confess to you, my friend, that when I saw you last\nautumn so unhappy, flying to solitude from the society of every\ncreature, I could not help supposing that you might regret our\nconnection and believe yourself bound in honour to fulfil the wishes of\nyour parents, although they opposed themselves to your inclinations.\nBut this is false reasoning. I confess to you, my friend, that I love\nyou and that in my airy dreams of futurity you have been my constant\nfriend and companion. But it is your happiness I desire as well as my\nown when I declare to you that our marriage would render me eternally\nmiserable unless it were the dictate of your own free choice. Even now\nI weep to think that, borne down as you are by the cruellest\nmisfortunes, you may stifle, by the word _honour_, all hope of that\nlove and happiness which would alone restore you to yourself. I, who\nhave so disinterested an affection for you, may increase your miseries\ntenfold by being an obstacle to your wishes. Ah! Victor, be assured\nthat your cousin and playmate has too sincere a love for you not to be\nmade miserable by this supposition. Be happy, my friend; and if you\nobey me in this one request, remain satisfied that nothing on earth\nwill have the power to interrupt my tranquillity.\n\n“Do not let this letter disturb you; do not answer tomorrow, or the\nnext day, or even until you come, if it will give you pain. My uncle\nwill send me news of your health, and if I see but one smile on your\nlips when we meet, occasioned by this or any other exertion of mine, I\nshall need no other happiness.\n\n“Elizabeth Lavenza.\n\n\n\n“Geneva, May 18th, 17—”\n\n\n\nThis letter revived in my memory what I had before forgotten, the threat of\nthe fiend—'}
21:02:57,929 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:57,931 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:57,932 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:02:59,784 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:02:59,785 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:02:59,786 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:06,256 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:03:06,593 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:06,595 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:06,595 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:07,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.108551970915869. input_tokens=34, output_tokens=1459
21:03:07,921 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:07,923 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:07,923 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:11,478 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:11,480 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:11,481 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:11,481 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19443, Requested 3429. Please try again in 8.613999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
21:03:11,484 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '; I had learned from\nthese the relative situations of the different countries of the earth.\nYou had mentioned Geneva as the name of your native town, and towards\nthis place I resolved to proceed.\n\n“But how was I to direct myself? I knew that I must travel in a\nsouthwesterly direction to reach my destination, but the sun was my\nonly guide. I did not know the names of the towns that I was to pass\nthrough, nor could I ask information from a single human being; but I\ndid not despair. From you only could I hope for succour, although\ntowards you I felt no sentiment but that of hatred. Unfeeling,\nheartless creator! You had endowed me with perceptions and passions\nand then cast me abroad an object for the scorn and horror of mankind.\nBut on you only had I any claim for pity and redress, and from you I\ndetermined to seek that justice which I vainly attempted to gain from\nany other being that wore the human form.\n\n“My travels were long and the sufferings I endured intense. It was\nlate in autumn when I quitted the district where I had so long resided.\nI travelled only at night, fearful of encountering the visage of a\nhuman being. Nature decayed around me, and the sun became heatless;\nrain and snow poured around me; mighty rivers were frozen; the surface\nof the earth was hard and chill, and bare, and I found no shelter. Oh,\nearth! How often did I imprecate curses on the cause of my being! The\nmildness of my nature had fled, and all within me was turned to gall\nand bitterness. The nearer I approached to your habitation, the more\ndeeply did I feel the spirit of revenge enkindled in my heart. Snow\nfell, and the waters were hardened, but I rested not. A few incidents\nnow and then directed me, and I possessed a map of the country; but I\noften wandered wide from my path. The agony of my feelings allowed me\nno respite; no incident occurred from which my rage and misery could\nnot extract its food; but a circumstance that happened when I arrived\non the confines of Switzerland, when the sun had recovered its warmth\nand the earth again began to look green, confirmed in an especial\nmanner the bitterness and horror of my feelings.\n\n“I generally rested during the day and travelled only when I was\nsecured by night from the view of man. One morning, however, finding\nthat my path lay through a deep wood, I ventured to continue my journey\nafter the sun had risen; the day, which was one of the first of spring,\ncheered even me by the loveliness of its sunshine and the balminess of\nthe air. I felt emotions of gentleness and pleasure, that had long\nappeared dead, revive within me. Half surprised by the novelty of\nthese sensations, I allowed myself to be borne away by them, and\nforgetting my solitude and deformity, dared to be happy. Soft tears\nagain bedewed my cheeks, and I even raised my humid eyes with\nthankfulness towards the blessed sun, which bestowed such joy upon me.\n\n“I continued to wind among the paths of the wood, until I came to its\nboundary, which was skirted by a deep and rapid river, into which many\nof the trees bent their branches, now budding with the fresh spring.\nHere I paused, not exactly knowing what path to pursue, when I heard\nthe sound of voices, that induced me to conceal myself under the shade\nof a cypress. I was scarcely hid when a young girl came running\ntowards the spot where I was concealed, laughing, as if she ran from\nsomeone in sport. She continued her course along the precipitous sides\nof the river, when suddenly her foot slipped, and she fell into the\nrapid stream. I rushed from my hiding-place and with extreme labour,\nfrom the force of the current, saved her and dragged her to shore. She\nwas senseless, and I endeavoured by every means in my power to restore\nanimation, when I was suddenly interrupted by the approach of a rustic,\nwho was probably the person from whom she had playfully fled. On\nseeing me, he darted towards me, and tearing the girl from my arms,\nhastened towards the deeper parts of the wood. I followed speedily, I\nhardly knew why; but when the man saw me draw near, he aimed a gun,\nwhich he carried, at my body and fired. I sank to the ground, and my\ninjurer, with increased swiftness, escaped into the wood.\n\n“This was then the reward of my benevolence! I had saved a human being\nfrom destruction, and as a recompense I now writhed under the miserable\npain of a wound which shattered the flesh and bone. The feelings of\nkindness and gentleness which I had entertained but a few moments\nbefore gave place to hellish rage and gnashing of teeth. Inflamed by\npain, I vowed eternal hatred and vengeance to all mankind. But the\nagony of my wound overcame me; my pulses paused, and I fainted.\n\n“For some weeks I led a miserable life in the woods, endeavouring to\ncure the wound which I had received. The ball had entered my shoulder,\nand I knew not whether it had remained there or passed through; at any\nrate I had no means of extracting it. My sufferings were augmented\nalso by the oppressive sense of the injustice and ingratitude of their\ninfliction. My daily vows rose for revenge—a deep and deadly revenge,\nsuch as would alone compensate for the outrages and anguish I had\nendured'}
21:03:12,46 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:12,48 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:12,48 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:13,651 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:13,653 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:13,653 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:16,35 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:16,36 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:16,37 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:16,37 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17829, Requested 3634. Please try again in 4.387s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
21:03:16,40 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'moments vengeance, that\nburned within me, died in my heart, and I pursued my path towards the\ndestruction of the dæmon more as a task enjoined by heaven, as the\nmechanical impulse of some power of which I was unconscious, than as the\nardent desire of my soul.\n\nWhat his feelings were whom I pursued I cannot know. Sometimes, indeed, he\nleft marks in writing on the barks of the trees or cut in stone that guided\nme and instigated my fury. “My reign is not yet\nover”—these words were legible in one of these\ninscriptions—“you live, and my power is complete. Follow me; I\nseek the everlasting ices of the north, where you will feel the misery of\ncold and frost, to which I am impassive. You will find near this place, if\nyou follow not too tardily, a dead hare; eat and be refreshed. Come on, my\nenemy; we have yet to wrestle for our lives, but many hard and miserable\nhours must you endure until that period shall arrive.”\n\nScoffing devil! Again do I vow vengeance; again do I devote thee,\nmiserable fiend, to torture and death. Never will I give up my search\nuntil he or I perish; and then with what ecstasy shall I join my\nElizabeth and my departed friends, who even now prepare for me the\nreward of my tedious toil and horrible pilgrimage!\n\nAs I still pursued my journey to the northward, the snows thickened and the\ncold increased in a degree almost too severe to support. The peasants were\nshut up in their hovels, and only a few of the most hardy ventured forth to\nseize the animals whom starvation had forced from their hiding-places to\nseek for prey. The rivers were covered with ice, and no fish could be\nprocured; and thus I was cut off from my chief article of maintenance.\n\nThe triumph of my enemy increased with the difficulty of my labours. One\ninscription that he left was in these words: “Prepare! Your toils\nonly begin; wrap yourself in furs and provide food, for we shall soon enter\nupon a journey where your sufferings will satisfy my everlasting\nhatred.”\n\nMy courage and perseverance were invigorated by these scoffing words; I\nresolved not to fail in my purpose, and calling on Heaven to support\nme, I continued with unabated fervour to traverse immense deserts,\nuntil the ocean appeared at a distance and formed the utmost boundary\nof the horizon. Oh! How unlike it was to the blue seasons of the\nsouth! Covered with ice, it was only to be distinguished from land by\nits superior wildness and ruggedness. The Greeks wept for joy when\nthey beheld the Mediterranean from the hills of Asia, and hailed with\nrapture the boundary of their toils. I did not weep, but I knelt down\nand with a full heart thanked my guiding spirit for conducting me in\nsafety to the place where I hoped, notwithstanding my adversary’s gibe,\nto meet and grapple with him.\n\nSome weeks before this period I had procured a sledge and dogs and thus\ntraversed the snows with inconceivable speed. I know not whether the\nfiend possessed the same advantages, but I found that, as before I had\ndaily lost ground in the pursuit, I now gained on him, so much so that\nwhen I first saw the ocean he was but one day’s journey in advance, and\nI hoped to intercept him before he should reach the beach. With new\ncourage, therefore, I pressed on, and in two days arrived at a wretched\nhamlet on the seashore. I inquired of the inhabitants concerning the\nfiend and gained accurate information. A gigantic monster, they said,\nhad arrived the night before, armed with a gun and many pistols,\nputting to flight the inhabitants of a solitary cottage through fear of\nhis terrific appearance. He had carried off their store of winter\nfood, and placing it in a sledge, to draw which he had seized on a\nnumerous drove of trained dogs, he had harnessed them, and the same\nnight, to the joy of the horror-struck villagers, had pursued his\njourney across the sea in a direction that led to no land; and they\nconjectured that he must speedily be destroyed by the breaking of the\nice or frozen by the eternal frosts.\n\nOn hearing this information I suffered a temporary access of despair.\nHe had escaped me, and I must commence a destructive and almost endless\njourney across the mountainous ices of the ocean, amidst cold that few\nof the inhabitants could long endure and which I, the native of a\ngenial and sunny climate, could not hope to survive. Yet at the idea\nthat the fiend should live and be triumphant, my rage and vengeance\nreturned, and like a mighty tide, overwhelmed every other feeling.\nAfter a slight repose, during which the spirits of the dead hovered\nround and instigated me to toil and revenge, I prepared for my journey.\n\nI exchanged my land-sledge for one fashioned for the inequalities of\nthe Frozen Ocean, and purchasing a plentiful stock of provisions, I\ndeparted from land.\n\nI cannot guess how many days have passed since then, but I have endured\nmisery which nothing but the eternal sentiment of a just retribution\nburning within my heart could have enabled me to support. Immense and\nrugged mountains of ice often barred up my passage, and I often heard\nthe thunder of the ground sea, which threatened my destruction. But\nagain the frost came and made'}
21:03:16,722 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:16,723 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:16,724 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:18,388 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:18,390 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:18,390 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:26,688 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:03:27,48 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:27,49 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:27,49 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:28,619 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:28,621 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:28,621 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:31,766 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:31,767 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:31,767 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:31,768 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 19349, Requested 3694. Please try again in 9.128s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
21:03:31,771 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'the verge of\nthe horizon when he departed. I knew that I ought to hasten my descent\ntowards the valley, as I should soon be encompassed in darkness; but my\nheart was heavy, and my steps slow. The labour of winding among the\nlittle paths of the mountain and fixing my feet firmly as I advanced\nperplexed me, occupied as I was by the emotions which the occurrences\nof the day had produced. Night was far advanced when I came to the\nhalfway resting-place and seated myself beside the fountain. The stars\nshone at intervals as the clouds passed from over them; the dark pines\nrose before me, and every here and there a broken tree lay on the\nground; it was a scene of wonderful solemnity and stirred strange\nthoughts within me. I wept bitterly, and clasping my hands in agony, I\nexclaimed, “Oh! stars and clouds and winds, ye are all about to mock\nme; if ye really pity me, crush sensation and memory; let me become as\nnought; but if not, depart, depart, and leave me in darkness.”\n\nThese were wild and miserable thoughts, but I cannot describe to you\nhow the eternal twinkling of the stars weighed upon me and how I\nlistened to every blast of wind as if it were a dull ugly siroc on its\nway to consume me.\n\nMorning dawned before I arrived at the village of Chamounix; I took no\nrest, but returned immediately to Geneva. Even in my own heart I could\ngive no expression to my sensations—they weighed on me with a\nmountain’s weight and their excess destroyed my agony beneath them.\nThus I returned home, and entering the house, presented myself to the\nfamily. My haggard and wild appearance awoke intense alarm, but I\nanswered no question, scarcely did I speak. I felt as if I were placed\nunder a ban—as if I had no right to claim their sympathies—as if\nnever more might I enjoy companionship with them. Yet even thus I\nloved them to adoration; and to save them, I resolved to dedicate\nmyself to my most abhorred task. The prospect of such an occupation\nmade every other circumstance of existence pass before me like a dream,\nand that thought only had to me the reality of life.\n\n\n\n\nChapter 18\n\n\nDay after day, week after week, passed away on my return to Geneva; and\nI could not collect the courage to recommence my work. I feared the\nvengeance of the disappointed fiend, yet I was unable to overcome my\nrepugnance to the task which was enjoined me. I found that I could not\ncompose a female without again devoting several months to profound\nstudy and laborious disquisition. I had heard of some discoveries\nhaving been made by an English philosopher, the knowledge of which was\nmaterial to my success, and I sometimes thought of obtaining my\nfather’s consent to visit England for this purpose; but I clung to\nevery pretence of delay and shrank from taking the first step in an\nundertaking whose immediate necessity began to appear less absolute to\nme. A change indeed had taken place in me; my health, which had\nhitherto declined, was now much restored; and my spirits, when\nunchecked by the memory of my unhappy promise, rose proportionably. My\nfather saw this change with pleasure, and he turned his thoughts\ntowards the best method of eradicating the remains of my melancholy,\nwhich every now and then would return by fits, and with a devouring\nblackness overcast the approaching sunshine. At these moments I took\nrefuge in the most perfect solitude. I passed whole days on the lake\nalone in a little boat, watching the clouds and listening to the\nrippling of the waves, silent and listless. But the fresh air and\nbright sun seldom failed to restore me to some degree of composure, and\non my return I met the salutations of my friends with a readier smile\nand a more cheerful heart.\n\nIt was after my return from one of these rambles that my father,\ncalling me aside, thus addressed me,\n\n“I am happy to remark, my dear son, that you have resumed your former\npleasures and seem to be returning to yourself. And yet you are still\nunhappy and still avoid our society. For some time I was lost in\nconjecture as to the cause of this, but yesterday an idea struck me,\nand if it is well founded, I conjure you to avow it. Reserve on such a\npoint would be not only useless, but draw down treble misery on us all.”\n\nI trembled violently at his exordium, and my father continued—\n\n“I confess, my son, that I have always looked forward to your\nmarriage with our dear Elizabeth as the tie of our domestic comfort and the\nstay of my declining years. You were attached to each other from your\nearliest infancy; you studied together, and appeared, in dispositions and\ntastes, entirely suited to one another. But so blind is the experience of\nman that what I conceived to be the best assistants to my plan may have\nentirely destroyed it. You, perhaps, regard her as your sister, without any\nwish that she might become your wife. Nay, you may have met with another\nwhom you may love; and considering yourself as bound in honour to\nElizabeth, this struggle may occasion the poignant misery which you appear\nto feel.”\n\n“My dear father, reassure yourself. I love my cousin tenderly and\nsincerely. I never saw any woman who excited, as Elizabeth does, my\nwarmest admiration and affection. My future hopes and'}
21:03:31,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 2.9137212919304147. input_tokens=34, output_tokens=1458
21:03:32,47 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:32,49 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:32,49 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:34,387 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:34,389 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:34,389 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:37,673 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
21:03:37,674 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
21:03:37,675 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/3 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
21:03:37,675 root ERROR error extracting graph
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 122, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 161, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17428, Requested 3456. Please try again in 2.65s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}
21:03:37,678 graphrag.index.reporting.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'as! Yes; I cannot withstand their demands. I cannot lead them\nunwillingly to danger, and I must return.”\n\n“Do so, if you will; but I will not. You may give up your purpose, but\nmine is assigned to me by Heaven, and I dare not. I am weak, but\nsurely the spirits who assist my vengeance will endow me with\nsufficient strength.” Saying this, he endeavoured to spring from the\nbed, but the exertion was too great for him; he fell back and fainted.\n\nIt was long before he was restored, and I often thought that life was\nentirely extinct. At length he opened his eyes; he breathed with\ndifficulty and was unable to speak. The surgeon gave him a composing\ndraught and ordered us to leave him undisturbed. In the meantime he\ntold me that my friend had certainly not many hours to live.\n\nHis sentence was pronounced, and I could only grieve and be patient. I sat\nby his bed, watching him; his eyes were closed, and I thought he slept; but\npresently he called to me in a feeble voice, and bidding me come near,\nsaid, “Alas! The strength I relied on is gone; I feel that I shall\nsoon die, and he, my enemy and persecutor, may still be in being. Think\nnot, Walton, that in the last moments of my existence I feel that burning\nhatred and ardent desire of revenge I once expressed; but I feel myself\njustified in desiring the death of my adversary. During these last days I\nhave been occupied in examining my past conduct; nor do I find it blamable.\nIn a fit of enthusiastic madness I created a rational creature and was\nbound towards him to assure, as far as was in my power, his happiness and\nwell-being. This was my duty, but there was another still paramount to\nthat. My duties towards the beings of my own species had greater claims to\nmy attention because they included a greater proportion of happiness or\nmisery. Urged by this view, I refused, and I did right in refusing, to\ncreate a companion for the first creature. He showed unparalleled malignity\nand selfishness in evil; he destroyed my friends; he devoted to destruction\nbeings who possessed exquisite sensations, happiness, and wisdom; nor do I\nknow where this thirst for vengeance may end. Miserable himself that he may\nrender no other wretched, he ought to die. The task of his destruction was\nmine, but I have failed. When actuated by selfish and vicious motives, I\nasked you to undertake my unfinished work, and I renew this request now,\nwhen I am only induced by reason and virtue.\n\n“Yet I cannot ask you to renounce your country and friends to fulfil\nthis task; and now that you are returning to England, you will have\nlittle chance of meeting with him. But the consideration of these\npoints, and the well balancing of what you may esteem your duties, I\nleave to you; my judgment and ideas are already disturbed by the near\napproach of death. I dare not ask you to do what I think right, for I\nmay still be misled by passion.\n\n“That he should live to be an instrument of mischief disturbs me; in\nother respects, this hour, when I momentarily expect my release, is the\nonly happy one which I have enjoyed for several years. The forms of\nthe beloved dead flit before me, and I hasten to their arms. Farewell,\nWalton! Seek happiness in tranquillity and avoid ambition, even if it\nbe only the apparently innocent one of distinguishing yourself in\nscience and discoveries. Yet why do I say this? I have myself been\nblasted in these hopes, yet another may succeed.”\n\nHis voice became fainter as he spoke, and at length, exhausted by his\neffort, he sank into silence. About half an hour afterwards he\nattempted again to speak but was unable; he pressed my hand feebly, and\nhis eyes closed for ever, while the irradiation of a gentle smile passed\naway from his lips.\n\nMargaret, what comment can I make on the untimely extinction of this\nglorious spirit? What can I say that will enable you to understand the\ndepth of my sorrow? All that I should express would be inadequate and\nfeeble. My tears flow; my mind is overshadowed by a cloud of\ndisappointment. But I journey towards England, and I may there find\nconsolation.\n\nI am interrupted. What do these sounds portend? It is midnight; the\nbreeze blows fairly, and the watch on deck scarcely stir. Again there\nis a sound as of a human voice, but hoarser; it comes from the cabin\nwhere the remains of Frankenstein still lie. I must arise and examine.\nGood night, my sister.\n\nGreat God! what a scene has just taken place! I am yet dizzy with the\nremembrance of it. I hardly know whether I shall have the power to detail\nit; yet the tale which I have recorded would be incomplete without this\nfinal and wonderful catastrophe.\n\nI entered the cabin where lay the remains of my ill-fated and admirable\nfriend. Over him hung a form which I cannot find words to\ndescribe—gigantic in stature, yet uncouth and distorted in its\nproportions. As he hung over the coffin, his face was concealed by long\nlocks of ragged hair; but one vast hand was extended, in colour and\napparent texture like that of a mummy. When he heard the sound of my\napproach, he ceased to utter exclamations of grief and'}
21:03:58,485 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 3.4279662540648133. input_tokens=34, output_tokens=1664
21:04:34,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 4.247820332995616. input_tokens=34, output_tokens=2218
21:05:06,309 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:05:19,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.677262105047703. input_tokens=2935, output_tokens=979
21:05:25,962 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:06:00,141 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:06:19,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.958669896936044. input_tokens=2936, output_tokens=1432
21:06:23,208 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:06:49,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.9690278810448945. input_tokens=34, output_tokens=1966
21:07:25,180 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 503 Service Unavailable"
21:07:25,187 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: Of course, we hope\nthat you will support the Project Gutenberg™ mission of promoting\nfree access to electronic works by freely sharing Project Gutenberg™\nworks in compliance with the terms of this agreement for keeping the\nProject Gutenberg™ name associated with the work. You can easily\ncomply with the terms of this agreement by keeping this work in the\nsame format with its attached full Project Gutenberg™ License when\nyou share it without charge with others.\n\n1.D. The copyright laws of the place where you are located also govern\nwhat you can do with this work. Copyright laws in most countries are\nin a constant state of change. If you are outside the United States,\ncheck the laws of your country in addition to the terms of this\nagreement before downloading, copying, displaying, performing,\ndistributing or creating derivative works based on this work or any\nother Project Gutenberg™ work. The Foundation makes no\nrepresentations concerning the copyright status of any work in any\ncountry other than the United States.\n\n1.E. Unless you have removed all references to Project Gutenberg:\n\n1.E.1. The following sentence, with active links to, or other\nimmediate access to, the full Project Gutenberg™ License must appear\nprominently whenever any copy of a Project Gutenberg™ work (any work\non which the phrase “Project Gutenberg” appears, or with which the\nphrase “Project Gutenberg” is associated) is accessed, displayed,\nperformed, viewed, copied or distributed:\n\n    This eBook is for the use of anyone anywhere in the United States and most\n    other parts of the world at no cost and with almost no restrictions\n    whatsoever. You may copy it, give it away or re-use it under the terms\n    of the Project Gutenberg License included with this eBook or online\n    at www.gutenberg.org. If you\n    are not located in the United States, you will have to check the laws\n    of the country where you are located before using this eBook.\n  \n1.E.2. If an individual Project Gutenberg™ electronic work is\nderived from texts not protected by U.S. copyright law (does not\ncontain a notice indicating that it is posted with permission of the\ncopyright holder), the work can be copied and distributed to anyone in\nthe United States without paying any fees or charges. If you are\nredistributing or providing access to a work with the phrase “Project\nGutenberg” associated with or appearing on the work, you must comply\neither with the requirements of paragraphs 1.E.1 through 1.E.7 or\nobtain permission for the use of the work and the Project Gutenberg™\ntrademark as set forth in paragraphs 1.E.8 or 1.E.9.\n\n1.E.3. If an individual Project Gutenberg™ electronic work is posted\nwith the permission of the copyright holder, your use and distribution\nmust comply with both paragraphs 1.E.1 through 1.E.7 and any\nadditional terms imposed by the copyright holder. Additional terms\nwill be linked to the Project Gutenberg™ License for all works\nposted with the permission of the copyright holder found at the\nbeginning of this work.\n\n1.E.4. Do not unlink or detach or remove the full Project Gutenberg™\nLicense terms from this work, or any files containing a part of this\nwork or any other work associated with Project Gutenberg™.\n\n1.E.5. Do not copy, display, perform, distribute or redistribute this\nelectronic work, or any part of this electronic work, without\nprominently displaying the sentence set forth in paragraph 1.E.1 with\nactive links or immediate access to the full terms of the Project\nGutenberg™ License.\n\n1.E.6. You may convert to and distribute this work in any binary,\ncompressed, marked up, nonproprietary or proprietary form, including\nany word processing or hypertext form. However, if you provide access\nto or distribute copies of a Project Gutenberg™ work in a format\nother than “Plain Vanilla ASCII” or other format used in the official\nversion posted on the official Project Gutenberg™ website\n(www.gutenberg.org), you must, at no additional cost, fee or expense\nto the user, provide a copy, a means of exporting a copy, or a means\nof obtaining a copy upon request, of the work in its original “Plain\nVanilla ASCII” or other form. Any alternate format must include the\nfull Project Gutenberg™ License as specified in paragraph 1.E.1.\n\n1.E.7. Do not charge a fee for access to, viewing, displaying,\nperforming, copying or distributing any Project Gutenberg™ works\nunless you comply with paragraph 1.E.8 or 1.E.9.\n\n1.E.8. You may charge a reasonable fee for copies of or providing\naccess to or distributing Project Gutenberg™ electronic works\nprovided that:\n\n    • You pay a royalty fee of 20% of the gross profits you derive from\n        the use of Project Gutenberg™ works calculated using the method\n        you already use to calculate your applicable taxes. The fee is owed\n        to the owner of the Project Gutenberg™ trademark, but he has\n        agreed to donate royalties under this paragraph to the Project\n        Gutenberg Literary Archive Foundation. Royalty payments must be paid\n        within 60 days following each date on which you prepare (or are\n        legally required to prepare) your periodic tax returns. Royalty\n        payments should be clearly marked as such and sent to the Project\n        Gutenberg Literary Archive Foundation at the address specified in\n        Section 4, “Information about donations to the Project Gutenberg\n        Literary Archive Foundation.”\n    \n    • You provide a full refund of any money paid by a user who notifies\n        you in writing (or by e-mail) within 30\n######################\nOutput:'}
21:08:05,166 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:08:13,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.665855080005713. input_tokens=2936, output_tokens=603
21:08:16,849 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:08:39,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.636767501011491. input_tokens=34, output_tokens=1683
21:09:14,734 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:09:22,485 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8068573619239032. input_tokens=2934, output_tokens=581
21:09:25,232 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:09:42,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.744666400947608. input_tokens=34, output_tokens=1273
21:10:14,10 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:10:25,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.9469451239565387. input_tokens=2675, output_tokens=836
21:10:31,817 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:11:05,835 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:11:18,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.241789276013151. input_tokens=2936, output_tokens=938
21:11:22,209 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:11:47,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.860123466933146. input_tokens=34, output_tokens=1919
21:12:29,264 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:12:41,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.329069062951021. input_tokens=2935, output_tokens=947
21:12:48,431 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:13:13,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.2914402360329404. input_tokens=34, output_tokens=3112
21:13:59,863 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:13:59,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 1.7513557589845732. input_tokens=2936, output_tokens=389
21:14:06,271 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:14:41,20 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:14:50,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.444209865992889. input_tokens=2936, output_tokens=697
21:14:53,868 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:15:15,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.520122413057834. input_tokens=34, output_tokens=1612
21:16:00,976 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:16:16,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.745971102034673. input_tokens=2936, output_tokens=1182
21:16:20,994 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:16:45,813 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:17:01,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.5646313270553946. input_tokens=2936, output_tokens=1177
21:17:04,298 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:17:20,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.786986302002333. input_tokens=34, output_tokens=1226
21:18:03,294 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:18:17,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.375767318997532. input_tokens=2936, output_tokens=1042
21:18:21,584 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:18:51,239 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:19:03,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.4561032169731334. input_tokens=2936, output_tokens=903
21:19:07,397 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:19:27,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.250423515914008. input_tokens=34, output_tokens=2002
21:20:01,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.113426450989209. input_tokens=34, output_tokens=2018
21:20:25,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.38977975898888. input_tokens=34, output_tokens=2323
21:21:12,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.500192258041352. input_tokens=34, output_tokens=3169
21:22:04,833 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:22:42,336 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:22:42,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.691372935893014. input_tokens=2935, output_tokens=601
21:22:46,537 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:23:16,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.197123192017898. input_tokens=34, output_tokens=2235
21:23:58,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.48759889206849. input_tokens=34, output_tokens=4007
21:24:40,125 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:24:51,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0716449059545994. input_tokens=2934, output_tokens=816
21:24:54,266 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:25:14,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.2568557809572667. input_tokens=34, output_tokens=1535
21:25:58,141 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:25:58,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7205150350928307. input_tokens=2936, output_tokens=454
21:26:01,836 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:26:28,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.690239524934441. input_tokens=34, output_tokens=1980
21:27:03,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 2 retries took 5.513213905040175. input_tokens=34, output_tokens=3036
21:27:57,54 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:27:57,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.087455492001027. input_tokens=2936, output_tokens=618
21:28:00,200 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:28:19,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.1418048300547525. input_tokens=34, output_tokens=1460
21:28:49,508 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:28:58,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.0999880010494962. input_tokens=2936, output_tokens=662
21:29:05,789 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:29:51,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.525223559001461. input_tokens=2936, output_tokens=4002
21:29:58,967 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:30:33,380 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:30:46,396 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.508235721033998. input_tokens=2936, output_tokens=976
21:30:50,63 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:31:14,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.661929100053385. input_tokens=34, output_tokens=1821
21:31:52,779 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:32:04,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2722810819977894. input_tokens=2936, output_tokens=914
21:32:08,852 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:32:35,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.862628701957874. input_tokens=34, output_tokens=2026
21:33:11,615 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:33:27,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6314353470224887. input_tokens=2936, output_tokens=1217
21:33:31,156 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:33:51,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.307652462972328. input_tokens=34, output_tokens=1541
21:34:42,425 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:34:42,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8778607439016923. input_tokens=2936, output_tokens=558
21:34:48,800 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:35:22,681 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:35:40,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.8413198039634153. input_tokens=2936, output_tokens=1338
21:35:44,961 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:36:16,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.42855899001006. input_tokens=34, output_tokens=2364
21:37:03,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.623646244988777. input_tokens=34, output_tokens=4000
21:37:56,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.255468619987369. input_tokens=2936, output_tokens=4007
21:38:03,777 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:38:50,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.344489604933187. input_tokens=2936, output_tokens=4076
21:38:58,154 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:39:44,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.298277179012075. input_tokens=2936, output_tokens=4000
21:39:51,333 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:40:37,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.541343060089275. input_tokens=34, output_tokens=4000
21:41:30,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.588268088991754. input_tokens=34, output_tokens=4000
21:42:24,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.679918566020206. input_tokens=34, output_tokens=4000
21:43:17,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.532496677013114. input_tokens=34, output_tokens=4000
21:44:10,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.405741011025384. input_tokens=34, output_tokens=4000
21:45:04,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.941556481993757. input_tokens=2936, output_tokens=4001
21:45:11,485 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:45:57,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.448320885072462. input_tokens=34, output_tokens=4000
21:46:50,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.25299420801457. input_tokens=34, output_tokens=4000
21:47:44,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.3700208039954305. input_tokens=34, output_tokens=4000
21:48:37,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.343617535079829. input_tokens=34, output_tokens=4000
21:49:30,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.351456061936915. input_tokens=34, output_tokens=4002
21:50:24,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.187105691060424. input_tokens=34, output_tokens=4002
21:51:17,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.330687835928984. input_tokens=34, output_tokens=4000
21:51:17,586 datashaper.workflow.workflow INFO executing verb merge_graphs
21:51:17,777 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
21:51:17,982 graphrag.index.run INFO Running workflow: create_summarized_entities...
21:51:17,983 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
21:51:17,983 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
21:51:17,993 datashaper.workflow.workflow INFO executing verb summarize_descriptions
21:51:18,631 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:18,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5026271159294993. input_tokens=145, output_tokens=20
21:51:18,659 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:19,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5352419080445543. input_tokens=211, output_tokens=77
21:51:22,817 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:22,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9726095439400524. input_tokens=139, output_tokens=12
21:51:24,387 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:24,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4526678930269554. input_tokens=145, output_tokens=20
21:51:26,286 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:26,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4082757409196347. input_tokens=145, output_tokens=21
21:51:28,384 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:28,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.36716606793925166. input_tokens=148, output_tokens=45
21:51:30,874 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:30,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5752918239450082. input_tokens=152, output_tokens=45
21:51:34,149 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:34,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.060905392980203. input_tokens=160, output_tokens=83
21:51:36,774 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:37,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5050304409814999. input_tokens=151, output_tokens=55
21:51:39,449 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:40,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.36242470797151327. input_tokens=157, output_tokens=63
21:51:42,554 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:42,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5671599491033703. input_tokens=149, output_tokens=20
21:51:44,751 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:45,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.36183906497899443. input_tokens=164, output_tokens=68
21:51:48,491 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:50,440 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:50,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49512072291690856. input_tokens=149, output_tokens=42
21:51:51,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6448252300033346. input_tokens=168, output_tokens=119
21:51:54,327 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:55,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45900837902445346. input_tokens=164, output_tokens=69
21:51:57,217 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:51:58,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5423119609476998. input_tokens=150, output_tokens=59
21:52:00,39 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:00,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5597765940474346. input_tokens=155, output_tokens=30
21:52:02,339 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:02,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.30632664007134736. input_tokens=146, output_tokens=28
21:52:05,210 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:06,398 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3906505380291492. input_tokens=154, output_tokens=89
21:52:08,228 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:09,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5306942259194329. input_tokens=160, output_tokens=58
21:52:12,22 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:13,994 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:15,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5069472329923883. input_tokens=172, output_tokens=78
21:52:17,140 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:18,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.481794151943177. input_tokens=184, output_tokens=101
21:52:20,810 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:22,907 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:22,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6131049890536815. input_tokens=154, output_tokens=40
21:52:25,397 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:25,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5693054019939154. input_tokens=144, output_tokens=34
21:52:27,657 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:28,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4821434289915487. input_tokens=159, output_tokens=55
21:52:30,927 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:30,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8803680728888139. input_tokens=154, output_tokens=38
21:52:32,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6485181880416349. input_tokens=155, output_tokens=121
21:52:37,62 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:37,319 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:39,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.501867403043434. input_tokens=154, output_tokens=37
21:52:41,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5000974369468167. input_tokens=158, output_tokens=79
21:52:45,123 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:47,49 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:49,185 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:51,740 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:52:53,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8562879998935387. input_tokens=171, output_tokens=66
21:52:56,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3749911729246378. input_tokens=189, output_tokens=75
21:52:58,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3103889439953491. input_tokens=137, output_tokens=13
21:53:03,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45750814699567854. input_tokens=144, output_tokens=35
21:53:03,473 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:06,458 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:08,566 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:11,172 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:13,277 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:15,727 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:19,921 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:20,877 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:22,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.564310336019844. input_tokens=151, output_tokens=48
21:53:25,350 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:27,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45298357401043177. input_tokens=193, output_tokens=129
21:53:29,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6506870670709759. input_tokens=155, output_tokens=38
21:53:32,635 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:34,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.35083876096177846. input_tokens=159, output_tokens=65
21:53:36,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46255654795095325. input_tokens=149, output_tokens=52
21:53:40,18 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:41,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2483799259644. input_tokens=166, output_tokens=51
21:53:44,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7987208240665495. input_tokens=173, output_tokens=53
21:53:47,603 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:49,936 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:52,34 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:54,386 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:53:56,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46112203202210367. input_tokens=178, output_tokens=150
21:53:58,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5424185339361429. input_tokens=196, output_tokens=138
21:54:01,470 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:04,754 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:05,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7517429189756513. input_tokens=179, output_tokens=193
21:54:08,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6425796149997041. input_tokens=150, output_tokens=58
21:54:11,511 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:13,543 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:15,913 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:17,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0152855010237545. input_tokens=147, output_tokens=23
21:54:20,768 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:23,490 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:25,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6107483889209107. input_tokens=146, output_tokens=72
21:54:27,693 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:30,172 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:32,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5484099249588326. input_tokens=203, output_tokens=203
21:54:36,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4227333969902247. input_tokens=154, output_tokens=46
21:54:38,290 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:39,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.392559926956892. input_tokens=225, output_tokens=121
21:54:42,562 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:44,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9421513719717041. input_tokens=150, output_tokens=41
21:54:46,965 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:49,373 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:51,800 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:53,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5346193189034238. input_tokens=165, output_tokens=196
21:54:56,716 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:54:58,879 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:01,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5773250369820744. input_tokens=176, output_tokens=64
21:55:03,928 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:06,136 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:08,577 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:10,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5691678570583463. input_tokens=210, output_tokens=206
21:55:13,402 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:15,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.28253967699129134. input_tokens=149, output_tokens=15
21:55:17,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5445988279534504. input_tokens=145, output_tokens=41
21:55:20,791 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:23,284 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:25,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.853887108969502. input_tokens=246, output_tokens=81
21:55:28,46 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:29,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9146548949647695. input_tokens=271, output_tokens=156
21:55:33,453 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:34,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5073250470450148. input_tokens=174, output_tokens=31
21:55:37,443 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:39,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5006496289279312. input_tokens=156, output_tokens=112
21:55:41,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5056753060780466. input_tokens=171, output_tokens=35
21:55:44,967 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:47,143 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:49,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.2799633169779554. input_tokens=146, output_tokens=10
21:55:52,4 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:53,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5374232470057905. input_tokens=147, output_tokens=181
21:55:56,842 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:55:58,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5465266390237957. input_tokens=185, output_tokens=68
21:56:01,631 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:03,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3262001759139821. input_tokens=149, output_tokens=39
21:56:05,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.35898464103229344. input_tokens=156, output_tokens=33
21:56:08,483 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:10,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5789471999742091. input_tokens=270, output_tokens=112
21:56:13,259 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:15,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6395101259695366. input_tokens=183, output_tokens=216
21:56:18,448 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:20,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3782550519099459. input_tokens=152, output_tokens=82
21:56:23,24 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:24,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5522800120525062. input_tokens=249, output_tokens=104
21:56:28,370 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:30,59 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:32,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5933825899846852. input_tokens=166, output_tokens=92
21:56:34,837 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:36,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1924765610601753. input_tokens=148, output_tokens=29
21:56:39,323 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8873191340826452. input_tokens=244, output_tokens=260
21:56:41,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.37221139995381236. input_tokens=149, output_tokens=66
21:56:44,517 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:47,185 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:48,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6894623590633273. input_tokens=306, output_tokens=104
21:56:52,113 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:54,481 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:56:56,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4503454070072621. input_tokens=155, output_tokens=98
21:56:58,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.708001950988546. input_tokens=153, output_tokens=154
21:57:00,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6340336540015414. input_tokens=158, output_tokens=30
21:57:03,962 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:05,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.60247406095732. input_tokens=163, output_tokens=179
21:57:08,681 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:11,302 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:13,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46626515698153526. input_tokens=232, output_tokens=89
21:57:16,101 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:17,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6785789100686088. input_tokens=285, output_tokens=121
21:57:20,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.40381551906466484. input_tokens=153, output_tokens=46
21:57:22,990 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:25,286 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:27,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7773282349808142. input_tokens=158, output_tokens=81
21:57:30,166 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:32,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.540548182092607. input_tokens=263, output_tokens=93
21:57:35,39 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:36,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4987686349777505. input_tokens=366, output_tokens=138
21:57:39,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0775368670001626. input_tokens=163, output_tokens=77
21:57:41,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.35418725898489356. input_tokens=181, output_tokens=53
21:57:44,810 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:46,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.32323463703505695. input_tokens=155, output_tokens=17
21:57:49,396 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:51,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8337345119798556. input_tokens=183, output_tokens=285
21:57:54,246 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:57:56,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3915396749507636. input_tokens=172, output_tokens=42
21:57:58,868 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:01,238 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:03,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6504525160416961. input_tokens=334, output_tokens=214
21:58:06,422 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:08,1 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7714408370666206. input_tokens=141, output_tokens=47
21:58:10,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9403419009177014. input_tokens=204, output_tokens=314
21:58:13,211 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:16,8 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:18,83 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:20,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5713807409629226. input_tokens=162, output_tokens=193
21:58:22,948 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:24,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4779838969698176. input_tokens=275, output_tokens=143
21:58:28,15 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:29,954 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:32,516 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:34,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6892789609264582. input_tokens=200, output_tokens=316
21:58:37,236 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:39,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47586552100256085. input_tokens=186, output_tokens=113
21:58:41,585 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7957993349991739. input_tokens=180, output_tokens=219
21:58:43,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.36442561994772404. input_tokens=173, output_tokens=67
21:58:46,767 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:49,379 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:51,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43194388202391565. input_tokens=162, output_tokens=95
21:58:54,210 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:56,659 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:58:58,730 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:01,352 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:03,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7255605909740552. input_tokens=175, output_tokens=94
21:59:06,69 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:08,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4987030249321833. input_tokens=162, output_tokens=78
21:59:10,912 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:13,507 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:15,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5424988379236311. input_tokens=238, output_tokens=73
21:59:18,676 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:20,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.35283478908240795. input_tokens=152, output_tokens=65
21:59:22,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4580870389472693. input_tokens=341, output_tokens=114
21:59:25,468 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:27,974 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:30,909 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:32,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4083214880665764. input_tokens=150, output_tokens=56
21:59:34,905 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:37,617 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:39,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4593211329774931. input_tokens=148, output_tokens=87
21:59:42,377 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:44,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5164044139673933. input_tokens=160, output_tokens=49
21:59:46,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5141256260685623. input_tokens=326, output_tokens=138
21:59:49,198 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:51,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7766442280262709. input_tokens=147, output_tokens=24
21:59:54,171 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
21:59:56,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.30686552496626973. input_tokens=142, output_tokens=34
21:59:58,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4174178489483893. input_tokens=169, output_tokens=30
22:00:01,347 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:04,35 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:06,601 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:08,727 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:10,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.38090936793014407. input_tokens=157, output_tokens=33
22:00:13,214 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:15,995 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:17,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5850653880042955. input_tokens=213, output_tokens=237
22:00:20,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6070297930855304. input_tokens=146, output_tokens=81
22:00:23,138 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:24,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6476938540581614. input_tokens=337, output_tokens=61
22:00:27,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3120676359394565. input_tokens=149, output_tokens=24
22:00:30,146 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:32,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5248402539873496. input_tokens=396, output_tokens=110
22:00:35,64 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:36,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4342170510208234. input_tokens=171, output_tokens=48
22:00:40,105 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:43,309 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:44,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4646633410593495. input_tokens=189, output_tokens=127
22:00:46,525 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6912133110454306. input_tokens=143, output_tokens=163
22:00:48,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8421394990291446. input_tokens=207, output_tokens=280
22:00:52,103 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:54,163 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:00:56,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6244892009999603. input_tokens=186, output_tokens=73
22:00:59,577 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:00,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7276228769915178. input_tokens=144, output_tokens=53
22:01:03,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6766801319317892. input_tokens=172, output_tokens=158
22:01:06,130 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:08,619 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:10,957 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:13,769 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:15,383 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0446015899069607. input_tokens=147, output_tokens=213
22:01:17,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4393523330800235. input_tokens=155, output_tokens=26
22:01:20,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.737413452938199. input_tokens=149, output_tokens=118
22:01:22,929 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:25,753 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:27,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6848676989320666. input_tokens=201, output_tokens=289
22:01:30,284 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:32,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.40652566810604185. input_tokens=160, output_tokens=78
22:01:34,961 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:37,586 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:39,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5745276409434155. input_tokens=164, output_tokens=73
22:01:42,498 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:44,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5358811120968312. input_tokens=214, output_tokens=132
22:01:47,125 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:49,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8211187809938565. input_tokens=275, output_tokens=135
22:01:51,940 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:54,361 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:57,104 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:01:58,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.37198936892673373. input_tokens=182, output_tokens=63
22:02:01,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6969265789957717. input_tokens=377, output_tokens=233
22:02:03,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7467971099540591. input_tokens=150, output_tokens=29
22:02:06,607 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:08,636 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:11,137 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:13,756 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:15,883 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:17,919 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4657349670305848. input_tokens=149, output_tokens=48
22:02:20,753 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:22,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9776326830033213. input_tokens=279, output_tokens=338
22:02:25,494 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:27,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5669423340586945. input_tokens=145, output_tokens=46
22:02:30,554 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:32,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7961442079395056. input_tokens=171, output_tokens=108
22:02:35,776 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:37,743 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:40,89 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:42,638 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:44,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5945330209797248. input_tokens=186, output_tokens=105
22:02:47,179 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:49,943 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:51,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7728377440944314. input_tokens=144, output_tokens=61
22:02:54,849 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:02:56,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.41696990199852735. input_tokens=195, output_tokens=90
22:02:59,370 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:01,858 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:03,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0175785060273483. input_tokens=150, output_tokens=128
22:03:06,816 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:08,907 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:11,297 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:15,107 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:16,150 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:18,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3576509340200573. input_tokens=152, output_tokens=51
22:03:20,940 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:22,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47164274903479964. input_tokens=152, output_tokens=65
22:03:25,634 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:27,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3907259029801935. input_tokens=163, output_tokens=87
22:03:30,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.795163276954554. input_tokens=162, output_tokens=61
22:03:32,924 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:35,543 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:37,718 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:40,262 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:42,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3398126370739192. input_tokens=143, output_tokens=19
22:03:44,987 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:46,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7709582030074671. input_tokens=154, output_tokens=38
22:03:49,969 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:52,67 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:54,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49174268008209765. input_tokens=154, output_tokens=49
22:03:57,235 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:03:59,532 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:01,756 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:03,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3555212270002812. input_tokens=157, output_tokens=50
22:04:06,533 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:08,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5739672880154103. input_tokens=357, output_tokens=86
22:04:12,258 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:13,815 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:15,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6718668499961495. input_tokens=157, output_tokens=70
22:04:18,798 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:20,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48589546396397054. input_tokens=149, output_tokens=148
22:04:24,367 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:26,992 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:27,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49311942397616804. input_tokens=156, output_tokens=45
22:04:30,592 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:33,342 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:35,835 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:37,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5069380940403789. input_tokens=148, output_tokens=44
22:04:41,80 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:42,650 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:44,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8448012929875404. input_tokens=190, output_tokens=288
22:04:47,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2515436669345945. input_tokens=288, output_tokens=500
22:04:49,990 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:51,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7321094189537689. input_tokens=142, output_tokens=62
22:04:54,794 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:57,198 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:04:59,622 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:02,216 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:03,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.35244302498176694. input_tokens=145, output_tokens=43
22:05:06,766 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:08,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4489323180168867. input_tokens=148, output_tokens=30
22:05:11,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6636440539732575. input_tokens=222, output_tokens=139
22:05:13,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.37275751598645. input_tokens=152, output_tokens=64
22:05:15,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43377570691518486. input_tokens=180, output_tokens=88
22:05:18,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3653033640002832. input_tokens=147, output_tokens=39
22:05:20,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6194768620189279. input_tokens=161, output_tokens=55
22:05:23,551 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:26,166 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:28,656 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:30,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0307203199481592. input_tokens=153, output_tokens=82
22:05:33,282 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:35,803 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:38,267 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:40,578 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:42,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5900369979208335. input_tokens=158, output_tokens=52
22:05:45,609 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:47,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5298383049666882. input_tokens=148, output_tokens=123
22:05:49,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6687983060255647. input_tokens=147, output_tokens=50
22:05:52,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3964987680083141. input_tokens=147, output_tokens=49
22:05:54,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7543855460826308. input_tokens=372, output_tokens=205
22:05:57,341 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:05:59,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8548803509911522. input_tokens=141, output_tokens=48
22:06:02,338 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:04,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5658827209845185. input_tokens=166, output_tokens=67
22:06:07,59 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:09,287 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:12,993 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:12,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7348897010087967. input_tokens=156, output_tokens=50
22:06:15,396 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6488913279026747. input_tokens=216, output_tokens=124
22:06:17,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7950404740404338. input_tokens=151, output_tokens=38
22:06:20,16 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8668877120362595. input_tokens=387, output_tokens=334
22:06:22,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48117200192064047. input_tokens=233, output_tokens=76
22:06:24,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4634770459961146. input_tokens=170, output_tokens=48
22:06:27,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49991649203002453. input_tokens=156, output_tokens=39
22:06:30,323 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:32,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47369242599233985. input_tokens=164, output_tokens=136
22:06:34,842 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:36,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.361535700969398. input_tokens=149, output_tokens=23
22:06:39,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44196623004972935. input_tokens=158, output_tokens=55
22:06:42,361 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:44,464 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:46,902 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:48,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.654467329964973. input_tokens=146, output_tokens=18
22:06:51,620 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:54,872 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:06:56,71 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42092097003478557. input_tokens=408, output_tokens=72
22:06:58,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5595617049839348. input_tokens=157, output_tokens=49
22:07:00,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4769236479187384. input_tokens=157, output_tokens=97
22:07:03,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6470317210769281. input_tokens=166, output_tokens=204
22:07:05,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3407102390192449. input_tokens=153, output_tokens=17
22:07:08,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6992695099906996. input_tokens=153, output_tokens=48
22:07:10,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.58950028300751. input_tokens=209, output_tokens=89
22:07:12,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.40592456702142954. input_tokens=161, output_tokens=94
22:07:15,807 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:18,41 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:20,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.372695603989996. input_tokens=156, output_tokens=63
22:07:23,76 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:25,211 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:27,797 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:30,284 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:32,716 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:35,135 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:36,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2817605519667268. input_tokens=166, output_tokens=100
22:07:39,851 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:42,614 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:44,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42887031799182296. input_tokens=163, output_tokens=116
22:07:47,61 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:48,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6037653120001778. input_tokens=190, output_tokens=243
22:07:51,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3595761760370806. input_tokens=203, output_tokens=310
22:07:54,655 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:07:56,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.581310983048752. input_tokens=186, output_tokens=109
22:07:58,955 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:00,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3559866590658203. input_tokens=146, output_tokens=24
22:08:03,969 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:06,93 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:08,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6890919500729069. input_tokens=169, output_tokens=106
22:08:11,309 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:12,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7759831770090386. input_tokens=160, output_tokens=120
22:08:15,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2107041559647769. input_tokens=184, output_tokens=22
22:08:17,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.37453410995658487. input_tokens=147, output_tokens=41
22:08:20,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4989005899988115. input_tokens=171, output_tokens=89
22:08:23,103 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:25,464 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:27,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48945811903104186. input_tokens=215, output_tokens=146
22:08:30,289 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:32,805 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:35,12 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:37,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4847186249680817. input_tokens=157, output_tokens=38
22:08:40,545 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:41,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5026835980825126. input_tokens=144, output_tokens=47
22:08:45,57 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:46,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6927639850182459. input_tokens=155, output_tokens=36
22:08:49,582 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:51,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4327989249723032. input_tokens=157, output_tokens=69
22:08:54,494 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:08:56,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3942568660713732. input_tokens=161, output_tokens=49
22:08:58,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.599595844047144. input_tokens=193, output_tokens=88
22:09:01,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6933558580931276. input_tokens=194, output_tokens=72
22:09:03,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.502974871895276. input_tokens=166, output_tokens=109
22:09:06,385 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:09,20 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:10,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6168975930195302. input_tokens=190, output_tokens=80
22:09:13,698 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:15,888 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:18,621 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:21,235 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:23,136 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:25,769 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:27,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6696657380089164. input_tokens=165, output_tokens=169
22:09:29,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5747513090027496. input_tokens=172, output_tokens=207
22:09:32,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8009342539589852. input_tokens=150, output_tokens=95
22:09:34,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5128417720552534. input_tokens=164, output_tokens=154
22:09:38,91 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:39,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.702499866951257. input_tokens=155, output_tokens=116
22:09:42,537 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:09:44,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6152176839532331. input_tokens=147, output_tokens=50
22:09:46,755 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43598033604212105. input_tokens=139, output_tokens=98
22:09:49,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.700567155960016. input_tokens=161, output_tokens=104
22:09:51,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4124060940230265. input_tokens=163, output_tokens=86
22:09:53,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.72317362902686. input_tokens=138, output_tokens=101
22:09:56,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4210120280040428. input_tokens=158, output_tokens=60
22:09:58,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45422260800842196. input_tokens=187, output_tokens=125
22:10:01,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3645588989602402. input_tokens=160, output_tokens=14
22:10:03,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2087035960284993. input_tokens=172, output_tokens=59
22:10:05,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5269251359859481. input_tokens=153, output_tokens=80
22:10:08,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.35359206795692444. input_tokens=152, output_tokens=65
22:10:10,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5817153819371015. input_tokens=159, output_tokens=140
22:10:13,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3094755079364404. input_tokens=150, output_tokens=16
22:10:15,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4879257819848135. input_tokens=159, output_tokens=13
22:10:17,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5700931240571663. input_tokens=155, output_tokens=59
22:10:20,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5946643439820036. input_tokens=154, output_tokens=37
22:10:22,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6100418459391221. input_tokens=195, output_tokens=65
22:10:25,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5165321870008484. input_tokens=184, output_tokens=131
22:10:27,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8727927490836009. input_tokens=169, output_tokens=78
22:10:29,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5152511410415173. input_tokens=197, output_tokens=75
22:10:32,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.898438923060894. input_tokens=161, output_tokens=18
22:10:34,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.38951851602178067. input_tokens=159, output_tokens=43
22:10:37,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5946164500201121. input_tokens=182, output_tokens=132
22:10:39,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3130819449434057. input_tokens=140, output_tokens=34
22:10:42,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7195572099881247. input_tokens=166, output_tokens=139
22:10:44,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4992590780602768. input_tokens=151, output_tokens=65
22:10:46,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4553235659841448. input_tokens=156, output_tokens=40
22:10:49,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.472478253999725. input_tokens=155, output_tokens=72
22:10:51,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5827392159262672. input_tokens=151, output_tokens=152
22:10:54,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.38416772393975407. input_tokens=154, output_tokens=79
22:10:56,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1103375139646232. input_tokens=161, output_tokens=89
22:10:58,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8148499500239268. input_tokens=154, output_tokens=194
22:11:01,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5340560500044376. input_tokens=172, output_tokens=106
22:11:03,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6314468201017007. input_tokens=167, output_tokens=60
22:11:06,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5110203140648082. input_tokens=151, output_tokens=28
22:11:08,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7383337070932612. input_tokens=163, output_tokens=158
22:11:10,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6091860870365053. input_tokens=184, output_tokens=101
22:11:13,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3907304280437529. input_tokens=182, output_tokens=67
22:11:15,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7178543970221654. input_tokens=158, output_tokens=85
22:11:18,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9247789429500699. input_tokens=223, output_tokens=123
22:11:20,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.41954758297652006. input_tokens=156, output_tokens=50
22:11:22,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6541560270125046. input_tokens=183, output_tokens=169
22:11:25,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9542706100037321. input_tokens=149, output_tokens=42
22:11:27,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5919430810026824. input_tokens=260, output_tokens=195
22:11:27,794 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:11:28,874 graphrag.index.run INFO Running workflow: create_base_entity_graph...
22:11:28,875 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:11:28,876 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
22:11:28,925 datashaper.workflow.workflow INFO executing verb cluster_graph
22:11:30,323 datashaper.workflow.workflow INFO executing verb select
22:11:30,329 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:11:30,568 graphrag.index.run INFO Running workflow: create_final_entities...
22:11:30,569 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:11:30,569 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:11:30,591 datashaper.workflow.workflow INFO executing verb unpack_graph
22:11:31,16 datashaper.workflow.workflow INFO executing verb rename
22:11:31,17 datashaper.workflow.workflow INFO executing verb select
22:11:31,18 datashaper.workflow.workflow INFO executing verb dedupe
22:11:31,20 datashaper.workflow.workflow INFO executing verb rename
22:11:31,21 datashaper.workflow.workflow INFO executing verb filter
22:11:31,40 datashaper.workflow.workflow INFO executing verb text_split
22:11:31,58 datashaper.workflow.workflow INFO executing verb drop
22:11:31,59 datashaper.workflow.workflow INFO executing verb merge
22:11:31,372 datashaper.workflow.workflow INFO executing verb text_embed
22:11:31,382 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
22:11:31,437 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
22:11:31,437 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
22:11:31,597 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 972 inputs via 972 snippets using 61 batches. max_batch_size=16, max_tokens=8191
22:11:32,137 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,151 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,160 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,168 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,171 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,176 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,181 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,203 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,232 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,516 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,525 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0177272340515628. input_tokens=383, output_tokens=0
22:11:32,659 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,666 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0357054369524121. input_tokens=1439, output_tokens=0
22:11:32,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0668973759748042. input_tokens=782, output_tokens=0
22:11:32,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0994242139859125. input_tokens=825, output_tokens=0
22:11:32,760 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,762 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,766 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,769 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,775 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:32,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1650096650701016. input_tokens=877, output_tokens=0
22:11:32,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1872356589883566. input_tokens=318, output_tokens=0
22:11:32,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2325560320168734. input_tokens=895, output_tokens=0
22:11:32,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.235374627984129. input_tokens=1095, output_tokens=0
22:11:32,919 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2698675009887666. input_tokens=747, output_tokens=0
22:11:32,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2761627400759608. input_tokens=728, output_tokens=0
22:11:33,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.415977881057188. input_tokens=793, output_tokens=0
22:11:33,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4432870019227266. input_tokens=548, output_tokens=0
22:11:33,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:33,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4524161380250007. input_tokens=1221, output_tokens=0
22:11:33,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5461723810294643. input_tokens=700, output_tokens=0
22:11:33,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5227934620343149. input_tokens=571, output_tokens=0
22:11:33,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.613487806986086. input_tokens=319, output_tokens=0
22:11:33,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6504881079308689. input_tokens=934, output_tokens=0
22:11:33,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6685658959904686. input_tokens=338, output_tokens=0
22:11:33,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7333652990637347. input_tokens=293, output_tokens=0
22:11:33,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7338362379232422. input_tokens=396, output_tokens=0
22:11:33,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.800322748022154. input_tokens=953, output_tokens=0
22:11:33,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8372936410596594. input_tokens=435, output_tokens=0
22:11:33,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.958470062003471. input_tokens=1135, output_tokens=0
22:11:33,651 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:33,652 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:33,653 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:33,655 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:33,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.041402026079595. input_tokens=478, output_tokens=0
22:11:33,769 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:33,779 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:33,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.256816701963544. input_tokens=450, output_tokens=0
22:11:33,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1727276289602742. input_tokens=420, output_tokens=0
22:11:34,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.240569071029313. input_tokens=316, output_tokens=0
22:11:34,236 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,238 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,239 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,242 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0462074389215559. input_tokens=286, output_tokens=0
22:11:34,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.6381291639991105. input_tokens=661, output_tokens=0
22:11:34,306 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,307 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,308 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,319 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,326 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5545872929506004. input_tokens=892, output_tokens=0
22:11:34,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,364 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,365 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,383 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1994847990572453. input_tokens=659, output_tokens=0
22:11:34,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.991257001995109. input_tokens=280, output_tokens=0
22:11:34,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4902831629151478. input_tokens=527, output_tokens=0
22:11:34,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7327614940004423. input_tokens=397, output_tokens=0
22:11:34,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4497219419572502. input_tokens=331, output_tokens=0
22:11:34,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0154774009715766. input_tokens=568, output_tokens=0
22:11:34,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9819915710249916. input_tokens=1260, output_tokens=0
22:11:34,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4326998289907351. input_tokens=555, output_tokens=0
22:11:34,670 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:34,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7081097529735416. input_tokens=268, output_tokens=0
22:11:34,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2382431720616296. input_tokens=328, output_tokens=0
22:11:34,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.31185679405462. input_tokens=315, output_tokens=0
22:11:34,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5193127369275317. input_tokens=492, output_tokens=0
22:11:35,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7927946849958971. input_tokens=723, output_tokens=0
22:11:35,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0505010479828343. input_tokens=653, output_tokens=0
22:11:35,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,67 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0902961909305304. input_tokens=1097, output_tokens=0
22:11:35,97 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,99 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,100 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,102 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,102 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1338925829622895. input_tokens=445, output_tokens=0
22:11:35,139 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.401989272912033. input_tokens=829, output_tokens=0
22:11:35,253 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,255 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8072034840006381. input_tokens=452, output_tokens=0
22:11:35,293 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0573183000087738. input_tokens=329, output_tokens=0
22:11:35,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0009189049014822. input_tokens=490, output_tokens=0
22:11:35,357 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.39511262497399. input_tokens=354, output_tokens=0
22:11:35,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9528168329270557. input_tokens=876, output_tokens=0
22:11:35,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4951690109446645. input_tokens=222, output_tokens=0
22:11:35,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0947623160900548. input_tokens=835, output_tokens=0
22:11:35,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.033629050012678. input_tokens=256, output_tokens=0
22:11:35,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0680018350249156. input_tokens=1098, output_tokens=0
22:11:35,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0518208430148661. input_tokens=375, output_tokens=0
22:11:35,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1834413580363616. input_tokens=285, output_tokens=0
22:11:35,831 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
22:11:35,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5139250459615141. input_tokens=316, output_tokens=0
22:11:36,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8974845058983192. input_tokens=285, output_tokens=0
22:11:36,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9285903570707887. input_tokens=401, output_tokens=0
22:11:36,567 datashaper.workflow.workflow INFO executing verb drop
22:11:36,568 datashaper.workflow.workflow INFO executing verb filter
22:11:36,592 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:11:37,37 graphrag.index.run INFO Running workflow: create_final_nodes...
22:11:37,38 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:11:37,39 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:11:37,71 datashaper.workflow.workflow INFO executing verb layout_graph
22:11:39,199 datashaper.workflow.workflow INFO executing verb unpack_graph
22:11:40,2 datashaper.workflow.workflow INFO executing verb unpack_graph
22:11:40,560 datashaper.workflow.workflow INFO executing verb drop
22:11:40,563 datashaper.workflow.workflow INFO executing verb filter
22:11:40,656 datashaper.workflow.workflow INFO executing verb select
22:11:40,657 datashaper.workflow.workflow INFO executing verb rename
22:11:40,658 datashaper.workflow.workflow INFO executing verb join
22:11:40,683 datashaper.workflow.workflow INFO executing verb convert
22:11:40,693 datashaper.workflow.workflow INFO executing verb rename
22:11:40,699 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:11:40,958 graphrag.index.run INFO Running workflow: create_final_communities...
22:11:40,959 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:11:40,960 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:11:40,982 datashaper.workflow.workflow INFO executing verb unpack_graph
22:11:41,487 datashaper.workflow.workflow INFO executing verb unpack_graph
22:11:42,20 datashaper.workflow.workflow INFO executing verb aggregate_override
22:11:42,25 datashaper.workflow.workflow INFO executing verb join
22:11:42,92 datashaper.workflow.workflow INFO executing verb join
22:11:42,155 datashaper.workflow.workflow INFO executing verb concat
22:11:42,160 datashaper.workflow.workflow INFO executing verb filter
22:11:43,438 datashaper.workflow.workflow INFO executing verb aggregate_override
22:11:43,468 datashaper.workflow.workflow INFO executing verb join
22:11:43,476 datashaper.workflow.workflow INFO executing verb filter
22:11:43,487 datashaper.workflow.workflow INFO executing verb fill
22:11:43,488 datashaper.workflow.workflow INFO executing verb merge
22:11:43,542 datashaper.workflow.workflow INFO executing verb copy
22:11:43,543 datashaper.workflow.workflow INFO executing verb select
22:11:43,546 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:11:43,803 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
22:11:43,804 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
22:11:43,805 graphrag.index.run INFO read table from storage: create_final_entities.parquet
22:11:43,889 datashaper.workflow.workflow INFO executing verb select
22:11:43,890 datashaper.workflow.workflow INFO executing verb unroll
22:11:43,893 datashaper.workflow.workflow INFO executing verb aggregate_override
22:11:43,907 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
22:11:44,137 graphrag.index.run INFO Running workflow: create_final_relationships...
22:11:44,137 graphrag.index.run INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:11:44,138 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
22:11:44,164 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
22:11:44,173 datashaper.workflow.workflow INFO executing verb unpack_graph
22:11:44,693 datashaper.workflow.workflow INFO executing verb filter
22:11:44,769 datashaper.workflow.workflow INFO executing verb rename
22:11:44,771 datashaper.workflow.workflow INFO executing verb filter
22:11:44,873 datashaper.workflow.workflow INFO executing verb drop
22:11:44,875 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
22:11:44,884 datashaper.workflow.workflow INFO executing verb convert
22:11:44,885 datashaper.workflow.workflow INFO executing verb convert
22:11:44,889 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:11:45,150 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
22:11:45,151 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
22:11:45,152 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
22:11:45,160 datashaper.workflow.workflow INFO executing verb select
22:11:45,162 datashaper.workflow.workflow INFO executing verb unroll
22:11:45,174 datashaper.workflow.workflow INFO executing verb aggregate_override
22:11:45,189 datashaper.workflow.workflow INFO executing verb select
22:11:45,198 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
22:11:45,417 graphrag.index.run INFO Running workflow: create_final_community_reports...
22:11:45,418 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:11:45,418 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
22:11:45,439 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
22:11:45,455 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:11:45,520 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:11:45,543 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:11:45,567 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:11:45,567 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 972
22:11:45,625 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 972
22:11:45,873 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 972
22:11:46,336 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 972
22:11:46,585 datashaper.workflow.workflow INFO executing verb create_community_reports
22:11:46,612 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:11:46,621 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:11:46,621 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 112
22:11:51,12 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:11:51,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.389290645020083. input_tokens=2172, output_tokens=646
22:12:22,172 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:12:22,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7461521059740335. input_tokens=2534, output_tokens=680
22:13:06,154 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:13:06,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2485154289752245. input_tokens=3130, output_tokens=737
22:13:09,143 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:13:09,145 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:13:09,145 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 86
22:13:09,156 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:13:09,161 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:13:09,161 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 85
22:13:37,685 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:13:37,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5568570219911635. input_tokens=2028, output_tokens=464
22:14:06,512 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:14:06,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6087643429636955. input_tokens=2054, output_tokens=488
22:14:37,296 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:14:45,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6060997979948297. input_tokens=2165, output_tokens=594
22:15:17,667 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:15:17,675 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:15:17,677 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:15:17,677 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 80
22:15:17,678 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:15:17,680 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:15:17,680 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 96
22:15:25,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.591377447010018. input_tokens=2380, output_tokens=601
22:15:58,528 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:16:06,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.830359332030639. input_tokens=2513, output_tokens=603
22:16:36,199 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:16:42,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6732816860312596. input_tokens=2200, output_tokens=481
22:17:17,383 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:17:25,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.9315861680079252. input_tokens=2309, output_tokens=573
22:17:50,401 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:17:56,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5581745230592787. input_tokens=2097, output_tokens=440
22:18:29,552 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:18:38,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7307829979108647. input_tokens=2390, output_tokens=675
22:19:14,559 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:19:26,990 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.13031571789179. input_tokens=2724, output_tokens=932
22:19:57,203 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:20:04,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.073743538931012. input_tokens=2175, output_tokens=525
22:20:35,279 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:20:43,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7015774940373376. input_tokens=2564, output_tokens=642
22:21:15,842 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:21:22,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5073202119674534. input_tokens=2468, output_tokens=517
22:22:03,974 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:22:03,977 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6928003099747. input_tokens=2081, output_tokens=531
22:22:41,500 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:22:41,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0767801940673962. input_tokens=2516, output_tokens=673
22:23:23,265 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
22:23:23,268 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n370,MONT BLANC,"Mont Blanc is a mountain located in the Alps, renowned for its supreme and magnificent presence in the region. It is a notable landmark that can be seen from various vantage points, including a lake where Victor and Elizabeth have a view of it.",7\n273,AUTHOR,"The entity ""AUTHOR"" can be described as follows:\n\nThe author is the narrator of the story, taking on a crucial role in shaping the narrative and conveying the events to the reader. However, in this particular case, the author of the text remains unknown, adding an air of mystery to their identity. Despite this, it is known that they are a person who has written a novel, specifically one that revolves around a scientist who creates a monster, showcasing their creative and imaginative capabilities.",12\n374,CHAMOUNIX,"Chamounix is a valley located in a region, characterized by its natural surroundings and rich collections of natural history.",11\n375,SERVOX,"Based on the provided data, here is a comprehensive summary of SERVOX:\n\nSERVOX is a valley located in a region, characterized by its natural history collections.",2\n378,PINE WOODS,Pine woods are a type of forest in the region,2\n379,EAGLE,Eagle is a type of bird in the region,2\n386,EVENT,The story is an event that took place in the region,1\n383,VALLEY,Valley is a type of natural formation in the region,1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,rank\n808,VICTOR FRANKENSTEIN,MONT BLANC,Victor Frankenstein and Elizabeth have a profound appreciation for the breathtaking beauty of Mont Blanc.,78\n408,ELIZABETH,MONT BLANC,Victor Frankenstein and Elizabeth see the beauty of the scene from Mont Blanc,58\n665,AUTHOR,SCIENTIST,The author of the text is the same person as the scientist in the novel,52\n670,AUTHOR,CHAMOUNIX,Author visited the valley of Chamounix,23\n666,AUTHOR,MONT BLANC,Author saw Mont Blanc from the valley of Chamounix,19\n956,MONT BLANC,CHAMOUNIX,Mont Blanc overlooks the valley of Chamounix,18\n673,AUTHOR,ALPS,Author saw Alps in the region,17\n967,CHAMOUNIX,ALPS,Alps are a mountain range in the region near Chamounix,16\n667,AUTHOR,MONTANVERT,Author visited Montanvert,15\n668,AUTHOR,ARVE,Author saw Arve in the valley of Chamounix,15\n669,AUTHOR,ARVEIRON,Author saw Arveiron in the valley of Chamounix,15\n672,AUTHOR,PELISSIER,Author saw Pélissier in the region,15\n437,LAKE,MONT BLANC,Mont Blanc and the lake are both in the Alps,15\n671,AUTHOR,SERVOX,Author visited Servox,14\n674,AUTHOR,PINE WOODS,Author saw pine woods in the region,14\n675,AUTHOR,EAGLE,Author saw eagle in the region,14\n966,CHAMOUNIX,PELISSIER,Pélissier is a bridge in the region near Chamounix,14\n959,MONTANVERT,CHAMOUNIX,Montanvert is a village near the valley of Chamounix,14\n961,ARVE,CHAMOUNIX,Arve flows through the valley of Chamounix,14\n963,ARVEIRON,CHAMOUNIX,Arveiron takes its rise in a glacier near Chamounix,14\n676,AUTHOR,EVENT,The story is an event that took place,13\n965,CHAMOUNIX,SERVOX,"Based on the provided data, here is a comprehensive summary of the entities ""CHAMOUNIX"" and ""SERVOX"":\n\nChamounix and Servox are two geographical locations in the same region. Servox is specifically described as a valley located near Chamounix. Both locations have similar natural history collections, indicating that they share a common interest in preserving and showcasing the natural world.",13\n968,CHAMOUNIX,PINE WOODS,Pine woods are a type of forest in the region near Chamounix,13\n969,CHAMOUNIX,EAGLE,Eagle is a type of bird in the region near Chamounix,13\n958,MONT BLANC,EVIAN,Mont Blanc and Evian are both in the Alps,13\n970,CHAMOUNIX,VALLEY,Chamounix is a valley,12\n957,MONT BLANC,DRANCE,Mont Blanc and Drance are both in the Alps,12\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
22:23:23,268 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\n  "title": "Mont Blanc and the Alps Community",\n  "summary": "The community revolves around Mont Blanc, a mountain located in the Alps, and its relationships with other entities in the region. The community includes various geographical locations, such as Chamounix and Servox, as well as natural formations like pine woods and eagles.",\n  "rating": 6.0,\n  "rating_explanation": "The impact severity rating is moderate due to the potential for natural disasters or environmental concerns associated with the mountain and its surroundings.",\n  "findings": [\n    {\n      "summary": "Mont Blanc as the central location",\n      "explanation": "Mont Blanc is the central entity in this community, serving as a notable landmark and a source of natural beauty. The mountain is associated with various entities, including Victor Frankenstein and Elizabeth, who have a profound appreciation for its beauty [Data: Relationships (808, 408)]. The relationship between Mont Blanc and the other entities in the community is crucial in understanding the dynamics of this community. [Data: Relationships (808, 408)]"\n    },\n    {\n      "summary": "Chamounix and Servox as geographical locations",\n      "explanation": "Chamounix and Servox are two geographical locations in the same region, characterized by their natural history collections and natural surroundings [Data: Entities (374, 375)]. Both locations have similar interests in preserving and showcasing the natural world, indicating a potential for collaboration or shared goals. [Data: Relationships (965)]"\n    },\n    {\n      "summary": "Author\'s role in the community",\n      "explanation": "The author of the text is a key entity in this community, being the narrator of the story and the same person as the scientist in the novel [Data: Relationships (665)]. The author\'s visit to various locations in the region, including Mont Blanc and Chamounix, suggests a deep understanding of the community and its dynamics. [Data: Relationships (670, 666)]"\n    },\n    {\n      "summary": "Natural formations in the community",\n      "explanation": "The community includes various natural formations, such as pine woods and eagles, which are associated with the region and its geographical locations [Data: Entities (378, 379)]. These natural formations could be a source of inspiration or concern for the entities in the community, depending on their perspectives and goals. [Data: Relationships (674, 675)]"\n    },\n    {\n      "summary": "Event as a significant occurrence",\n      "explanation": "The story is an event that took place in the region, which is associated with various entities, including the author and the scientist [Data: Relationships (676)]. This event could be a significant occurrence in the community, potentially shaping the relationships and goals of the entities involved. [Data: Relationships (676)]"\n    },\n    {\n      "summary": "Potential for natural disasters",\n      "explanation": "The community\'s association with a mountain and its surroundings raises concerns about potential natural disasters, such as landslides or avalanches [Data: Entities (370)]. The impact of such disasters could be severe, affecting the entities and their relationships in the community. [Data: Relationships (808, 408)]"\n    },\n    {\n      "summary": "Environmental concerns",\n      "explanation": "The community\'s focus on natural history collections and preservation raises concerns about environmental sustainability and conservation [Data: Entities (374, 375)]. The entities in the community may have different perspectives on environmental issues, potentially leading to conflicts or collaborations. [Data: Relationships (965)]"\n    },\n    {\n      "summary": "Author\'s creative capabilities",\n      "explanation": "The author\'s creative capabilities, as demonstrated by their novel about a scientist who creates a monster, suggest a deep understanding of the community and its dynamics [Data: Relationships (665)]. The author\'s ability to convey complex ideas and emotions could be a valuable asset to the community. [Data: Relationships (665)]"\n    },\n    {\n      "summary": "Scientist\'s role in the community",\n      "explanation": "The scientist, as described in the author\'s novel, is a key entity in this community, being the creator of a monster and a source of inspiration for the author [Data: Relationships (665)]. The scientist\'s goals and motivations could be a significant factor in the community\'s dynamics, potentially shaping the relationships and goals of the entities involved. [Data: Relationships (665)]"\n    }'}}
22:23:23,279 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:23:23,279 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 100
22:24:08,558 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
22:24:08,560 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n8,MARGARET,"Based on the provided data, here is a comprehensive summary of Margaret:\n\nMargaret is a significant character in the narrative, closely related to the narrator. She is often referred to as the narrator\'s sister, which suggests a familial bond between them. However, in some instances, she is also described as the narrator\'s friend, indicating a close but non-kinship relationship. Margaret is also the recipient of the narrator\'s story, as she is the one to whom Victor Frankenstein\'s tale is being read by Walton. Additionally, Margaret is anxiously awaiting the return of Victor Frankenstein, which implies that she is concerned about his well-being.",20\n66,OCEAN,"The ocean is a vast body of water that serves as a barrier between Victor Frankenstein and his fellow creatures, as depicted in various narratives. It is also the setting for exploration, as the narrator is about to embark on a journey to discover its depths and secrets. Furthermore, the ocean is the body of water that the ship is on, indicating its significance as a mode of transportation and a pathway for human exploration.",5\n68,SHIP,"The entity ""SHIP"" can be described as follows: \n\nThe ship is a vessel that played a significant role in the narrator\'s journey. It is the vessel that rescued the narrator, indicating its importance in their survival. Furthermore, the narrator is planning to sail on this ship, suggesting that it will be their mode of transportation for a period of time. Additionally, the ship is the vessel that the narrator and the stranger are currently on, implying that it serves as their temporary home or means of travel. Overall, the ship is a crucial element in the narrator\'s story, providing both rescue and transportation.",4\n102,DECK,The deck is the area on the ship where the narrator and the stranger often go,3\n105,ELEMENTAL FOES,The elemental foes are a metaphorical concept that the narrator refers to,3\n106,RACE,The race is a metaphorical concept that the narrator refers to,3\n111,SKY,The sky is the atmosphere around the Earth,3\n103,SLICHE,The sledge is a mysterious object that the narrator and the stranger are watching for,3\n110,STAR,The star is a celestial object that the narrator and the stranger refer to,3\n101,STRANGER,The stranger is a man who has been on the ship and is recovering from illness,15\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,rank\n95,MARGARET,VICTOR FRANKENSTEIN,Victor Frankenstein\'s sister Margaret is anxiously awaiting his return,91\n57,FRANKENSTEIN,MARGARET,Margaret is listening to Frankenstein\'s storyFrankenstein is telling his story to Margaret,82\n165,OCEAN,MONSTER,The monster is on the ocean,76\n164,OCEAN,NARRATOR,The narrator is on the ocean with the stranger,55\n169,SHIP,NARRATOR,The narrator is on the ship with the stranger,54\n240,DECK,NARRATOR,The narrator often goes to the deck of the ship,53\n244,ELEMENTAL FOES,NARRATOR,The narrator refers to the elemental foes as a metaphorical concept,53\n245,RACE,NARRATOR,The narrator refers to the race as a metaphorical concept,53\n257,SKY,NARRATOR,The narrator refers to the sky as the atmosphere around the Earth,53\n241,SLICHE,NARRATOR,The narrator is watching for the sledge,53\n256,STAR,NARRATOR,The narrator refers to the star as a celestial object,53\n79,MARGARET,STRANGER,The stranger is a friend of the narrator\'s sister or friend,35\n94,MARGARET,WALTON,Walton was reading the narrator\'s story to Margaret,33\n88,MARGARET,FRIEND,Margaret is a friend of the narrator,31\n15,MARY WOLLSTONECRAFT SHELLEY,MARGARET,Margaret is the narrator\'s sister and the recipient of the letter,30\n78,MARGARET,R. WALTON,Margaret is R. Walton\'s sister and only friendR. Walton\'s sister Margaret is his only friend,28\n93,MARGARET,SEA,Margaret is on the sea with the narrator and the stranger,27\n90,MARGARET,NATURE,"Margaret is not referred to as being affected by nature, but the narrator and the stranger are",26\n175,FRIEND,STRANGER,The stranger is a friend of the narrator,26\n83,MARGARET,OCEAN,Margaret is on the ocean with the narrator and the stranger,25\n84,MARGARET,HEAVEN,"Margaret is not referred to as being in heaven, but the narrator and the stranger are",25\n87,MARGARET,WORLD,Margaret is on the world with the narrator and the stranger,25\n89,MARGARET,BROTHER,"Margaret is not a brother to the narrator, but the stranger is",25\n80,MARGARET,SHIP,Margaret is the narrator\'s sister or friend who is on the ship,24\n81,MARGARET,DECK,"Margaret is not on the deck, but the narrator and the stranger are",23\n85,MARGARET,ELEMENTAL FOES,"Margaret is not referred to as being affected by the elemental foes, but the narrator and the stranger are",23\n82,MARGARET,SLICHE,"Margaret is not watching for the sledge, but the narrator and the stranger are",23\n86,MARGARET,RACE,"Margaret is not referred to as being part of the race, but the narrator and the stranger are",23\n91,MARGARET,STAR,"Margaret is not referred to as being affected by the star, but the narrator and the stranger are",23\n92,MARGARET,SKY,"Margaret is not referred to as being affected by the sky, but the narrator and the stranger are",23\n239,STRANGER,SEA,The stranger is on the sea with the narrator,22\n236,STRANGER,NATURE,The stranger refers to nature as the world around them,21\n163,OCEAN,STRANGER,The stranger is on the ocean with the narrator,20\n231,STRANGER,HEAVEN,The stranger refers to heaven as a metaphorical place,20\n234,STRANGER,WORLD,The stranger is on the world with the narrator,20\n235,STRANGER,BROTHER,The stranger is a brother to the narrator,20\n168,SHIP,STRANGER,The stranger is on the ship and is recovering from illness,19\n229,STRANGER,DECK,The stranger often goes to the deck of the ship,18\n232,STRANGER,ELEMENTAL FOES,The stranger refers to the elemental foes as a metaphorical concept,18\n233,STRANGER,RACE,The stranger refers to the race as a metaphorical concept,18\n238,STRANGER,SKY,The stranger refers to the sky as the atmosphere around the Earth,18\n230,STRANGER,SLICHE,The stranger is watching for the sledge,18\n237,STRANGER,STAR,The stranger refers to the star as a celestial object,18\n167,SHIP,LIEUTENANT,The narrator\'s lieutenant will sail on the ship,9\n128,ARCHANGEL,OCEAN,The narrator is going to explore the ocean from Archangel,8\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
22:24:08,560 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\n    "title": "Margaret and the Ocean",\n    "summary": "The community revolves around Margaret, who is closely related to the narrator and is anxiously awaiting the return of Victor Frankenstein. The ocean is another key entity in this community, serving as a barrier between Victor Frankenstein and his fellow creatures. The ship is also a crucial element in the community, providing both rescue and transportation for the narrator and the stranger.",\n    "rating": 6.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for conflict or unrest related to Victor Frankenstein\'s return and the narrator\'s journey.",\n    "findings": [\n        {\n            "summary": "Margaret\'s role in the community",\n            "explanation": "Margaret is a significant character in the narrative, closely related to the narrator. She is often referred to as the narrator\'s sister, which suggests a familial bond between them. However, in some instances, she is also described as the narrator\'s friend, indicating a close but non-kinship relationship. Margaret is also the recipient of the narrator\'s story, as she is the one to whom Victor Frankenstein\'s tale is being read by Walton. Additionally, Margaret is anxiously awaiting the return of Victor Frankenstein, which implies that she is concerned about his well-being. [Data: Entities (8), Relationships (95, 57, 94, 88, 15)]"\n        },\n        {\n            "summary": "The ocean as a barrier",\n            "explanation": "The ocean is a vast body of water that serves as a barrier between Victor Frankenstein and his fellow creatures, as depicted in various narratives. It is also the setting for exploration, as the narrator is about to embark on a journey to discover its depths and secrets. Furthermore, the ocean is the body of water that the ship is on, indicating its significance as a mode of transportation and a pathway for human exploration. [Data: Entities (66), Relationships (165, 163, 128)]"\n        },\n        {\n            "summary": "The ship as a crucial element",\n            "explanation": "The ship is a vessel that played a significant role in the narrator\'s journey. It is the vessel that rescued the narrator, indicating its importance in their survival. Furthermore, the narrator is planning to sail on this ship, suggesting that it will be their mode of transportation for a period of time. Additionally, the ship is the vessel that the narrator and the stranger are currently on, implying that it serves as their temporary home or means of travel. Overall, the ship is a crucial element in the narrator\'s story, providing both rescue and transportation. [Data: Entities (68), Relationships (169, 167)]"\n        },\n        {\n            "summary": "The narrator\'s journey",\n            "explanation": "The narrator is about to embark on a journey to explore the ocean and discover its depths and secrets. This journey is significant, as it will take the narrator and the stranger to new and uncharted territories. The narrator\'s journey is also motivated by a desire to learn and discover new things, which suggests a sense of curiosity and wonder. [Data: Entities (101), Relationships (164, 164)]"\n        },\n        {\n            "summary": "The stranger\'s role",\n            "explanation": "The stranger is a man who has been on the ship and is recovering from illness. He is a friend of the narrator and is often referred to as a brother. The stranger is also a key character in the narrative, as he is involved in the narrator\'s journey and is a source of support and guidance. [Data: Entities (101), Relationships (79, 175)]"\n        },\n        {\n            "summary": "The narrator\'s relationships",\n            "explanation": "The narrator has several relationships with other entities in the community, including Margaret, the stranger, and the ocean. These relationships are significant, as they shape the narrator\'s experiences and interactions with the world around them. The narrator\'s relationships are also complex and multifaceted, reflecting the nuances and complexities of human relationships. [Data: Entities (101), Relationships (239, 236, 163)]"\n        },\n        {\n            "summary": "The community\'s dynamics",\n            "explanation": "The community is dynamic and constantly evolving, with new relationships and interactions emerging all the time. The community is also characterized by a sense of uncertainty and unpredictability, as the narrator and the stranger navigate the challenges and obstacles of their journey. [Data: Entities (101), Relationships (164, 164)]"\n        }'}}
22:24:08,563 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:24:08,564 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 108
22:24:48,438 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:24:58,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5606475500389934. input_tokens=2750, output_tokens=732
22:25:31,675 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:25:40,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.9384514489211142. input_tokens=2777, output_tokens=660
22:26:19,859 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:26:31,890 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.138802960049361. input_tokens=2850, output_tokens=902
22:27:26,720 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
22:27:26,726 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n387,THE MONSTER,"The Monster, a creature created by Victor Frankenstein, is a being of immense complexity and depth. According to the narrative, The Monster is a creature that was brought to life by Frankenstein\'s scientific endeavors, and it is described as being deformed and wicked. This creature, created by the author, has a profound impact on Frankenstein, as it speaks to him and causes him great pain and suffering.\n\nThe Monster\'s existence is marked by a desire for freedom and autonomy, as it seeks to be released into the wilds of South America. This desire for liberation is a testament to the creature\'s capacity for self-awareness and its longing for a life beyond the confines of its creator\'s laboratory.\n\nIt is worth noting that the narrative presents a nuanced and multifaceted portrayal of The Monster, highlighting both its monstrous and sympathetic aspects. Through its interactions with Frankenstein, The Monster reveals a capacity for intelligence, emotion, and even a sense of morality, which challenges the notion of its inherent wickedness.\n\nUltimately, The Monster remains a powerful and thought-provoking symbol in the world of literature, inviting readers to consider the complexities of creation, responsibility, and the human condition.",36\n388,FRANKENSTEIN\'S CREATURE,"Frankenstein\'s creature is a being created by Frankenstein. It is the monster created by Frankenstein, and is notable for being able to speak to its creator.",3\n533,SOUTH AMERICA,"South America is a continent located in the Western Hemisphere, where the fiend would go if the creator were to consent to his request. It is also the location where the monster will be released, suggesting a potential threat or danger associated with this region.",2\n389,HELL,The monster mentions hell as a place of torture,1\n391,THE CAVES OF ICE,The monster mentions the caves of ice as a place of refuge,1\n797,THE COMMISSION,The commission is a crime committed by the monster,1\n834,THE CREATURE\'S HOME,The creature\'s home is the location where the monster lives,1\n836,THE CREATURE\'S HOME LAND,The creature\'s home land is the location where the monster lives,1\n835,THE CREATURE\'S WORLD,The creature\'s world is the location where the monster lives,1\n390,THE DESERT MOUNTAINS,The monster mentions the desert mountains as a place of refuge,1\n393,THE GLACIERS,"The Glaciers are a location where a monster can hide from pursuit, serving as a place of refuge for it.",1\n392,THE MOUNTAINS,"The Mountains are a location that holds significant importance in various contexts. They are described as a place of refuge, suggesting a sense of safety and security. However, they are also associated with a darker aspect, as they are mentioned as a location where a murderer had wandered. This juxtaposition implies that the mountains may serve as a sanctuary for some, but also as a site of turmoil and violence for others.",1\n394,THE SKIES,The monster mentions the skies as a place of refuge,1\n548,THE SEA OF ICE,"The Sea of Ice is a location of significant importance, particularly in relation to a powerful monster. It is here that the creature possesses the ability to traverse, suggesting a unique environment or properties that facilitate its movement. Furthermore, this location is also where the monster is destined to be released, implying a connection to its origins, containment, or eventual freedom.",1\n549,THE RIDGES OF INACCESSIBLE PRECIPICES,The ridges of inaccessible precipices are the location where the monster can hide from pursuit,1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,rank\n612,THE AUTHOR,THE MONSTER,The author created the monster and is its creator,115\n46,FRANKENSTEIN,THE MONSTER,"The entity ""FRANKENSTEIN"" is a scientist who created the entity ""THE MONSTER"". According to the descriptions, there is a significant interaction between the two, as the monster is speaking to Frankenstein, asking for compassion. This implies that the monster is capable of complex thought and communication, and is seeking a deeper connection with its creator. The fact that Frankenstein created the monster is a repeated point in the descriptions, emphasizing the origin of the monster and its relationship with its creator.",98\n940,THE NARRATOR,THE MONSTER,The narrator created the monster and seeks revenge against it,78\n983,THE MONSTER,THE CREATOR,The creator is considering releasing the monster into the wilds of South America,66\n47,FRANKENSTEIN,FRANKENSTEIN\'S CREATURE,Frankenstein\'s creature is speaking to Frankenstein and asking for compassion,65\n862,THE CREATURE,THE MONSTER,The creature is the monster created by the authorThe monster is the creature created by the author,49\n952,THE NARRATOR\'S FATHER,THE MONSTER,The narrator\'s father was destroyed by the monster,49\n994,THE MONSTER,THE PEASANTS,The monster was not present when the author wished to rouse the peasants,48\n119,ENGLAND,THE MONSTER,The monster lived in the heaths of England with the author,47\n995,THE MONSTER,THE EVENT,The event of the monster\'s creation and the author\'s regret,42\n989,THE MONSTER,SWITZERLAND,The monster left Switzerland with the author,41\n974,THE MONSTER,FRANKENSTEIN\'S CREATURE,Frankenstein\'s creature and the monster are the same being,39\n985,THE MONSTER,THE FEMALE,The female is being released with the monsterThe monster is seeking to be released with the female,39\n999,THE MONSTER,THE CRIMINAL JUDGE,The criminal judge agrees to help the narrator capture the monster,39\n984,THE MONSTER,SOUTH AMERICA,The monster is seeking to be released into the wilds of South America,38\n981,THE MONSTER,FRANKENSTEIN\'S FAMILY,The monster is a threat to Frankenstein\'s family,38\n982,THE MONSTER,THOUSANDS OF OTHERS,The monster is a threat to thousands of others,38\n986,THE MONSTER,THE MONSTER\'S COMPANION,The monster is seeking to be released with its companion,38\n990,THE MONSTER,THE RHINE,The monster traveled along the shores of the Rhine with the author,38\n991,THE MONSTER,SCOTLAND,The monster lived in the deserts of Scotland with the author,38\n992,THE MONSTER,THE SEA,The monster was not present when the author gazed at the sea,38\n993,THE MONSTER,THE FISHERMEN,The monster was not present when the author heard the fishermen calling to each other,38\n996,THE MONSTER,THE LABOUR,The author\'s labour of creating the monster,38\n997,THE MONSTER,THE PROMISE,The author\'s promise to create another monster like the first,38\n998,THE MONSTER,THE VOW,The author\'s vow to never resume his labour,38\n975,THE MONSTER,HELL,The monster mentions hell as a place of torture,37\n977,THE MONSTER,THE CAVES OF ICE,The monster mentions the caves of ice as a place of refuge,37\n1000,THE MONSTER,THE COMMISSION,The monster committed the commission,37\n1001,THE MONSTER,THE CREATURE\'S HOME,The creature\'s home is the location where the monster lives,37\n1003,THE MONSTER,THE CREATURE\'S HOME LAND,The creature\'s home land is the location where the monster lives,37\n1002,THE MONSTER,THE CREATURE\'S WORLD,The creature\'s world is the location where the monster lives,37\n976,THE MONSTER,THE DESERT MOUNTAINS,The monster mentions the desert mountains as a place of refuge,37\n979,THE MONSTER,THE GLACIERS,"Based on the provided data, here is a comprehensive summary of the information related to ""THE MONSTER"" and ""THE GLACIERS"":\n\nTHE MONSTER is a mysterious entity that has a unique relationship with THE GLACIERS. According to available information, THE MONSTER is able to hide from pursuit in THE GLACIERS, suggesting that it has a means of concealment or camouflage within these icy landscapes. Furthermore, THE MONSTER itself views THE GLACIERS as a place of refuge, indicating that it finds solace or safety within these frozen environments.\n\nOverall, it appears that THE GLACIERS play a significant role in THE MONSTER\'s behavior and survival, serving as both a means of evasion and a source of comfort.",37\n978,THE MONSTER,THE MOUNTAINS,The monster mentions the mountains as a place of refuge,37\n980,THE MONSTER,THE SKIES,The monster mentions the skies as a place of refuge,37\n987,THE MONSTER,THE SEA OF ICE,"Based on the provided data, here is a comprehensive summary of the entities and their descriptions:\n\nThe Monster is a powerful entity that is said to have the ability to traverse the Sea of Ice. According to the available information, The Monster is expected to be released into the Sea of Ice, where it will utilize its traversing abilities to navigate through the icy terrain.",37\n988,THE MONSTER,THE RIDGES OF INACCESSIBLE PRECIPICES,The monster can hide from pursuit in the ridges of inaccessible precipices,37\n1033,THE FIEND,SOUTH AMERICA,The fiend would go to South America if the creator were to consent to his request,19\n1004,FRANKENSTEIN\'S CREATURE,FRANKENSTEIN\'S CREATOR,Frankenstein\'s creator made the monster,14\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
22:27:26,726 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\n  "title": "The Monster and Frankenstein\'s Creation",\n  "summary": "The community revolves around The Monster, a creature created by Victor Frankenstein, and its relationships with other entities such as Frankenstein\'s Creature, South America, and Hell. The Monster\'s existence is marked by a desire for freedom and autonomy, and its interactions with Frankenstein reveal a capacity for intelligence, emotion, and morality.",\n  "rating": 8.0,\n  "rating_explanation": "The impact severity rating is high due to the potential threat posed by The Monster\'s existence and its desire for freedom and autonomy.",\n  "findings": [\n    {\n      "summary": "The Monster as a central entity",\n      "explanation": "The Monster is the central entity in this community, serving as the focal point for all other entities. Its creation by Victor Frankenstein and its interactions with other entities such as Frankenstein\'s Creature and South America are crucial in understanding the dynamics of this community. The Monster\'s desire for freedom and autonomy is a key aspect of its existence, and its capacity for intelligence, emotion, and morality challenges the notion of its inherent wickedness. [Data: Entities (387), Relationships (612, 46, 940)]"\n    },\n    {\n      "summary": "Frankenstein\'s Creature as a related entity",\n      "explanation": "Frankenstein\'s Creature is another key entity in this community, being the being created by Frankenstein and associated with The Monster. The relationship between Frankenstein\'s Creature and The Monster is crucial in understanding the dynamics of this community, and the fact that they are the same being suggests a deep connection between them. [Data: Entities (388), Relationships (47, 974)]"\n    },\n    {\n      "summary": "South America as a significant location",\n      "explanation": "South America is a significant location in this community, being the continent where The Monster seeks to be released into the wilds. The relationship between South America and The Monster is crucial in understanding the dynamics of this community, and the fact that The Monster views South America as a place of freedom and autonomy suggests a deep connection between them. [Data: Entities (533), Relationships (984, 1033)]"\n    },\n    {\n      "summary": "Hell as a place of torture",\n      "explanation": "Hell is a place of torture mentioned by The Monster, suggesting a deep connection between The Monster and the concept of hell. The fact that The Monster views hell as a place of torture suggests a complex and nuanced understanding of the concept of hell, and its relationship with The Monster is crucial in understanding the dynamics of this community. [Data: Entities (389), Relationships (975)]"\n    },\n    {\n      "summary": "The Monster\'s capacity for intelligence and emotion",\n      "explanation": "The Monster\'s capacity for intelligence and emotion is a key aspect of its existence, and its interactions with Frankenstein reveal a deep understanding of these concepts. The fact that The Monster is able to speak to Frankenstein and ask for compassion suggests a complex and nuanced understanding of the human condition, and its relationship with Frankenstein is crucial in understanding the dynamics of this community. [Data: Entities (387), Relationships (46, 940)]"\n    },\n    {\n      "summary": "The Monster\'s desire for freedom and autonomy",\n      "explanation": "The Monster\'s desire for freedom and autonomy is a key aspect of its existence, and its interactions with Frankenstein reveal a deep connection between The Monster and the concept of freedom. The fact that The Monster seeks to be released into the wilds of South America suggests a deep understanding of the concept of freedom, and its relationship with Frankenstein is crucial in understanding the dynamics of this community. [Data: Entities (387), Relationships (984, 1033)]"\n    },\n    {\n      "summary": "The Monster\'s capacity for morality",\n      "explanation": "The Monster\'s capacity for morality is a key aspect of its existence, and its interactions with Frankenstein reveal a deep understanding of this concept. The fact that The Monster is able to speak to Frankenstein and ask for compassion suggests a complex and nuanced understanding of the human condition, and its relationship with Frankenstein is crucial in understanding the dynamics of this community. [Data: Entities (387), Relationships (46, 940)]"\n    }'}}
22:27:26,733 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:27:26,733 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 92
22:28:19,130 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:28:32,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5474156690761447. input_tokens=3582, output_tokens=995
22:29:21,351 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:29:21,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.354487147065811. input_tokens=3306, output_tokens=952
22:30:13,109 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:30:24,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2780957160284743. input_tokens=3483, output_tokens=836
22:31:04,80 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:31:15,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.482433035969734. input_tokens=3497, output_tokens=822
22:32:15,907 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
22:32:15,913 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n113,NARRATOR,"Based on the provided data, the comprehensive summary of the narrator is as follows:\n\nThe narrator is the main character of the story, who is a person writing the journal. He is a being created by the creator, often referred to as the monster. The narrator is seeking revenge for the murder of his loved ones, which drives his actions throughout the story. He is also observing the cottagers, indicating that he is watching and learning from them. Additionally, the narrator is eager to learn and desires to uncover the secrets of heaven and earth, suggesting that he is curious and driven by a desire for knowledge.",50\n141,CLERALV,"Clerval is a significant character in the narrative, associated with the entity CLERALV. He is described as a friend of Frankenstein, who was tragically murdered by the monster. This event is mentioned by the narrator, highlighting the devastating impact of the daemon\'s actions. \n\nFurther, Clerval is portrayed as a person who is deeply concerned with the moral implications of various aspects of life and hopes to make a positive contribution to humanity. This introspective nature of Clerval\'s personality is a notable aspect of his character.\n\nInterestingly, Clerval appears in the author\'s dreams, suggesting a lasting impression on the author\'s subconscious. The recurring theme of Clerval\'s presence in the author\'s dreams underscores the significance of his character in the narrative.\n\nIt is worth noting that Clerval\'s life was cut short due to the monster\'s actions, resulting in his untimely death. This event serves as a pivotal moment in the narrative, highlighting the consequences of the monster\'s actions and the impact on those around him.",17\n138,RONCESVALLES,Roncesvalles is a location associated with heroic songs and tales of chivalry,3\n156,ROUND TABLE,Round Table is a location associated with King Arthur,3\n153,CAMPAGNE,Campagne is a location associated with the narrator\'s family,2\n155,CITY,City is a location associated with the narrator\'s home,2\n154,LEAGUE,League is a unit of measurement associated with the narrator\'s home,2\n646,MAGISTRATE,The Magistrate is a Genevan official who is skeptical of the narrator\'s story.,2\n676,BARRED WINDOWS,The barred windows were a feature of the prison,1\n800,DEPARTED SPIRITS,The spirits of the dead are flitting around the narrator in the cemetery,1\n678,DUNGEON,The dungeon was a part of the prison,1\n799,FIENDISH ENEMY,The narrator\'s enemy is a monster who murdered his loved ones,1\n680,GAOLERS,The gaolers were people who worked at the prison,1\n436,HOVEL,The hovel is where the narrator resides,1\n437,VILLAGERS,The villagers are a group of people who are barbarous and hostile,1\n675,WRETCHED BED,The wretched bed was where the narrator was held,1\n677,SQUALIDNESS,The squalidness of the prison was a notable feature,1\n679,PHYSICIAN,The physician was a person who cared for the narrator,1\n681,TURNKEYS,The turnkeys were people who worked at the prison,1\n682,NURSE,The nurse was a person who cared for the narrator,1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,rank\n275,NARRATOR,MONSTER,"The narrator is the monster, a being created by the creator",121\n262,NARRATOR,ELIZABETH,"The narrator and Elizabeth are closely related entities. Based on the provided descriptions, here is a comprehensive summary:\n\nThe narrator has a complex and multifaceted relationship with Elizabeth. On one hand, Elizabeth is a source of inspiration and comfort to the narrator, who is deeply influenced by her gentleness and kindness. This suggests a positive and nurturing dynamic between the two. However, the narrator is also driven by a desire for revenge, which is motivated by Elizabeth\'s murder. This indicates a darker and more intense aspect of their relationship. Furthermore, the narrator\'s worry about Elizabeth implies a sense of concern and care for her well-being, which is in contrast to the desire for revenge. To reconcile these contradictions, it can be inferred that the narrator\'s worry about Elizabeth was likely a response to her murder, and their desire for revenge is a result of their emotional response to her loss. Overall, the narrator\'s relationship with Elizabeth is marked by a mix of positive and negative emotions, with a deep sense of connection and a strong desire for justice.",101\n431,CLERALV,MONSTER,"The monster murdered Clerval, a friend of Frankenstein",88\n72,FRANKENSTEIN,CLERALV,Frankenstein\'s friend Clerval was murdered by the monster,79\n263,NARRATOR,GENEVA,The narrator\'s parents are residents of Geneva,79\n371,ELIZABETH,CLERALV,Clerval is deeply influenced by Elizabeth\'s gentleness and kindnessClerval is a source of inspiration and comfort to ElizabethElizabeth is a source of inspiration and comfort to Clerval,68\n266,NARRATOR,CLERALV,The narrator and Clerval are close friendsClerval is a source of inspiration and comfort to the narratorClerval and the narrator are close friends,67\n292,NARRATOR,WILLIAM,The narrator is seeking revenge for the murder of William,66\n281,NARRATOR,FATHER,The narrator was worried about his father,65\n176,FRIEND,NARRATOR,The narrator is a friend of the stranger,61\n267,NARRATOR,LAKE,The narrator resides near a lake,58\n282,NARRATOR,DAEMON,The narrator is worried about his daemon,58\n276,NARRATOR,OLD MAN,The narrator observes the old man,57\n258,SEA,NARRATOR,The narrator is on the sea with the stranger,57\n277,NARRATOR,YOUNG GIRL,The narrator observes the young girl,56\n278,NARRATOR,YOUNG MAN,The narrator observes the young man,56\n252,NATURE,NARRATOR,The narrator refers to nature as the world around them,56\n264,NARRATOR,BELRIVE,The narrator resides in a campagne on Belrive,55\n164,OCEAN,NARRATOR,The narrator is on the ocean with the stranger,55\n242,HEAVEN,NARRATOR,The narrator refers to heaven as a metaphorical place,55\n246,WORLD,NARRATOR,The narrator is on the world with the stranger,55\n249,BROTHER,NARRATOR,The narrator is a brother to the stranger,55\n272,NARRATOR,HOLY SEPULCHRE,The narrator is associated with the holy sepulchre,54\n273,NARRATOR,INFIDELS,The narrator is opposed to the infidels,54\n291,NARRATOR,WIFE,The narrator saw the wife of the turnkey,54\n296,NARRATOR,WIND,The narrator is standing in the cemetery with the wind gently agitating the leaves of the trees,54\n169,SHIP,NARRATOR,The narrator is on the ship with the stranger,54\n375,ELIZABETH,RONCESVALLES,Elizabeth is associated with Roncesvalles,54\n374,ELIZABETH,ROUND TABLE,Elizabeth is associated with the Round Table,54\n265,NARRATOR,RONCESVALLES,The narrator is associated with Roncesvalles,53\n271,NARRATOR,ROUND TABLE,The narrator is associated with the Round Table,53\n240,DECK,NARRATOR,The narrator often goes to the deck of the ship,53\n241,SLICHE,NARRATOR,The narrator is watching for the sledge,53\n244,ELEMENTAL FOES,NARRATOR,The narrator refers to the elemental foes as a metaphorical concept,53\n245,RACE,NARRATOR,The narrator refers to the race as a metaphorical concept,53\n256,STAR,NARRATOR,The narrator refers to the star as a celestial object,53\n257,SKY,NARRATOR,The narrator refers to the sky as the atmosphere around the Earth,53\n268,NARRATOR,CAMPAGNE,The narrator resides in a campagne,52\n270,NARRATOR,CITY,The narrator resides in a city,52\n269,NARRATOR,LEAGUE,The narrator\'s home is located at a distance of rather more than a league from the city,52\n294,NARRATOR,MAGISTRATE,The magistrate is skeptical of the narrator\'s story,52\n274,NARRATOR,CREATOR,"The creator made the narrator, a monster",52\n297,NARRATOR,NIGHT,The narrator is standing in the cemetery at night,52\n284,NARRATOR,BARRED WINDOWS,The narrator saw the barred windows,51\n295,NARRATOR,DEPARTED SPIRITS,The narrator is standing in the cemetery with the departed spiritsThe narrator is seeking the aid of the departed spirits in his quest for revenge,51\n286,NARRATOR,DUNGEON,The narrator was held in the dungeon,51\n293,NARRATOR,FIENDISH ENEMY,The narrator is seeking revenge for the murder of his loved ones at the hands of the fiendish enemy,51\n288,NARRATOR,GAOLERS,The narrator was guarded by the gaolers,51\n279,NARRATOR,HOVEL,The narrator resides in the hovel,51\n280,NARRATOR,VILLAGERS,The narrator has a negative relationship with the villagers,51\n283,NARRATOR,WRETCHED BED,The narrator was held on the wretched bed,51\n285,NARRATOR,SQUALIDNESS,The narrator experienced the squalidness of the prison,51\n287,NARRATOR,PHYSICIAN,The narrator was cared for by the physician,51\n289,NARRATOR,TURNKEYS,The narrator was guarded by the turnkeys,51\n290,NARRATOR,NURSE,The narrator was cared for by the nurse,51\n320,GENEVA,CLERALV,Clerval is a resident of Geneva,46\n429,CLERALV,FIEND,The fiend murdered Clerval,41\n432,CLERALV,FRANKENSTEIN\'S CREATOR,Frankenstein\'s creator\'s friend Clerval was murdered by the monster,28\n422,CLERALV,LAKE,Clerval resides near a lake,25\n430,CLERALV,DAEMON,The daemon was responsible for the death of Clerval,25\n417,BELRIVE,CLERALV,Clerval resides in a campagne on Belrive,22\n427,CLERALV,HOLY SEPULCHRE,Clerval is associated with the holy sepulchre,21\n428,CLERALV,INFIDELS,Clerval is opposed to the infidels,21\n426,CLERALV,ROUND TABLE,Clerval is associated with the Round Table,20\n421,RONCESVALLES,CLERALV,Clerval is associated with Roncesvalles,20\n423,CLERALV,CAMPAGNE,Clerval resides in a campagne,19\n425,CLERALV,CITY,Clerval resides in a city,19\n424,CLERALV,LEAGUE,Clerval\'s home is located at a distance of rather more than a league from the city,19\n1230,MAGISTRATE,MR. KIRWIN,Mr. Kirwin is a magistrate,7\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
22:32:15,914 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\n  "title": "Narrator\'s Community",\n  "summary": "The community revolves around the narrator, who is a central figure with complex relationships with various entities. The narrator\'s relationships with entities such as Elizabeth, Clerval, and the monster are significant, and their interactions have a profound impact on the community.",\n  "rating": 8.0,\n  "rating_explanation": "The impact severity rating is high due to the narrator\'s desire for revenge and the potential for conflict with other entities in the community.",\n  "findings": [\n    {\n      "summary": "Narrator\'s central role",\n      "explanation": "The narrator is the central figure in this community, with complex relationships with various entities. The narrator\'s relationships with entities such as Elizabeth, Clerval, and the monster are significant, and their interactions have a profound impact on the community. The narrator\'s desire for revenge and their emotional response to the murder of their loved ones drive their actions and shape the dynamics of the community. [Data: Entities (113), Relationships (275, 262, 431)]"\n    },\n    {\n      "summary": "Elizabeth\'s significance",\n      "explanation": "Elizabeth is a significant entity in this community, with a complex relationship with the narrator. The narrator\'s desire for revenge is motivated by Elizabeth\'s murder, and their interactions have a profound impact on the community. Elizabeth\'s gentleness and kindness are notable aspects of her character, and her influence on the narrator is significant. [Data: Entities (141), Relationships (262, 371)]"\n    },\n    {\n      "summary": "Clerval\'s tragic fate",\n      "explanation": "Clerval is a significant entity in this community, with a tragic fate that has a profound impact on the community. The monster\'s murder of Clerval is a pivotal moment in the narrative, highlighting the consequences of the monster\'s actions and the impact on those around him. Clerval\'s life was cut short due to the monster\'s actions, resulting in his untimely death. [Data: Entities (141), Relationships (431, 72)]"\n    },\n    {\n      "summary": "Monster\'s impact",\n      "explanation": "The monster is a significant entity in this community, with a profound impact on the community. The monster\'s actions have a devastating effect on the narrator and other entities, and their interactions have a profound impact on the community. The monster\'s creation by the creator and its subsequent actions drive the narrative and shape the dynamics of the community. [Data: Entities (113), Relationships (275, 431)]"\n    },\n    {\n      "summary": "Geneva\'s significance",\n      "explanation": "Geneva is a significant location in this community, with a profound impact on the community. The narrator\'s parents are residents of Geneva, and the city\'s influence on the narrator is significant. Geneva\'s association with Clerval and the monster is also notable, highlighting the complex relationships between entities in the community. [Data: Entities (138), Relationships (263, 320)]"\n    },\n    {\n      "summary": "Roncesvalles\' significance",\n      "explanation": "Roncesvalles is a significant location in this community, with a profound impact on the community. Elizabeth is associated with Roncesvalles, and the location\'s influence on the narrator is significant. Roncesvalles\' association with Clerval and the monster is also notable, highlighting the complex relationships between entities in the community. [Data: Entities (156), Relationships (375, 421)]"\n    },\n    {\n      "summary": "Round Table\'s significance",\n      "explanation": "The Round Table is a significant location in this community, with a profound impact on the community. Elizabeth is associated with the Round Table, and the location\'s influence on the narrator is significant. The Round Table\'s association with Clerval and the monster is also notable, highlighting the complex relationships between entities in the community. [Data: Entities (156), Relationships (374, 426)]"\n    },\n    {\n      "summary": "Campagne\'s significance",\n      "explanation": "Campagne is a significant location in this community, with a profound impact on the community. The narrator resides in a campagne, and the location\'s influence on the narrator is significant. Campagne\'s association with Clerval and the monster is also notable, highlighting the complex relationships between entities in the community. [Data: Entities (153), Relationships (264, 423)]"\n    }'}}
22:32:15,919 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:32:15,919 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 102
22:33:07,262 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:33:17,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5176296490244567. input_tokens=3860, output_tokens=805
22:34:16,402 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
22:34:16,405 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n357,THE NARRATOR,"The Narrator is a complex and multifaceted character. He is a person who has a dark and troubled past, having created a monster through a cursed creator. However, this act of creation was not a benevolent one, as the narrator seeks revenge against the monster he created. This suggests that the narrator\'s intentions were malevolent from the start.\n\nFurthermore, the narrator is also a scientist who has committed deeds of mischief beyond description, which are horrible in nature. This implies that the narrator has a history of engaging in unethical and possibly even inhumane scientific experiments.\n\nAdditionally, the narrator is also a young man who is accused of murder and is being held in prison. This suggests that the narrator has a history of violence and has likely committed other crimes in the past.\n\nHowever, the narrator is also the one who discovered De Lacey and Agatha in the cottage, which suggests that he may have a more benevolent side to his personality. This discovery could also imply that the narrator has a certain level of curiosity and a desire to learn about others.\n\nOverall, the narrator is a complex and conflicted character with a dark and troubled past. He is driven by a desire for revenge and has a history of engaging in unethical and violent behavior. However, he also has a certain level of curiosity and a desire to learn about others, which may suggest that he is not entirely beyond redemption.",42\n649,MR. KIRWIN,"Mr. Kirwin is a sympathetic magistrate who exhibits kindness and compassion towards the narrator. He attempts to comfort the narrator and shows a genuine concern for their well-being. Additionally, Mr. Kirwin is a physician who possesses a deep understanding of the human body and its effects on individuals. He desires to observe the impact of the sight of the body on the narrator, suggesting a desire to understand the emotional and psychological effects of such a traumatic experience.",5\n792,THE CRIMINAL JUDGE,The criminal judge is a person who listens to the narrator\'s story and agrees to help him capture the monster,3\n798,THE ACCUSATION,The accusation is a charge made by the narrator against the monster,2\n514,THE ARCH-FIEND,The arch-fiend is the narrator,2\n37,CHAPTER 15,"CHAPTER 15 is the fifteenth chapter of the novel. In this chapter, the narrator gained significant insight into the history of the cottagers, a group of people he holds dear.",1\n38,CHAPTER 16,"CHAPTER 16 is the sixteenth chapter of the novel, where the narrator reflects on his situation.",1\n493,LEATHERN PORTMANTEAU,Container found by the narrator in the wood,1\n793,MARY,Mary is a person mentioned in the text as being a friend of the narrator,1\n506,PARADISE LOST,Paradise Lost was the book that the narrator read,1\n495,PLUTARCH,"Plutarch was the author of the book ""Plutarch\'s Lives""",1\n507,PLUTARCH\'S LIVES,Plutarch\'s Lives was the book that the narrator read,1\n795,THE CELL,The cell is a location where the narrator was held,1\n794,THE DUNGEON,The dungeon is a location where the narrator was held,1\n510,THE HAVEL,The hovel is the narrator\'s retreat,1\n503,THE MONTH OF AUGUST,The month of August was the time when the narrator found the leathern portmanteau,1\n496,WERTER,"Werter was the author of the book ""The Sorrows of Werter""",1\n504,THE NIGHT,The night was the time when the narrator found the leathern portmanteau,1\n505,THE WOOD,"The Wood is a location where the narrator often wanders at night. It is also the place where the narrator discovered a significant item, a leathern portmanteau, which may hold some importance or significance to the narrator\'s journey or experiences.",1\n508,THE SORROWS OF WERTER,The Sorrows of Werter was the book that the narrator read,1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,rank\n401,ELIZABETH,THE NARRATOR,The narrator\'s sister Elizabeth is mentioned as being safe and well,93\n940,THE NARRATOR,THE MONSTER,The narrator created the monster and seeks revenge against it,78\n924,THE NARRATOR,THE CREATOR,The creator formed the narratorThe narrator is a creature created by the creator,72\n330,GENEVA,THE NARRATOR,The narrator is from Geneva and his father writes to Geneva to inform them of his misfortune,71\n55,FRANKENSTEIN,MR. KIRWIN,Mr. Kirwin was a character in the novel Frankenstein,67\n936,THE NARRATOR,THE FIEND,The narrator mentions the fiend as a person who is responsible for his friend\'s murder,59\n855,WILLIAM,THE NARRATOR,William is a friend of the narrator,58\n914,THE NARRATOR,AGATHA,The narrator discovered De Lacey and Agatha in the cottage,55\n930,THE NARRATOR,THE NARRATOR\'S FATHER,The narrator\'s father visits him in prison and tries to comfort him,55\n861,THE CREATURE,THE NARRATOR,The narrator is a creature,55\n847,ALBERT,THE NARRATOR,Albert is a friend of the narrator,54\n929,THE NARRATOR,CLERVAL,The narrator and Clerval were friends who were murdered,52\n913,THE NARRATOR,DE LACEY,"The narrator is a creature created by De Lacey\'s creator. This creature discovered De Lacey and Agatha in their cottage, indicating a close relationship between the narrator and the De Lacey family.",51\n938,THE NARRATOR,THE NURSE,The nurse takes care of the narrator,50\n939,THE NARRATOR,ALZIRE,Alzire is not directly related to the narrator,50\n638,THE PRISON,THE NARRATOR,"The narrator is being held in the prison, where they are accused of murder.",50\n933,THE NARRATOR,THE EVENT,The narrator is affected by the event of his friend\'s murder,48\n928,THE NARRATOR,MR. KIRWIN,Mr. Kirwin tries to comfort the narrator and help him with his case,47\n925,THE NARRATOR,THE COTTAGE,The narrator was at the cottage,47\n935,THE NARRATOR,THE PRISONER,The narrator is a prisoner in the prison,47\n932,THE NARRATOR,THE COFFIN OF HENRY,The narrator mentions the coffin of Henry as a reference to his friend who was murdered,46\n934,THE NARRATOR,THE MURDER,The narrator is accused of murder,46\n937,THE NARRATOR,THE ACCUSER,The narrator is accused of murder by the accuser,46\n941,THE NARRATOR,THE CRIMINAL JUDGE,The narrator meets the criminal judge to ask for help in capturing the monster,45\n923,THE NARRATOR,FELIX DE LACEY,The narrator was attacked by Felix De Lacey,45\n945,THE NARRATOR,THE ACCUSATION,The narrator made the accusation against the monster,44\n927,THE NARRATOR,THE ARCH-FIEND,The narrator is like the arch-fiend,44\n931,THE NARRATOR,ERNST,The narrator\'s brother Ernest is mentioned as being safe and well,44\n101,CHAPTER 15,THE NARRATOR,Chapter 15 was the chapter where the narrator learned about the history of his beloved cottagers,43\n102,CHAPTER 16,THE NARRATOR,Chapter 16 is about the narrator\'s reflection,43\n912,THE NARRATOR,LEATHERN PORTMANTEAU,The narrator found the leathern portmanteau in the wood,43\n942,THE NARRATOR,MARY,Mary is a friend of the narrator,43\n920,THE NARRATOR,PARADISE LOST,"The narrator read the book ""Paradise Lost""",43\n915,THE NARRATOR,PLUTARCH,"The narrator read the book ""Plutarch\'s Lives""",43\n921,THE NARRATOR,PLUTARCH\'S LIVES,"The narrator read the book ""Plutarch\'s Lives""",43\n944,THE NARRATOR,THE CELL,The narrator was held in the cell,43\n943,THE NARRATOR,THE DUNGEON,The narrator was held in the dungeon,43\n926,THE NARRATOR,THE HAVEL,The narrator hid in the hovel,43\n917,THE NARRATOR,THE MONTH OF AUGUST,The month of August was the time when the narrator found the leathern portmanteau,43\n916,THE NARRATOR,WERTER,"The narrator read the book ""The Sorrows of Werter""",43\n918,THE NARRATOR,THE NIGHT,The night was the time when the narrator found the leathern portmanteau,43\n919,THE NARRATOR,THE WOOD,"The narrator, a person who is recounting a story, was found in a wooded area, specifically in the wood. They had wandered into the wood, which is a dense area of trees, and during their exploration, they discovered a leathern portmanteau, a type of bag or suitcase made of leather.",43\n922,THE NARRATOR,THE SORROWS OF WERTER,"The narrator read the book ""The Sorrows of Werter""",43\n999,THE MONSTER,THE CRIMINAL JUDGE,The criminal judge agrees to help the narrator capture the monster,39\n1010,THE CREATOR,THE ARCH-FIEND,The creator formed the arch-fiend,32\n762,PRISON,MR. KIRWIN,Mr. Kirwin visited the narrator in prison,10\n1232,MR. KIRWIN,OLD WOMAN,Mr. Kirwin and the old woman worked together to care for the narrator,9\n1230,MAGISTRATE,MR. KIRWIN,Mr. Kirwin is a magistrate,7\n1251,THE CRIMINAL JUDGE,THE ACCUSATION,The criminal judge listened to the narrator\'s accusation,5\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
22:34:16,405 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 80, in _invoke_json
    result = await generate()
             ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 72, in generate
    await self._native_json(input, **{**kwargs, "name": call_name})
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 96, in _native_json
    result = await self._invoke(
             ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 1339, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1816, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1510, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/openai/_base_client.py", line 1611, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Failed to generate JSON. Please adjust your prompt. See 'failed_generation' for more details.", 'type': 'invalid_request_error', 'code': 'json_validate_failed', 'failed_generation': '{\n  "title": "The Narrator\'s Community",\n  "summary": "The community revolves around the narrator, who is a complex and conflicted character with a dark and troubled past. The narrator is connected to various entities, including Mr. Kirwin, the criminal judge, and the monster, through relationships and descriptions.",\n  "rating": 6.5,\n  "rating_explanation": "The impact severity rating is moderate due to the narrator\'s history of violence and unethical behavior, as well as the presence of a monster that seeks revenge against the narrator.",\n  "findings": [\n    {\n      "summary": "The Narrator\'s Complex Personality",\n      "explanation": "The narrator is a complex and conflicted character with a dark and troubled past. He is driven by a desire for revenge and has a history of engaging in unethical and violent behavior. However, he also has a certain level of curiosity and a desire to learn about others, which may suggest that he is not entirely beyond redemption. [Data: Entities (357); Relationships (940, 924, 330)]"\n    },\n    {\n      "summary": "Mr. Kirwin\'s Role in the Community",\n      "explanation": "Mr. Kirwin is a sympathetic magistrate who exhibits kindness and compassion towards the narrator. He attempts to comfort the narrator and shows a genuine concern for their well-being. Mr. Kirwin\'s relationship with the narrator is crucial in understanding the dynamics of this community. [Data: Entities (649); Relationships (55, 762, 1230)]"\n    },\n    {\n      "summary": "The Monster\'s Impact on the Community",\n      "explanation": "The monster is a significant entity in this community, seeking revenge against the narrator who created it. The monster\'s presence has a profound impact on the narrator\'s life and behavior, and its actions have consequences for the community as a whole. [Data: Entities (514); Relationships (940, 999)]"\n    },\n    {\n      "summary": "The Narrator\'s Relationships with Other Entities",\n      "explanation": "The narrator has relationships with various entities, including Mr. Kirwin, the criminal judge, and the monster. These relationships are complex and multifaceted, and they play a significant role in shaping the narrator\'s behavior and the dynamics of the community. [Data: Relationships (401, 924, 330, 55, 762)]"\n    },\n    {\n      "summary": "The Community\'s Overall Structure",\n      "explanation": "The community revolves around the narrator, who is connected to various entities through relationships and descriptions. The narrator\'s complex personality and history of violence and unethical behavior are central to the community\'s dynamics, and the presence of the monster adds an element of danger and unpredictability. [Data: Entities (357, 649, 514); Relationships (940, 924, 330, 55, 762)]"\n    },\n    {\n      "summary": "The Impact of the Narrator\'s Past on the Community",\n      "explanation": "The narrator\'s dark and troubled past has a profound impact on the community, shaping his behavior and relationships with other entities. The narrator\'s history of violence and unethical behavior is a significant factor in the community\'s dynamics, and it has consequences for the narrator and those around him. [Data: Entities (357); Relationships (940, 924, 330)]"\n    },\n    {\n      "summary": "The Role of Mr. Kirwin in the Community",\n      "explanation": "Mr. Kirwin plays a significant role in the community, exhibiting kindness and compassion towards the narrator and attempting to comfort him. Mr. Kirwin\'s relationship with the narrator is crucial in understanding the dynamics of this community, and his presence adds a sense of stability and support. [Data: Entities (649); Relationships (55, 762, 1230)]"\n    },\n    {\n      "summary": "The Monster\'s Impact on the Narrator",\n      "explanation": "The monster has a profound impact on the narrator, seeking revenge against him and causing him significant distress. The monster\'s presence adds an element of danger and unpredictability to the community, and it has consequences for the narrator and those around him. [Data: Entities (514); Relationships (940, 999)]"\n    },\n    {\n      "summary": "The Community\'s Overall Dynamics",\n      "explanation": "The community revolves around the narrator, who is connected to various entities through relationships and descriptions. The narrator\'s complex personality and history of violence and unethical behavior are central to the community\'s dynamics, and the presence of the monster adds an element of danger and unpredictability. [Data: Entities (357, 649, 514); Relationships (940, 924, 330, 55, 762)]"\n    }'}}
22:34:16,417 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:34:16,417 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 90
22:34:17,390 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:34:17,392 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:34:17,392 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 25
22:34:17,405 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:34:17,407 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:34:17,407 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 29
22:34:46,45 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:34:46,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6358010680414736. input_tokens=2023, output_tokens=422
22:35:15,202 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:35:15,205 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:35:15,214 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:35:15,214 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 47
22:35:22,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5036464920267463. input_tokens=2110, output_tokens=532
22:35:50,6 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:35:56,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7146053890464827. input_tokens=2272, output_tokens=504
22:36:31,190 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:36:38,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5827312199398875. input_tokens=2478, output_tokens=555
22:37:08,438 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:37:08,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.4607145950431004. input_tokens=2120, output_tokens=368
22:37:37,614 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:37:43,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.4014593840111047. input_tokens=2130, output_tokens=470
22:38:14,337 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:38:21,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6513849770417437. input_tokens=2206, output_tokens=560
22:38:58,29 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:38:58,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5733374280389398. input_tokens=2325, output_tokens=578
22:39:26,875 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:39:33,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5196479490259662. input_tokens=2033, output_tokens=524
22:40:06,288 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:40:06,291 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 162, in execute_with_retry
    await self._rate_limiter.acquire(input_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/graphrag/llm/limiting/tpm_rpm_limiter.py", line 32, in acquire
    await self._tpm_limiter.acquire(num_tokens)
  File "/opt/anaconda3/envs/chat_bot/lib/python3.12/site-packages/aiolimiter/leakybucket.py", line 95, in acquire
    raise ValueError("Can't acquire more than the maximum capacity")
ValueError: Can't acquire more than the maximum capacity
22:40:06,293 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
22:40:06,293 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 56
22:40:13,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.453229786013253. input_tokens=2170, output_tokens=534
22:40:46,291 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:40:46,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5466638109646738. input_tokens=2253, output_tokens=493
22:41:17,537 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:41:24,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7549746219301596. input_tokens=2042, output_tokens=521
22:41:50,779 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:41:59,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.39074402989354. input_tokens=2098, output_tokens=640
22:42:35,284 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:42:42,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6306159819941968. input_tokens=2480, output_tokens=536
22:43:06,809 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:43:12,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.2996946109924465. input_tokens=2048, output_tokens=425
22:43:39,575 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
22:43:45,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8051162220072001. input_tokens=2006, output_tokens=423
22:44:18,450 httpx INFO HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
