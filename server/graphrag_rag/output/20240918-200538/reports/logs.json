{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 18277, Requested 3421. Please try again in 5.092s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 18277, Requested 3421. Please try again in 5.092s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 18178, Requested 3937. Please try again in 6.345s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 18178, Requested 3937. Please try again in 6.345s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17763, Requested 3421. Please try again in 3.552s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 17763, Requested 3421. Please try again in 3.552s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 27520, Requested 3937. Please try again in 34.373s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 27520, Requested 3937. Please try again in 34.373s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26931, Requested 4030. Please try again in 32.883s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26931, Requested 4030. Please try again in 32.883s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26932, Requested 3421. Please try again in 31.061s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26932, Requested 3421. Please try again in 31.061s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26932, Requested 3421. Please try again in 31.061s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26932, Requested 3421. Please try again in 31.061s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "Queen, thou art the fairest in the land!\"\n\nNow there was peace at last in the heart of the Queen--that is,\nas much peace as can ever be found in a heart full of envy and\nhate.\n\n*          *          *          *          *\n\nAfter the wicked Queen had gone away, the sun sank down behind\nthe seventh hill and the dwarfs came trudging home from work.\nWhen they reached their little home, no light gleamed from its\nwindows, no smoke streamed from its chimney. Inside all was dark\nand silent--no lamps were lit and no supper was on the table.\nSnow White lay on the floor and no breath came from her lips.\n\nAt this sight the seven little dwarfs were filled with woe, for\nwell they knew that this was once more the work of the wicked\nQueen.\n\n\"We must save her!\" they cried, and hurried here and there. They\nlit their seven lights, then took Snow White and laid her on the\nbed. They searched for something poisonous but found nothing.\nThey loosened her bodice, combed her hair and washed her face\nwith water and wine, but nothing helped: the poor child did not\nmove, did not speak, did not open her eyes.\n\n\"Alas!\" cried the dwarfs. \"We have done all we could, and now\nSnow White is lost to us forever!\"\n\nGravely they shook their heads, sadly they stroked their beards,\nand then they all began to cry. They cried for three whole days\nand when at last they dried their tears, there lay Snow White,\nstill motionless to be sure, but so fresh and rosy that she\nseemed to be blooming with health.\n\n\"She is as beautiful as ever,\" said the dwarfs to each other,\n\"and although we cannot wake her, we must watch her well and keep\nher safe from harm.\"\n\nSo they made a beautiful crystal casket for Snow White to lie in.\nIt was transparent all over so that she could be seen from every\nside. On its lid they wrote in golden letters:\n\n   SNOW WHITE--A PRINCESS\n\nand when it was all finished they laid Snow White inside and\ncarried it to one of the seven hilltops. There they placed it\namong the trees and flowers, and the birds of the wood came and\nmourned for her, first an owl, then a raven, and last of all a\nlittle dove.\n\nNow only six little dwarfs went to dig in the hills every day,\nfor each in his turn stayed behind to watch over Snow White so\nthat she was never alone.\n\nWeeks and months and years passed by, and all this time Snow\nWhite lay in her crystal casket and did not move or open her\neyes. She seemed to be in a deep deep sleep, her face as fair as\na happy dream, her cheeks as rosy as ever. The flowers grew gaily\nabout her, the clouds flew blithely above. Birds perched on the\ncrystal casket and trilled and sang, the woodland beasts grew\ntame and came to gaze in wonder.\n\nSome one else came too and gazed in wonder--not a bird or a\nrabbit or a deer, but a young Prince who had lost his way while\nwandering among the seven hills. When he saw the motionless\nmaiden, so beautiful and rosy red, he looked and looked and\nlooked. Then he went to the dwarfs and said, \"Please let me take\nthis crystal casket home with me and I will give you all the gold\nyou may ask for.\"\n\nBut the dwarfs shook their heads and said, \"We would not give it\nup for all the riches in the world.\"\n\nAt this the Prince looked troubled and his eyes filled with\ntears.\n\n\"If you won't take gold,\" he said, \"then please give her to me\nout of the goodness of your golden hearts. I know not why, but my\nheart is drawn toward this beautiful Princess. If you will let me\ntake her home with me, I will guard and honor her as my greatest\ntreasure.\"\n\nWhen they heard this, the kind little dwarfs took pity on the\nPrince and made him a present of Snow White in her beautiful\ncasket.\n\nThe Prince thanked them joyfully and called for his servants.\nGently they placed the crystal casket on their shoulders, slowly\nthey walked away. But in spite of all their care, one of the\nservants made a false step and stumbled over a gnarly root. This\njoggled the casket, and the jolt shook the piece of poisoned\napple right out of Snow White's throat. And lo! she woke up at\nlast and was as well as ever. Then all by herself she opened the\nlid, sat up, and looked about her in astonishment.\n\nThe Prince rushed up and lifted her out of the casket. He told\nher all that had happened and begged her to be his bride. Snow\nWhite consented with sparkling eyes, so they rode away to the\nPrince's home where they prepared for a gay and gala wedding.\n\n*          *          *          *          *\n\nBut while this was going on in the Prince's castle, something\nelse was happening in that other castle where lived the wicked\nQueen. She had been invited to a mysterious wedding, so she\ndressed herself in her festive best and stood in front of her\nmirror and said:\n\n   \"Mirror, Mirror, on the wall,\n   Who's the fairest one of all?\"\n\nand the mirror answered:\n\n   \"Thou art very fair, Oh Queen,\n   But the fairest ever seen\n   Is Snow White, alive and well,\n   Standing 'neath a wedding"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26580, Requested 3937. Please try again in 31.552s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26580, Requested 3937. Please try again in 31.552s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26580, Requested 3937. Please try again in 31.552s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 26580, Requested 3937. Please try again in 31.552s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "away then, poor child, and may the beasts of\nthe wood have mercy on you.\" As a token he brought back the heart\nof a wild boar, and the wicked Queen thought it was Snow White's.\nShe had it cooked and ate it, I am sorry to say, with salt and\ngreat relish.\n\n*          *          *          *          *\n\nLittle Snow White wandered off into the depths of the wildwood.\nAbove her were leaves and leaves and leaves, about her the trunks\nof hundreds of trees, and she didn't know what to do. She began\nto run, over jagged stones and through thorny thickets. She\npassed many wild animals on the way, but they did not hurt her.\nShe ran all day, through woods and woods and over seven high high\nhills. At last, just at sunset, she came upon a tiny hut in a\nwooded glen. The door was open and there was no one at home, so\nshe thought she would stay and rest herself a little.\n\nShe went in and looked around. Everything was very small inside,\nbut as neat and charming as could be, and very very clean. At one\nend of the room stood a table decked in white, and on it were\nseven little plates, seven little knives and forks and spoons,\nand seven little goblets. In front of the table, each in its\nplace, were seven little chairs; and at the far side of the room\nwere seven beds, one beside the other, all made up with coverlets\nas pure and white as plum blossoms.\n\nSnow White was hungry and thirsty, so she took from each little\nplate a bit of vegetable and a bite of bread, and from each\nlittle goblet a sip of sweet wine. She had become very tired,\ntoo, from all her running, and felt like taking a nap. She tried\none bed after another but found it hard to choose the one which\nreally suited her.\n\nThe first little bed was too hard.\n\nThe second little bed was too soft.\n\nThe third little bed was too short.\n\nThe fourth little bed was too narrow.\n\nThe fifth little bed was too flat.\n\nThe sixth little bed was too fluffy.\n\nBut the seventh little bed was just right so she lay down in it\nand was soon fast asleep.\n\nAfter the sun had set behind the seventh hilltop and darkness had\ncrept into the room, the masters of the little hut came\nhome--they were seven little dwarfs who dug all day and hacked\naway at the hills, in search of gems and gold. They lit their\nseven little lights and saw right away that someone had been\nthere, for things were not quite the same as they had left them\nin the morning.\n\nSaid the first little dwarf, \"Who's been sitting in my chair?\"\n\nSaid the second little dwarf, \"Who's been eating from my plate?\"\n\nSaid the third, \"Who's been nibbling at my bread?\"\n\nSaid the fourth, \"Who's been tasting my vegetables?\"\n\nSaid the fifth, \"Who's been eating with my fork?\"\n\nAnd the sixth, \"Who's been cutting with my knife?\"\n\nAnd the seventh, \"Who's been drinking from my little goblet?\"\n\nNow the first little dwarf turned around, and saw a hollow in his\nbed and said, \"Someone's been sleeping in my bed.\"\n\nAnd the second little dwarf looked at his bed and said,\n\"Someone's been sleeping in mine too. It's rumpled.\"\n\nAnd the third said, \"In mine too, it's all humped up and\ncrumpled.\"\n\nAnd the fourth said, \"In mine too. It's full of wrinkles.\"\n\nAnd the fifth said, \"And mine. It's full of crinkles.\"\n\nAnd the sixth said, \"Mine too. It's all tumbled up and jumbled.\"\n\nBut the seventh cried, \"Well, someone's been sleeping in my bed,\nAND HERE SHE IS!\"\n\nThe others came crowding around, murmuring and whispering in\nwonderment at the sight. \"Ei! Ei!\" they said, \"how beautiful is\nthis child!\" They brought their tiny lights and held them high,\nand looked and looked and looked. So pleased were they with their\nnew little guest that they did not even wake her, but let her\nsleep in the bed all night. The seventh dwarf now had no bed, to\nbe sure, but he slept with his comrades, one hour with each in\nturn until the night was over.\n\nIn the morning when Snow White awoke and saw seven little men\ntiptoeing about the room, she was frightened, but not for long.\nShe soon saw that they were friendly little folk, so she sat up\nin bed and smiled at them. Now that she was awake and well\nrested, she looked more lovely than ever, with her rosy cheeks\nand big black eyes. The seven little dwarfs circled round her in\nnew admiration and awe, and said, \"What is your name, dear\nchild?\"\n\n\"They call me Snow White,\" said she.\n\n\"And how did you find your way to our little home?\" asked the\ndwarfs. So she told them her story.\n\nAll seven stood around and listened, nodding their heads and\nstroking their long long beards, and then they said, \"Do you\nthink you could be our little housekeeper--cook and knit and sew\nfor us, make up our beds and wash our little clothes? If you will\nkeep everything tidy and homelike, you can stay with us, and\nyou shall want for nothing in the world.\"\n\n\"Oh yes, with all my heart!\" cried Snow White. So there she\nstayed, and washed and sewed and knitted"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 33589, Requested 4030. Please try again in 52.858999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 33589, Requested 4030. Please try again in 52.858999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 32718, Requested 4030. Please try again in 50.245999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 32718, Requested 4030. Please try again in 50.245999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 32718, Requested 4030. Please try again in 50.245999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 32718, Requested 4030. Please try again in 50.245999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "king their long long beards, and then they said, \"Do you\nthink you could be our little housekeeper--cook and knit and sew\nfor us, make up our beds and wash our little clothes? If you will\nkeep everything tidy and homelike, you can stay with us, and\nyou shall want for nothing in the world.\"\n\n\"Oh yes, with all my heart!\" cried Snow White. So there she\nstayed, and washed and sewed and knitted, and kept house for the\nkindly little men. Every day the seven dwarfs went off to one of\nthe seven hills to dig for gems and gold. Each evening after\nsunset they returned, and then their supper had to be all ready\nand laid out on the table. But every morning before they left\nthey would warn Snow White about the Queen.\n\n\"We don't trust her,\" they said. \"One of these days she'll find\nout that you are here. So be careful, child, and don't let anyone\ninto the house.\"\n\n*          *          *          *          *\n\nThe dwarfs were right. One day the Queen, just to make sure,\nstood in front of her mirror and said:\n\n   \"Mirror, Mirror, on the wall,\n   Who's the fairest one of all?\"\n\nand the mirror replied:\n\n   \"Thou art very fair, Oh Queen,\n   But the fairest ever seen\n   Dwells within the wooded glen\n   With the seven little men.\"\n\nThe Queen turned green with fury when she heard this, for now she\nknew that the huntsman had deceived her, and that Snow White was\nstill alive.\n\nDay and night she sat and pondered, and wondered what to do, for\nas long as she was not the fairest in the land, her jealous heart\ngave her no rest. At last she thought out a plan: she dyed her\nface and dressed herself to look like a peddler woman. She did it\nso well that no one would have known her, and then, with a\nbasketful of strings and laces, she made her way over the seven\nhills to the home of the seven dwarfs. When she reached it she\nknocked at the little door and cried, \"Fine wares for sale! Fine\nwares for sale!\"\n\nSnow White peeped out of the window and said, \"Good day, my dear\nwoman, what have you there in your basket?\"\n\n\"Good wares! Fine wares!\" said the woman. \"Strings, cords and\nlaces, of all kinds and colors,\" and she held up a loop of gaily\ncolored bodice laces.\n\nSnow White was entranced with the gaudy trifle and she thought to\nherself, \"The dwarfs were only afraid of the wicked Queen, but\nsurely there can be no harm in letting this honest woman into the\nhouse.\" So she opened the door and bought the showy laces.\n\n\"Child,\" said the woman as she entered the little room, \"what a\nsight you are with that loose bodice! Come, let me fix you up\nwith your new laces, so you'll look neat and trim for once.\"\n\nSnow White, who suspected nothing, stood up to have the new gay\nlaces put into her bodice, but the woman worked quickly and laced\nher up so tightly that Snow White lost her breath and sank to the\nfloor.\n\n\"Now!\" cried the Queen as she cast a last look at the motionless\nchild, \"now you have _been_ the fairest in the land!\"\n\nLuckily this happened just as the sun was sinking behind the\nseventh hill, so it was not long before the dwarfs came trudging\nhome from work. When they saw their dear little Snow White lying\nthere, not moving, not talking, they were deeply alarmed. They\nlifted her up, and when they saw how tightly she was laced, they\nhurriedly cut the cords in two. And in that moment Snow White\ncaught her breath again, opened her eyes, and all was well once\nmore.\n\nWhen the dwarfs heard what had happened they said, \"That was no\npeddler woman, Snow White; that was the wicked Queen. So please\nbeware, dear child, and let no one into the house while we're\ngone.\"\n\n*          *          *          *          *\n\nBy this time the Queen had reached her home, so she rushed to her\nmirror and said:\n\n   \"Mirror, Mirror, on the wall,\n   Who's the fairest one of all?\"\n\nand to her dismay it answered as before:\n\n\"Thou art very fair, Oh Queen,\nBut the fairest ever seen\nDwells within the wooded glen\nWith the seven little men.\"\n\nAt this the Queen's fury knew no bounds and she said, \"But now,\nmy pretty one--now I'll think up something which _will_ be\nthe end of you!\" And soon she was very busy.\n\nYou will not be surprised, I am sure, when I tell you that this\nwicked creature was skilled in the arts of witchcraft; and with\nthe help of these arts she now worked out her second scheme. She\nfashioned a comb--a beautiful golden comb, but a poisonous one.\nThen, disguising herself as a different old woman, she crossed\nthe seven hills to the home of the seven dwarfs. When she reached\nit she knocked at the door and cried as before, \"Good wares for\nsale! Fine wares! For sale! For sale!\"\n\nSnow White peeped out of the window but this time she said, \"You\nmay as well go on your way, good"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 30303, Requested 4718. Please try again in 45.063s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 30303, Requested 4718. Please try again in 45.063s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 29652, Requested 4718. Please try again in 43.11s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 29652, Requested 4718. Please try again in 43.11s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 28702, Requested 4718. Please try again in 40.262s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 28702, Requested 4718. Please try again in 40.262s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 28702, Requested 4718. Please try again in 40.262s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 28702, Requested 4718. Please try again in 40.262s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "ed a comb--a beautiful golden comb, but a poisonous one.\nThen, disguising herself as a different old woman, she crossed\nthe seven hills to the home of the seven dwarfs. When she reached\nit she knocked at the door and cried as before, \"Good wares for\nsale! Fine wares! For sale! For sale!\"\n\nSnow White peeped out of the window but this time she said, \"You\nmay as well go on your way, good woman. I am not allowed to let\nanyone in.\"\n\n\"Very well!\" said the old woman. \"You needn't let me in, but\nsurely there can be no harm in _looking_ at my wares,\" and\nshe held up the glittering poisonous comb.\n\nSnow White was so charmed by it that she forgot all about the\ndwarfs' warning and opened the door. The old woman stepped inside\nand said in honeyed tones, \"Why don't you try it on right now, my\nlittle rabbit? Look, I'll show you how it should be worn!\"\n\nPoor Snow White, innocent and trusting, stood there with\nsparkling eyes as the woman thrust the comb into her ebon hair.\nBut as soon as the comb touched her head, the poison began to\nwork, and Snow White sank to the floor unconscious.\n\n\"You paragon of beauty!\" muttered the Queen. \"That will do for\nyou, I think.\"\n\nShe hurried away just as the sun was sinking behind the seventh\nhill, and a few minutes later the dwarfs came trudging home from\nwork. When they saw Snow White lying there on the floor, they\nknew at once that the Queen had been there again. Quickly they\nsearched, and soon enough they found the glittering poisonous\ncomb which was still fastened in the girl's black hair. But at\nthe very moment that they pulled it out, the poison lost its\npower and Snow White opened her eyes and sat up, as well as ever\nbefore.\n\nWhen she told the seven dwarfs what had happened, they looked\nvery solemn and said, \"You can see, Snow White, it was not an old\nwoman who came, but the wicked Queen in disguise. So please, dear\nchild, beware! Buy nothing from anyone and let no one, no one at\nall, into the house while we're gone!\"\n\nAnd Snow White promised.\n\n*          *          *          *          *\n\nBy this time the Queen had reached her home and there she stood\nin front of her mirror and said:\n\n   \"Mirror, Mirror, on the wall,\n   Who's the fairest one of all?\"\n\nand the mirror answered as before:\n\n   \"Thou art very fair, Oh Queen,\n   But the fairest ever seen\n   Dwells within the wooded glen\n   With the seven little men.\"\n\nWhen she heard this, the Queen trembled with rage and\ndisappointment. \"I must, I _will_ be the fairest in the\nland!\" she cried, and away she went to a lonely secret chamber\nwhere no one ever came. There, by means of her wicked witchery,\nshe fashioned an apple. A very beautiful apple it was, so waxy\nwhite and rosy red that it made one's mouth water to look at it.\nBut it was far from being as good as it looked, for it was so\nartfully made that half of it--the rosiest half--was full of\npoison.\n\nWhen the Queen had finished this apple she put it into a basket\nwith some ordinary apples, and disguised herself as a\npeasant-wife. She crossed the seven hills to the home of the\nseven dwarfs and knocked at the door as before.\n\nSnow White peeped out of the window and said, \"I am not allowed\nto let anyone in, nor to buy anything either--the seven dwarfs\nhave forbidden it.\"\n\n\"Suits me,\" said the peasant-wife, \"I can easily sell my fine\napples elsewhere. Here, I'll give you one for nothing.\"\n\n\"No,\" said Snow White, \"I'm not allowed to take anything from\nstrangers.\"\n\n\"Are you afraid? Of poison, perhaps?\" said the woman. \"See, I'll\ncut the apple in two and I myself will eat half of it to show you\nhow harmless it is. Here, you can have the nice rosy half, I'll\ntake the white part.\"\n\nBy this time Snow White's mouth was fairly watering for the\nluscious-looking fruit, and when the woman took a big bite out of\nthe white half and smacked her lips, the poor girl could bear it\nno longer. She stretched her little hand out through the window,\ntook the rosy half of the apple and bit into it. Immediately she\nsank to the floor and knew no more.\n\nWith a glance of glee and a laugh over-loud, the Queen cried,\n\"Now, you! White as snow, red as blood and black as\nebony--_now_ let the dwarfs revive you!\"\n\nShe could scarcely wait to get home to her mirror and say:\n\n   \"Mirror, Mirror, on the wall,\n   Who's the fairest one of all?\"\n\nand to her joy it said:\n\n   \"Oh Queen, thou art the fairest in the land!\"\n\nNow there was peace at last in the heart of the Queen--that is,\nas much peace as can ever be found in a heart full of envy and\nhate.\n\n*          *          *          *          *\n\nAfter the wicked Queen had gone away, the sun sank down behind\nthe seventh hill and the dwarfs came trudging home from work.\nWhen they reached their little home, no light gleamed from its\nwindows, no"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24215, Requested 191. Please try again in 13.218s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24215, Requested 191. Please try again in 13.218s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"FIRUZABAD\"\nDescription List: [\"Firuzabad is a location where the Aurelians were jailed\", \"Firuzabad is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24215, Requested 199. Please try again in 13.242s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24215, Requested 199. Please try again in 13.242s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"MEGGIE TAZBAH\"\nDescription List: [\"Meggie Tazbah is a Bratinas national and environmentalist who was held hostage\", \"Meggie Tazbah is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24213, Requested 190. Please try again in 13.209s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24213, Requested 190. Please try again in 13.209s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"AURELIA\"\nDescription List: [\"Aurelia is a country seeking to release the hostages\", \"Aurelia is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24211, Requested 202. Please try again in 13.24s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24211, Requested 202. Please try again in 13.24s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"WANDA G\\u00c1G\"\nDescription List: [\"Wanda Gg is the author of Snow White and the Seven Dwarfs\", \"Wanda Gg writes the storyWanda Gg is the author of the story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24205, Requested 189. Please try again in 13.184s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24205, Requested 189. Please try again in 13.184s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"ALHAMIA PRISON\"\nDescription List: [\"Alhamia Prison is a prison in Tiruzia\", \"Alhamia Prison is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24200, Requested 197. Please try again in 13.191s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24200, Requested 197. Please try again in 13.191s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"SAMUEL NAMARA\"\nDescription List: [\"Samuel Namara is an Aurelian who spent time in Tiruzia's Alhamia Prison\", \"Samuel Namara is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24195, Requested 185. Please try again in 13.141s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24195, Requested 185. Please try again in 13.141s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"TIRUZIA\"\nDescription List: [\"Tiruzia is not in this story\", \"Tiruzia is the capital of Firuzabad\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24191, Requested 196. Please try again in 13.162s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24191, Requested 196. Please try again in 13.162s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"DURKE BATAGLANI\"\nDescription List: [\"Durke Bataglani is an Aurelian journalist who was held hostage\", \"Durke Bataglani is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24191, Requested 186. Please try again in 13.133s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24191, Requested 186. Please try again in 13.133s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"KROHAARA\"\nDescription List: [\"Krohaara is not in this story\", \"Krohaara is the capital of Quintara\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24174, Requested 185. Please try again in 13.077s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 24174, Requested 185. Please try again in 13.077s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"CASHION\"\nDescription List: [\"Cashion is not in this story\", \"Cashion is the capital of Aurelia\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23656, Requested 185. Please try again in 11.523s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23656, Requested 185. Please try again in 11.523s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"CASHION\"\nDescription List: [\"Cashion is not in this story\", \"Cashion is the capital of Aurelia\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23660, Requested 189. Please try again in 11.549s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23660, Requested 189. Please try again in 11.549s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"ALHAMIA PRISON\"\nDescription List: [\"Alhamia Prison is a prison in Tiruzia\", \"Alhamia Prison is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23656, Requested 191. Please try again in 11.543s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23656, Requested 191. Please try again in 11.543s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"FIRUZABAD\"\nDescription List: [\"Firuzabad is a location where the Aurelians were jailed\", \"Firuzabad is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23558, Requested 202. Please try again in 11.281s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23558, Requested 202. Please try again in 11.281s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"WANDA G\\u00c1G\"\nDescription List: [\"Wanda Gg is the author of Snow White and the Seven Dwarfs\", \"Wanda Gg writes the storyWanda Gg is the author of the story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23545, Requested 190. Please try again in 11.205s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23545, Requested 190. Please try again in 11.205s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"AURELIA\"\nDescription List: [\"Aurelia is a country seeking to release the hostages\", \"Aurelia is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23545, Requested 199. Please try again in 11.232s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23545, Requested 199. Please try again in 11.232s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"MEGGIE TAZBAH\"\nDescription List: [\"Meggie Tazbah is a Bratinas national and environmentalist who was held hostage\", \"Meggie Tazbah is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23523, Requested 196. Please try again in 11.157s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23523, Requested 196. Please try again in 11.157s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"DURKE BATAGLANI\"\nDescription List: [\"Durke Bataglani is an Aurelian journalist who was held hostage\", \"Durke Bataglani is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23522, Requested 185. Please try again in 11.123s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23522, Requested 185. Please try again in 11.123s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"TIRUZIA\"\nDescription List: [\"Tiruzia is not in this story\", \"Tiruzia is the capital of Firuzabad\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23504, Requested 186. Please try again in 11.07s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23504, Requested 186. Please try again in 11.07s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"KROHAARA\"\nDescription List: [\"Krohaara is not in this story\", \"Krohaara is the capital of Quintara\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23430, Requested 197. Please try again in 10.881s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 23430, Requested 197. Please try again in 10.881s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"SAMUEL NAMARA\"\nDescription List: [\"Samuel Namara is an Aurelian who spent time in Tiruzia's Alhamia Prison\", \"Samuel Namara is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 22805, Requested 189. Please try again in 8.982s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 22805, Requested 189. Please try again in 8.982s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"ALHAMIA PRISON\"\nDescription List: [\"Alhamia Prison is a prison in Tiruzia\", \"Alhamia Prison is not in this story\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error executing verb \"summarize_descriptions\" in create_summarized_entities: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 22805, Requested 189. Please try again in 8.982s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/datashaper/workflow/workflow.py\", line 415, in _execute_verb\n    result = await result\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/description_summarize.py\", line 183, in summarize_descriptions\n    results = [\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/description_summarize.py\", line 184, in <listcomp>\n    await get_resolved_entities(row, semaphore) for row in output.itertuples()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/description_summarize.py\", line 147, in get_resolved_entities\n    results = await asyncio.gather(*futures)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/description_summarize.py\", line 167, in do_summarize_descriptions\n    results = await strategy_exec(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/strategies/graph_intelligence/run_graph_intelligence.py\", line 34, in run\n    return await run_summarize_descriptions(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/strategies/graph_intelligence/run_graph_intelligence.py\", line 67, in run_summarize_descriptions\n    result = await extractor(items=items, descriptions=descriptions)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py\", line 73, in __call__\n    result = await self._summarize_descriptions(items, descriptions)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py\", line 110, in _summarize_descriptions\n    result = await self._summarize_descriptions_with_llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py\", line 129, in _summarize_descriptions_with_llm\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 22805, Requested 189. Please try again in 8.982s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 22805, Requested 189. Please try again in 8.982s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": null}
