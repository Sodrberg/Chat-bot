{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n", "source": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n", "source": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 42655, Requested 3634. Please try again in 1m18.867s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 42655, Requested 3634. Please try again in 1m18.867s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n", "source": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 52489, Requested 3456. Please try again in 1m47.835s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 52489, Requested 3456. Please try again in 1m47.835s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}\n", "source": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 138829, Requested 3893. Please try again in 6m8.168s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 138829, Requested 3893. Please try again in 6m8.168s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 138166, Requested 3591. Please try again in 6m5.271s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 138166, Requested 3591. Please try again in 6m5.271s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 136772, Requested 3634. Please try again in 6m1.218s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 136772, Requested 3634. Please try again in 6m1.218s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 136158, Requested 3649. Please try again in 5m59.423s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 136158, Requested 3649. Please try again in 5m59.423s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 133452, Requested 3500. Please try again in 5m50.857s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 133452, Requested 3500. Please try again in 5m50.857s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 132814, Requested 3456. Please try again in 5m48.812s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 132814, Requested 3456. Please try again in 5m48.812s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 130123, Requested 3893. Please try again in 5m42.05s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 130123, Requested 3893. Please try again in 5m42.05s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 130123, Requested 3893. Please try again in 5m42.05s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 130123, Requested 3893. Please try again in 5m42.05s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "of my own country; I have visited the lakes of Lucerne and Uri, where the\nsnowy mountains descend almost perpendicularly to the water, casting black\nand impenetrable shades, which would cause a gloomy and mournful appearance\nwere it not for the most verdant islands that relieve the eye by their gay\nappearance; I have seen this lake agitated by a tempest, when the wind tore\nup whirlwinds of water and gave you an idea of what the water-spout must be\non the great ocean; and the waves dash with fury the base of the mountain,\nwhere the priest and his mistress were overwhelmed by an avalanche and\nwhere their dying voices are still said to be heard amid the pauses of the\nnightly wind; I have seen the mountains of La Valais, and the Pays de Vaud;\nbut this country, Victor, pleases me more than all those wonders. The\nmountains of Switzerland are more majestic and strange, but there is a\ncharm in the banks of this divine river that I never before saw equalled.\nLook at that castle which overhangs yon precipice; and that also on the\nisland, almost concealed amongst the foliage of those lovely trees; and now\nthat group of labourers coming from among their vines; and that village\nhalf hid in the recess of the mountain. Oh, surely the spirit that inhabits\nand guards this place has a soul more in harmony with man than those who\npile the glacier or retire to the inaccessible peaks of the mountains of\nour own country.”\n\nClerval! Beloved friend! Even now it delights me to record your words and\nto dwell on the praise of which you are so eminently deserving. He was a\nbeing formed in the “very poetry of nature.” His wild and\nenthusiastic imagination was chastened by the sensibility of his heart. His\nsoul overflowed with ardent affections, and his friendship was of that\ndevoted and wondrous nature that the worldly-minded teach us to look for only\nin the imagination. But even human sympathies were not sufficient to\nsatisfy his eager mind. The scenery of external nature, which others regard\nonly with admiration, he loved with ardour:—\n\n    ——The sounding cataract\n    Haunted him like a passion: the tall rock,\n    The mountain, and the deep and gloomy wood,\n    Their colours and their forms, were then to him\n    An appetite; a feeling, and a love,\n    That had no need of a remoter charm,\n    By thought supplied, or any interest\n    Unborrow’d from the eye.\n\n          [Wordsworth’s “Tintern Abbey”.]\n\nAnd where does he now exist? Is this gentle and lovely being lost\nfor ever? Has this mind, so replete with ideas, imaginations fanciful\nand magnificent, which formed a world, whose existence depended on the\nlife of its creator;—has this mind perished? Does it now only exist\nin my memory? No, it is not thus; your form so divinely wrought, and\nbeaming with beauty, has decayed, but your spirit still visits and\nconsoles your unhappy friend.\n\nPardon this gush of sorrow; these ineffectual words are but a slight\ntribute to the unexampled worth of Henry, but they soothe my heart,\noverflowing with the anguish which his remembrance creates. I will\nproceed with my tale.\n\nBeyond Cologne we descended to the plains of Holland; and we resolved to\npost the remainder of our way, for the wind was contrary and the stream of\nthe river was too gentle to aid us.\n\nOur journey here lost the interest arising from beautiful scenery, but we\narrived in a few days at Rotterdam, whence we proceeded by sea to England.\nIt was on a clear morning, in the latter days of December, that I first saw\nthe white cliffs of Britain. The banks of the Thames presented a new scene;\nthey were flat but fertile, and almost every town was marked by the\nremembrance of some story. We saw Tilbury Fort and remembered the Spanish\nArmada, Gravesend, Woolwich, and Greenwich—places which I had heard\nof even in my country.\n\nAt length we saw the numerous steeples of London, St. Paul’s towering\nabove all, and the Tower famed in English history.\n\n\n\n\nChapter 19\n\n\nLondon was our present point of rest; we determined to remain several\nmonths in this wonderful and celebrated city. Clerval desired the\nintercourse of the men of genius and talent who flourished at this\ntime, but this was with me a secondary object; I was principally\noccupied with the means of obtaining the information necessary for the\ncompletion of my promise and quickly availed myself of the letters of\nintroduction that I had brought with me, addressed to the most\ndistinguished natural philosophers.\n\nIf this journey had taken place during my days of study and happiness,\nit would have afforded me inexpressible pleasure. But a blight had\ncome over my existence, and I only visited these people for the sake of\nthe information they might give me on the subject in which my interest\nwas so terribly profound. Company was irksome to me; when alone, I\ncould fill my mind with the sights of heaven and earth; the voice of\nHenry soothed me, and I could thus cheat myself into a transitory\npeace. But busy, uninteresting, joyous faces brought back despair to\nmy heart. I saw an insurmountable barrier placed between me and my\nfellow men; this barrier was sealed with the blood of William and\nJustine, and to reflect on the events connected with those names filled\nmy soul"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 129440, Requested 3591. Please try again in 5m39.094s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 129440, Requested 3591. Please try again in 5m39.094s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 129440, Requested 3591. Please try again in 5m39.094s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 129440, Requested 3591. Please try again in 5m39.094s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "irretrievable, and after much consideration I resolved to return to the\ncottage, seek the old man, and by my representations win him to my\nparty.\n\n“These thoughts calmed me, and in the afternoon I sank into a profound\nsleep; but the fever of my blood did not allow me to be visited by\npeaceful dreams. The horrible scene of the preceding day was for ever\nacting before my eyes; the females were flying and the enraged Felix\ntearing me from his father’s feet. I awoke exhausted, and finding that\nit was already night, I crept forth from my hiding-place, and went in\nsearch of food.\n\n“When my hunger was appeased, I directed my steps towards the\nwell-known path that conducted to the cottage. All there was at peace.\nI crept into my hovel and remained in silent expectation of the\naccustomed hour when the family arose. That hour passed, the sun\nmounted high in the heavens, but the cottagers did not appear. I\ntrembled violently, apprehending some dreadful misfortune. The inside\nof the cottage was dark, and I heard no motion; I cannot describe the\nagony of this suspense.\n\n“Presently two countrymen passed by, but pausing near the cottage, they\nentered into conversation, using violent gesticulations; but I did not\nunderstand what they said, as they spoke the language of the country,\nwhich differed from that of my protectors. Soon after, however, Felix\napproached with another man; I was surprised, as I knew that he had not\nquitted the cottage that morning, and waited anxiously to discover from\nhis discourse the meaning of these unusual appearances.\n\n“‘Do you consider,’ said his companion to him,\n‘that you will be obliged to pay three months’ rent and to lose\nthe produce of your garden? I do not wish to take any unfair advantage, and\nI beg therefore that you will take some days to consider of your\ndetermination.’\n\n“‘It is utterly useless,’ replied Felix; ‘we can\nnever again inhabit your cottage. The life of my father is in the greatest\ndanger, owing to the dreadful circumstance that I have related. My wife and\nmy sister will never recover from their horror. I entreat you not to reason\nwith me any more. Take possession of your tenement and let me fly from this\nplace.’\n\n“Felix trembled violently as he said this. He and his companion\nentered the cottage, in which they remained for a few minutes, and then\ndeparted. I never saw any of the family of De Lacey more.\n\n“I continued for the remainder of the day in my hovel in a state of\nutter and stupid despair. My protectors had departed and had broken\nthe only link that held me to the world. For the first time the\nfeelings of revenge and hatred filled my bosom, and I did not strive to\ncontrol them, but allowing myself to be borne away by the stream, I\nbent my mind towards injury and death. When I thought of my friends,\nof the mild voice of De Lacey, the gentle eyes of Agatha, and the\nexquisite beauty of the Arabian, these thoughts vanished and a gush of\ntears somewhat soothed me. But again when I reflected that they had\nspurned and deserted me, anger returned, a rage of anger, and unable to\ninjure anything human, I turned my fury towards inanimate objects. As\nnight advanced, I placed a variety of combustibles around the cottage,\nand after having destroyed every vestige of cultivation in the garden,\nI waited with forced impatience until the moon had sunk to commence my\noperations.\n\n“As the night advanced, a fierce wind arose from the woods and quickly\ndispersed the clouds that had loitered in the heavens; the blast tore\nalong like a mighty avalanche and produced a kind of insanity in my\nspirits that burst all bounds of reason and reflection. I lighted the\ndry branch of a tree and danced with fury around the devoted cottage,\nmy eyes still fixed on the western horizon, the edge of which the moon\nnearly touched. A part of its orb was at length hid, and I waved my\nbrand; it sank, and with a loud scream I fired the straw, and heath,\nand bushes, which I had collected. The wind fanned the fire, and the\ncottage was quickly enveloped by the flames, which clung to it and\nlicked it with their forked and destroying tongues.\n\n“As soon as I was convinced that no assistance could save any part of\nthe habitation, I quitted the scene and sought for refuge in the woods.\n\n“And now, with the world before me, whither should I bend my steps? I\nresolved to fly far from the scene of my misfortunes; but to me, hated\nand despised, every country must be equally horrible. At length the\nthought of you crossed my mind. I learned from your papers that you\nwere my father, my creator; and to whom could I apply with more fitness\nthan to him who had given me life? Among the lessons that Felix had\nbestowed upon Safie, geography had not been omitted; I had learned from\nthese the relative situations of the different countries of the earth.\nYou had mentioned Geneva as the name of your native town, and towards\nthis place I resolved to proceed.\n\n“But how was I to direct myself? I knew that I must travel in a\nsouthwesterly direction to reach my destination, but the sun was my\nonly guide. I did not know the names of the towns that I was to pass\nthrough, nor could I ask information from a"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 128136, Requested 3634. Please try again in 5m35.31s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 128136, Requested 3634. Please try again in 5m35.31s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 128136, Requested 3634. Please try again in 5m35.31s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 128136, Requested 3634. Please try again in 5m35.31s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "moments vengeance, that\nburned within me, died in my heart, and I pursued my path towards the\ndestruction of the dæmon more as a task enjoined by heaven, as the\nmechanical impulse of some power of which I was unconscious, than as the\nardent desire of my soul.\n\nWhat his feelings were whom I pursued I cannot know. Sometimes, indeed, he\nleft marks in writing on the barks of the trees or cut in stone that guided\nme and instigated my fury. “My reign is not yet\nover”—these words were legible in one of these\ninscriptions—“you live, and my power is complete. Follow me; I\nseek the everlasting ices of the north, where you will feel the misery of\ncold and frost, to which I am impassive. You will find near this place, if\nyou follow not too tardily, a dead hare; eat and be refreshed. Come on, my\nenemy; we have yet to wrestle for our lives, but many hard and miserable\nhours must you endure until that period shall arrive.”\n\nScoffing devil! Again do I vow vengeance; again do I devote thee,\nmiserable fiend, to torture and death. Never will I give up my search\nuntil he or I perish; and then with what ecstasy shall I join my\nElizabeth and my departed friends, who even now prepare for me the\nreward of my tedious toil and horrible pilgrimage!\n\nAs I still pursued my journey to the northward, the snows thickened and the\ncold increased in a degree almost too severe to support. The peasants were\nshut up in their hovels, and only a few of the most hardy ventured forth to\nseize the animals whom starvation had forced from their hiding-places to\nseek for prey. The rivers were covered with ice, and no fish could be\nprocured; and thus I was cut off from my chief article of maintenance.\n\nThe triumph of my enemy increased with the difficulty of my labours. One\ninscription that he left was in these words: “Prepare! Your toils\nonly begin; wrap yourself in furs and provide food, for we shall soon enter\nupon a journey where your sufferings will satisfy my everlasting\nhatred.”\n\nMy courage and perseverance were invigorated by these scoffing words; I\nresolved not to fail in my purpose, and calling on Heaven to support\nme, I continued with unabated fervour to traverse immense deserts,\nuntil the ocean appeared at a distance and formed the utmost boundary\nof the horizon. Oh! How unlike it was to the blue seasons of the\nsouth! Covered with ice, it was only to be distinguished from land by\nits superior wildness and ruggedness. The Greeks wept for joy when\nthey beheld the Mediterranean from the hills of Asia, and hailed with\nrapture the boundary of their toils. I did not weep, but I knelt down\nand with a full heart thanked my guiding spirit for conducting me in\nsafety to the place where I hoped, notwithstanding my adversary’s gibe,\nto meet and grapple with him.\n\nSome weeks before this period I had procured a sledge and dogs and thus\ntraversed the snows with inconceivable speed. I know not whether the\nfiend possessed the same advantages, but I found that, as before I had\ndaily lost ground in the pursuit, I now gained on him, so much so that\nwhen I first saw the ocean he was but one day’s journey in advance, and\nI hoped to intercept him before he should reach the beach. With new\ncourage, therefore, I pressed on, and in two days arrived at a wretched\nhamlet on the seashore. I inquired of the inhabitants concerning the\nfiend and gained accurate information. A gigantic monster, they said,\nhad arrived the night before, armed with a gun and many pistols,\nputting to flight the inhabitants of a solitary cottage through fear of\nhis terrific appearance. He had carried off their store of winter\nfood, and placing it in a sledge, to draw which he had seized on a\nnumerous drove of trained dogs, he had harnessed them, and the same\nnight, to the joy of the horror-struck villagers, had pursued his\njourney across the sea in a direction that led to no land; and they\nconjectured that he must speedily be destroyed by the breaking of the\nice or frozen by the eternal frosts.\n\nOn hearing this information I suffered a temporary access of despair.\nHe had escaped me, and I must commence a destructive and almost endless\njourney across the mountainous ices of the ocean, amidst cold that few\nof the inhabitants could long endure and which I, the native of a\ngenial and sunny climate, could not hope to survive. Yet at the idea\nthat the fiend should live and be triumphant, my rage and vengeance\nreturned, and like a mighty tide, overwhelmed every other feeling.\nAfter a slight repose, during which the spirits of the dead hovered\nround and instigated me to toil and revenge, I prepared for my journey.\n\nI exchanged my land-sledge for one fashioned for the inequalities of\nthe Frozen Ocean, and purchasing a plentiful stock of provisions, I\ndeparted from land.\n\nI cannot guess how many days have passed since then, but I have endured\nmisery which nothing but the eternal sentiment of a just retribution\nburning within my heart could have enabled me to support. Immense and\nrugged mountains of ice often barred up my passage, and I often heard\nthe thunder of the ground sea, which threatened my destruction. But\nagain the frost came and made"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 127464, Requested 3649. Please try again in 5m33.341s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 127464, Requested 3649. Please try again in 5m33.341s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 127464, Requested 3649. Please try again in 5m33.341s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 127464, Requested 3649. Please try again in 5m33.341s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "again felt as\nif I belonged to a race of human beings like myself, and I began to\nreflect upon what had passed with greater composure; yet still the\nwords of the fiend rang in my ears like a death-knell; they appeared\nlike a dream, yet distinct and oppressive as a reality.\n\nThe sun had far descended, and I still sat on the shore, satisfying my\nappetite, which had become ravenous, with an oaten cake, when I saw a\nfishing-boat land close to me, and one of the men brought me a packet;\nit contained letters from Geneva, and one from Clerval entreating me to\njoin him. He said that he was wearing away his time fruitlessly where\nhe was, that letters from the friends he had formed in London desired\nhis return to complete the negotiation they had entered into for his\nIndian enterprise. He could not any longer delay his departure; but as\nhis journey to London might be followed, even sooner than he now\nconjectured, by his longer voyage, he entreated me to bestow as much of\nmy society on him as I could spare. He besought me, therefore, to\nleave my solitary isle and to meet him at Perth, that we might proceed\nsouthwards together. This letter in a degree recalled me to life, and\nI determined to quit my island at the expiration of two days.\n\nYet, before I departed, there was a task to perform, on which I shuddered\nto reflect; I must pack up my chemical instruments, and for that purpose I\nmust enter the room which had been the scene of my odious work, and I must\nhandle those utensils the sight of which was sickening to me. The next\nmorning, at daybreak, I summoned sufficient courage and unlocked the door\nof my laboratory. The remains of the half-finished creature, whom I had\ndestroyed, lay scattered on the floor, and I almost felt as if I had\nmangled the living flesh of a human being. I paused to collect myself and\nthen entered the chamber. With trembling hand I conveyed the instruments\nout of the room, but I reflected that I ought not to leave the relics of my\nwork to excite the horror and suspicion of the peasants; and I accordingly\nput them into a basket, with a great quantity of stones, and laying them\nup, determined to throw them into the sea that very night; and in the\nmeantime I sat upon the beach, employed in cleaning and arranging my\nchemical apparatus.\n\nNothing could be more complete than the alteration that had taken place\nin my feelings since the night of the appearance of the dæmon. I had\nbefore regarded my promise with a gloomy despair as a thing that, with\nwhatever consequences, must be fulfilled; but I now felt as if a film\nhad been taken from before my eyes and that I for the first time saw\nclearly. The idea of renewing my labours did not for one instant occur\nto me; the threat I had heard weighed on my thoughts, but I did not\nreflect that a voluntary act of mine could avert it. I had resolved in\nmy own mind that to create another like the fiend I had first made\nwould be an act of the basest and most atrocious selfishness, and I\nbanished from my mind every thought that could lead to a different\nconclusion.\n\nBetween two and three in the morning the moon rose; and I then, putting my\nbasket aboard a little skiff, sailed out about four miles from the shore.\nThe scene was perfectly solitary; a few boats were returning towards land,\nbut I sailed away from them. I felt as if I was about the commission of a\ndreadful crime and avoided with shuddering anxiety any encounter with my\nfellow creatures. At one time the moon, which had before been clear, was\nsuddenly overspread by a thick cloud, and I took advantage of the moment of\ndarkness and cast my basket into the sea; I listened to the gurgling sound\nas it sank and then sailed away from the spot. The sky became clouded, but\nthe air was pure, although chilled by the northeast breeze that was then\nrising. But it refreshed me and filled me with such agreeable sensations\nthat I resolved to prolong my stay on the water, and fixing the rudder in a\ndirect position, stretched myself at the bottom of the boat. Clouds hid the\nmoon, everything was obscure, and I heard only the sound of the boat as its\nkeel cut through the waves; the murmur lulled me, and in a short time I\nslept soundly.\n\nI do not know how long I remained in this situation, but when I awoke I\nfound that the sun had already mounted considerably. The wind was high, and\nthe waves continually threatened the safety of my little skiff. I found\nthat the wind was northeast and must have driven me far from the coast from\nwhich I had embarked. I endeavoured to change my course but quickly found\nthat if I again made the attempt the boat would be instantly filled with\nwater. Thus situated, my only resource was to drive before the wind. I\nconfess that I felt a few sensations of terror. I had no compass with me\nand was so slenderly acquainted with the geography of this part of the\nworld that the sun was of little benefit to me. I might be driven into the\nwide Atlantic and feel all the tortures of starvation or be swallowed up in\nthe immeasurable waters that roared and buffeted around me. I had already\nbeen out many hours and felt the torment of a burning thirst"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 125447, Requested 3500. Please try again in 5m26.841s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 125447, Requested 3500. Please try again in 5m26.841s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 125447, Requested 3500. Please try again in 5m26.841s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 125447, Requested 3500. Please try again in 5m26.841s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "age rendered him extremely averse to delay. For myself, there was one\nreward I promised myself from my detested toils—one consolation for my\nunparalleled sufferings; it was the prospect of that day when,\nenfranchised from my miserable slavery, I might claim Elizabeth and\nforget the past in my union with her.\n\nI now made arrangements for my journey, but one feeling haunted me\nwhich filled me with fear and agitation. During my absence I should\nleave my friends unconscious of the existence of their enemy and\nunprotected from his attacks, exasperated as he might be by my\ndeparture. But he had promised to follow me wherever I might go, and\nwould he not accompany me to England? This imagination was dreadful in\nitself, but soothing inasmuch as it supposed the safety of my friends.\nI was agonised with the idea of the possibility that the reverse of\nthis might happen. But through the whole period during which I was the\nslave of my creature I allowed myself to be governed by the impulses of\nthe moment; and my present sensations strongly intimated that the fiend\nwould follow me and exempt my family from the danger of his\nmachinations.\n\nIt was in the latter end of September that I again quitted my native\ncountry. My journey had been my own suggestion, and Elizabeth\ntherefore acquiesced, but she was filled with disquiet at the idea of\nmy suffering, away from her, the inroads of misery and grief. It had\nbeen her care which provided me a companion in Clerval—and yet a man\nis blind to a thousand minute circumstances which call forth a woman’s\nsedulous attention. She longed to bid me hasten my return; a thousand\nconflicting emotions rendered her mute as she bade me a tearful, silent\nfarewell.\n\nI threw myself into the carriage that was to convey me away, hardly\nknowing whither I was going, and careless of what was passing around.\nI remembered only, and it was with a bitter anguish that I reflected on\nit, to order that my chemical instruments should be packed to go with\nme. Filled with dreary imaginations, I passed through many beautiful\nand majestic scenes, but my eyes were fixed and unobserving. I could\nonly think of the bourne of my travels and the work which was to occupy\nme whilst they endured.\n\nAfter some days spent in listless indolence, during which I traversed\nmany leagues, I arrived at Strasburgh, where I waited two days for\nClerval. He came. Alas, how great was the contrast between us! He\nwas alive to every new scene, joyful when he saw the beauties of the\nsetting sun, and more happy when he beheld it rise and recommence a new\nday. He pointed out to me the shifting colours of the landscape and\nthe appearances of the sky. “This is what it is to live,” he cried;\n“now I enjoy existence! But you, my dear Frankenstein, wherefore are\nyou desponding and sorrowful!” In truth, I was occupied by gloomy\nthoughts and neither saw the descent of the evening star nor the golden\nsunrise reflected in the Rhine. And you, my friend, would be far more\namused with the journal of Clerval, who observed the scenery with an\neye of feeling and delight, than in listening to my reflections. I, a\nmiserable wretch, haunted by a curse that shut up every avenue to\nenjoyment.\n\nWe had agreed to descend the Rhine in a boat from Strasburgh to\nRotterdam, whence we might take shipping for London. During this\nvoyage we passed many willowy islands and saw several beautiful towns.\nWe stayed a day at Mannheim, and on the fifth from our departure from\nStrasburgh, arrived at Mainz. The course of the Rhine below Mainz\nbecomes much more picturesque. The river descends rapidly and winds\nbetween hills, not high, but steep, and of beautiful forms. We saw\nmany ruined castles standing on the edges of precipices, surrounded by\nblack woods, high and inaccessible. This part of the Rhine, indeed,\npresents a singularly variegated landscape. In one spot you view\nrugged hills, ruined castles overlooking tremendous precipices, with\nthe dark Rhine rushing beneath; and on the sudden turn of a promontory,\nflourishing vineyards with green sloping banks and a meandering river\nand populous towns occupy the scene.\n\nWe travelled at the time of the vintage and heard the song of the labourers\nas we glided down the stream. Even I, depressed in mind, and my spirits\ncontinually agitated by gloomy feelings, even I was pleased. I lay at the\nbottom of the boat, and as I gazed on the cloudless blue sky, I seemed to\ndrink in a tranquillity to which I had long been a stranger. And if these\nwere my sensations, who can describe those of Henry? He felt as if he had\nbeen transported to Fairy-land and enjoyed a happiness seldom tasted by\nman. “I have seen,” he said, “the most beautiful scenes\nof my own country; I have visited the lakes of Lucerne and Uri, where the\nsnowy mountains descend almost perpendicularly to the water, casting black\nand impenetrable shades, which would cause a gloomy and mournful appearance\nwere it not for the most verdant islands that relieve the eye by their gay\nappearance; I have seen this lake agitated by a tempest, when the wind tore\nup whirlwinds of water and gave you an idea"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 124778, Requested 3456. Please try again in 5m24.702s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 124778, Requested 3456. Please try again in 5m24.702s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n"}}
{"type": "error", "data": "Entity Extraction Error", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 122, in __call__\n    result = await self._process_document(text, prompt_variables)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py\", line 161, in _process_document\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 124778, Requested 3456. Please try again in 5m24.702s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 124778, Requested 3456. Please try again in 5m24.702s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"doc_index": 0, "text": "as! Yes; I cannot withstand their demands. I cannot lead them\nunwillingly to danger, and I must return.”\n\n“Do so, if you will; but I will not. You may give up your purpose, but\nmine is assigned to me by Heaven, and I dare not. I am weak, but\nsurely the spirits who assist my vengeance will endow me with\nsufficient strength.” Saying this, he endeavoured to spring from the\nbed, but the exertion was too great for him; he fell back and fainted.\n\nIt was long before he was restored, and I often thought that life was\nentirely extinct. At length he opened his eyes; he breathed with\ndifficulty and was unable to speak. The surgeon gave him a composing\ndraught and ordered us to leave him undisturbed. In the meantime he\ntold me that my friend had certainly not many hours to live.\n\nHis sentence was pronounced, and I could only grieve and be patient. I sat\nby his bed, watching him; his eyes were closed, and I thought he slept; but\npresently he called to me in a feeble voice, and bidding me come near,\nsaid, “Alas! The strength I relied on is gone; I feel that I shall\nsoon die, and he, my enemy and persecutor, may still be in being. Think\nnot, Walton, that in the last moments of my existence I feel that burning\nhatred and ardent desire of revenge I once expressed; but I feel myself\njustified in desiring the death of my adversary. During these last days I\nhave been occupied in examining my past conduct; nor do I find it blamable.\nIn a fit of enthusiastic madness I created a rational creature and was\nbound towards him to assure, as far as was in my power, his happiness and\nwell-being. This was my duty, but there was another still paramount to\nthat. My duties towards the beings of my own species had greater claims to\nmy attention because they included a greater proportion of happiness or\nmisery. Urged by this view, I refused, and I did right in refusing, to\ncreate a companion for the first creature. He showed unparalleled malignity\nand selfishness in evil; he destroyed my friends; he devoted to destruction\nbeings who possessed exquisite sensations, happiness, and wisdom; nor do I\nknow where this thirst for vengeance may end. Miserable himself that he may\nrender no other wretched, he ought to die. The task of his destruction was\nmine, but I have failed. When actuated by selfish and vicious motives, I\nasked you to undertake my unfinished work, and I renew this request now,\nwhen I am only induced by reason and virtue.\n\n“Yet I cannot ask you to renounce your country and friends to fulfil\nthis task; and now that you are returning to England, you will have\nlittle chance of meeting with him. But the consideration of these\npoints, and the well balancing of what you may esteem your duties, I\nleave to you; my judgment and ideas are already disturbed by the near\napproach of death. I dare not ask you to do what I think right, for I\nmay still be misled by passion.\n\n“That he should live to be an instrument of mischief disturbs me; in\nother respects, this hour, when I momentarily expect my release, is the\nonly happy one which I have enjoyed for several years. The forms of\nthe beloved dead flit before me, and I hasten to their arms. Farewell,\nWalton! Seek happiness in tranquillity and avoid ambition, even if it\nbe only the apparently innocent one of distinguishing yourself in\nscience and discoveries. Yet why do I say this? I have myself been\nblasted in these hopes, yet another may succeed.”\n\nHis voice became fainter as he spoke, and at length, exhausted by his\neffort, he sank into silence. About half an hour afterwards he\nattempted again to speak but was unable; he pressed my hand feebly, and\nhis eyes closed for ever, while the irradiation of a gentle smile passed\naway from his lips.\n\nMargaret, what comment can I make on the untimely extinction of this\nglorious spirit? What can I say that will enable you to understand the\ndepth of my sorrow? All that I should express would be inadequate and\nfeeble. My tears flow; my mind is overshadowed by a cloud of\ndisappointment. But I journey towards England, and I may there find\nconsolation.\n\nI am interrupted. What do these sounds portend? It is midnight; the\nbreeze blows fairly, and the watch on deck scarcely stir. Again there\nis a sound as of a human voice, but hoarser; it comes from the cabin\nwhere the remains of Frankenstein still lie. I must arise and examine.\nGood night, my sister.\n\nGreat God! what a scene has just taken place! I am yet dizzy with the\nremembrance of it. I hardly know whether I shall have the power to detail\nit; yet the tale which I have recorded would be incomplete without this\nfinal and wonderful catastrophe.\n\nI entered the cabin where lay the remains of my ill-fated and admirable\nfriend. Over him hung a form which I cannot find words to\ndescribe—gigantic in stature, yet uncouth and distorted in its\nproportions. As he hung over the coffin, his face was concealed by long\nlocks of ragged hair; but one vast hand was extended, in colour and\napparent texture like that of a mummy. When he heard the sound of my\napproach, he ceased to utter exclamations of grief and"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111799, Requested 261. Please try again in 4m36.182s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111799, Requested 261. Please try again in 4m36.182s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"MARGARET\"\nDescription List: [\"Margaret is Victor Frankenstein's sister, who is anxiously awaiting his return\", \"Margaret is the narrator's friend who is listening to Frankenstein's story\", \"Margaret is the narrator's sister\", \"Margaret is the narrator's sister or friend\", \"Margaret is the person to whom Walton is reading the narrator's story\", \"Margaret is the person who the captain is writing to\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111798, Requested 235. Please try again in 4m36.099s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111798, Requested 235. Please try again in 4m36.099s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"HOME\"\nDescription List: [\"Home is the place where the narrator's family lives\", \"The home of Victor Frankenstein's family\", \"The narrator's home is where he is leaving\", \"The narrator's home was on a common where he spent his childhoodThe narrator spent his childhood on a common near his home\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111798, Requested 504. Please try again in 4m36.906s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111798, Requested 504. Please try again in 4m36.906s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"FRANKENSTEIN\"\nDescription List: [\"Frankenstein is a novel written by Mary Shelley, first published in 1818\", \"Frankenstein is a novel written by Mary Wollstonecraft Shelley\", \"Frankenstein is a person who has been referred to as modest\", \"Frankenstein is the captain of the vessel and a brave man who encourages his crew to be heroes\", \"Frankenstein is the creator of the monster and is being addressed by the monster\", \"Frankenstein is the creator of the monster and is being addressed by the monsterFrankenstein is the creator of the monster\", \"Frankenstein is the creator of the monster and the author of its existence and tormentsFrankenstein is the author of the monster's existence and torments\", \"Frankenstein is the monster created by Victor Frankenstein\", \"Frankenstein is the narrator of the story\", \"Frankenstein is the narrator of the story and the creator of the monster\", \"Frankenstein is the narrator's enemy and the father of the child he kills\", \"Frankenstein is the narrator's father\", \"Frankenstein is the narrator's name and he is a person who is filled with remorse and horror for his actions\", \"Frankenstein is the novel in which the narrator's story takes place\", \"Frankenstein is the protagonist of the story who is a scientist and a monster\", \"Frankenstein is the scientist who created the monster\", \"The story of Victor Frankenstein's creation\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111794, Requested 221. Please try again in 4m36.045s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111794, Requested 221. Please try again in 4m36.045s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"POETS\"\nDescription List: [\"The narrator became acquainted with the celebrated poets of his own country\", \"The narrator became acquainted with the celebrated poets of their own country\", \"Victor mentions reading the poetry of Greece and Rome\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111790, Requested 479. Please try again in 4m36.807s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111790, Requested 479. Please try again in 4m36.807s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"ALHAMIA PRISON\"\nDescription List: [\"\", \"Alhamia Prison is a location in Tiruzia\", \"Alhamia Prison is a location in the story\", \"Alhamia Prison is a location mentioned in the letter\", \"Alhamia Prison is a location mentioned in the story\", \"Alhamia Prison is a location where Samuel Namara was held\", \"Alhamia Prison is a place that Samuel Namara was held in\", \"Alhamia Prison is a place that was not mentioned in the text\", \"Alhamia Prison is a place where Samuel Namara was held\", \"Alhamia Prison is a place where Samuel Namara was held captive\", \"Alhamia Prison is a prison in Tiruzia\", \"Alhamia Prison is a prison in TiruziaAlhamia Prison is a place mentioned in the text\", \"Alhamia Prison is a prison that is mentioned in the context of Frankenstein\", \"Alhamia Prison is the location where Justine was held\", \"Alhamia Prison is the location where the monster was being held\", \"Alhamia Prison was where Samuel Namara was held\", \"Alhamia prison is a place where Samuel Namara was held\", \"Alhamia prison is the place where the stranger was held captive\", \"I had been conversing with several persons in the island I had inhabited\", \"Samuel Namara was a prisoner at Alhamia prison\", \"The narrator spent time in Alhamia Prison\", \"The prison where Muhammadan was being held\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111631, Requested 196. Please try again in 4m35.482s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111631, Requested 196. Please try again in 4m35.482s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"MASTER\"\nDescription List: [\"The master is the person in charge of the ship\", \"The master of the ship is a person of an excellent disposition\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111542, Requested 202. Please try again in 4m35.233s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111542, Requested 202. Please try again in 4m35.233s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"ICE\"\nDescription List: [\"Ice is the obstacle that Victor Frankenstein and his companions are facing\", \"The ice is the barrier that the ship had to navigate through\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111502, Requested 220. Please try again in 4m35.166s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111502, Requested 220. Please try again in 4m35.166s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"SHIP\"\nDescription List: [\"Ship is the vessel that rescued the narrator\", \"The narrator is going to sail on a ship\", \"The ship is the vessel that rescued the stranger\", \"The ship is the vessel that the narrator and the stranger are on\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111281, Requested 247. Please try again in 4m34.584999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111281, Requested 247. Please try again in 4m34.584999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"UNIVERSITY\"\nDescription List: [\"The narrator is self-educated and never attended university\", \"The university is the narrator's destination\", \"The university is where Frankenstein studies and meets his professors\", \"University is a type of educational institution\", \"University where I procured great esteem and admiration for my discoveries\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111151, Requested 235. Please try again in 4m34.159s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 111151, Requested 235. Please try again in 4m34.159s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"HOME\"\nDescription List: [\"Home is the place where the narrator's family lives\", \"The home of Victor Frankenstein's family\", \"The narrator's home is where he is leaving\", \"The narrator's home was on a common where he spent his childhoodThe narrator spent his childhood on a common near his home\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110932, Requested 202. Please try again in 4m33.402s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110932, Requested 202. Please try again in 4m33.402s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"ICE\"\nDescription List: [\"Ice is the obstacle that Victor Frankenstein and his companions are facing\", \"The ice is the barrier that the ship had to navigate through\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110802, Requested 196. Please try again in 4m32.996s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110802, Requested 196. Please try again in 4m32.996s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"MASTER\"\nDescription List: [\"The master is the person in charge of the ship\", \"The master of the ship is a person of an excellent disposition\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110696, Requested 220. Please try again in 4m32.748s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110696, Requested 220. Please try again in 4m32.748s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"SHIP\"\nDescription List: [\"Ship is the vessel that rescued the narrator\", \"The narrator is going to sail on a ship\", \"The ship is the vessel that rescued the stranger\", \"The ship is the vessel that the narrator and the stranger are on\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110494, Requested 243. Please try again in 4m32.213s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110494, Requested 243. Please try again in 4m32.213s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"FRIEND\"\nDescription List: [\"A friend is a person who is close to the narrator and the stranger\", \"The friend is the one who is being mourned by the monster\", \"The friend is the one who was driven from Felix's door with contumely\", \"The narrator's friend is a noble fellow\", \"Victor Frankenstein and Elizabeth Lavenza as friends\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110331, Requested 206. Please try again in 4m31.613s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110331, Requested 206. Please try again in 4m31.613s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"GROUND SEA\"\nDescription List: [\"Ground sea is a natural phenomenon that threatened the narrator's destruction\", \"The ground sea is the area where the ice broke and freed the ship\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110156, Requested 202. Please try again in 4m31.076s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110156, Requested 202. Please try again in 4m31.076s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": {"input": "\nYou are a helpful assistant responsible for generating a comprehensive summary of the data provided below.\nGiven one or two entities, and a list of descriptions, all related to the same entity or group of entities.\nPlease concatenate all of these into a single, comprehensive description. Make sure to include information collected from all the descriptions.\nIf the provided descriptions are contradictory, please resolve the contradictions and provide a single, coherent summary.\nMake sure it is written in third person, and include the entity names so we have the full context.\n\n#######\n-Data-\nEntities: \"ICE\"\nDescription List: [\"Ice is the obstacle that Victor Frankenstein and his companions are facing\", \"The ice is the barrier that the ship had to navigate through\"]\n#######\nOutput:\n"}}
{"type": "error", "data": "Error executing verb \"summarize_descriptions\" in create_summarized_entities: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110156, Requested 202. Please try again in 4m31.076s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "stack": "Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/datashaper/workflow/workflow.py\", line 415, in _execute_verb\n    result = await result\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/description_summarize.py\", line 183, in summarize_descriptions\n    results = [\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/description_summarize.py\", line 184, in <listcomp>\n    await get_resolved_entities(row, semaphore) for row in output.itertuples()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/description_summarize.py\", line 147, in get_resolved_entities\n    results = await asyncio.gather(*futures)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/description_summarize.py\", line 167, in do_summarize_descriptions\n    results = await strategy_exec(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/strategies/graph_intelligence/run_graph_intelligence.py\", line 34, in run\n    return await run_summarize_descriptions(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/verbs/entities/summarize/strategies/graph_intelligence/run_graph_intelligence.py\", line 67, in run_summarize_descriptions\n    result = await extractor(items=items, descriptions=descriptions)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py\", line 73, in __call__\n    result = await self._summarize_descriptions(items, descriptions)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py\", line 110, in _summarize_descriptions\n    result = await self._summarize_descriptions_with_llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py\", line 129, in _summarize_descriptions_with_llm\n    response = await self._llm(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 418, in exc_check\n    raise retry_exc.reraise()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/tenacity/__init__.py\", line 185, in reraise\n    raise self.last_attempt.result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 151, in do_attempt\n    await sleep_for(sleep_time)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 49, in __call__\n    return await self._invoke(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py\", line 53, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1412, in create\n    return await self._post(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1816, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1510, in request\n    return await self._request(\n  File \"/opt/anaconda3/envs/chat_bot/lib/python3.10/site-packages/openai/_base_client.py\", line 1611, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110156, Requested 202. Please try again in 4m31.076s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n", "source": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j29qft5xeht85dfx2f6yzrfe` on tokens per minute (TPM): Limit 20000, Used 110156, Requested 202. Please try again in 4m31.076s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}", "details": null}
